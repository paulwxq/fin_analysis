# æ¨¡å—Bï¼šç½‘ç»œæœç´¢ä¸æ‘˜è¦ï¼ˆDeep Researchï¼‰â€” è¯¦ç»†è®¾è®¡

## ä¸€ã€æ¨¡å—æ¦‚è¿°

### 1.1 å®šä½

æ¨¡å—B æ˜¯ `stock_analyzer` çš„ç½‘ç»œä¿¡æ¯é‡‡é›†æ¨¡å—ï¼Œè´Ÿè´£é€šè¿‡ **Tavily Search API** å¯¹ç›®æ ‡è‚¡ç¥¨è¿›è¡Œå¤šä¸»é¢˜æ·±åº¦ç½‘ç»œæœç´¢ï¼Œé‡‡ç”¨ **Deep Research é€’å½’æœç´¢**æ¨¡å¼ï¼ˆæ¯ä¸»é¢˜ `breadth=3, depth=2`ï¼‰ï¼Œæœ€ç»ˆç”± LLM å°†æ‰€æœ‰æœç´¢å‘ç°ï¼ˆlearningsï¼‰æ•´åˆä¸ºç»“æ„åŒ–çš„æŠ•èµ„ç ”ç©¶æ‘˜è¦ã€‚

### 1.2 è¾“å…¥è¾“å‡º

| | è¯´æ˜ |
|------|------|
| **è¾“å…¥** | è‚¡ç¥¨ä»£ç ï¼ˆ`symbol`ï¼‰ã€è‚¡ç¥¨åç§°ï¼ˆ`name`ï¼‰ã€æ‰€å±è¡Œä¸šï¼ˆ`industry`ï¼‰ |
| **è¾“å‡º** | `WebResearchResult`ï¼ˆPydantic æ¨¡å‹å¯¹è±¡ï¼‰ |

### 1.3 æŠ€æœ¯é€‰å‹

| ç»„ä»¶ | é€‰å‹ | è¯´æ˜ |
|------|------|------|
| Python ç‰ˆæœ¬ | **3.12+** | é¡¹ç›®è¦æ±‚ï¼ˆä½¿ç”¨ PEP 695 æ³›å‹è¯­æ³•ï¼‰ |
| Agent æ¡†æ¶ | Microsoft Agent Framework `1.0.0b260130` | ä¸é¡¹ç›®ç»Ÿä¸€ |
| LLM | é˜¿é‡Œäº‘ DashScope `qwen-plus` | é€šè¿‡ OpenAI å…¼å®¹æ¥å£è°ƒç”¨ |
| ç½‘ç»œæœç´¢ | Tavily Search API | è¿”å›ç»“æ„åŒ–å†…å®¹æ‘˜è¦ï¼Œæ— éœ€çˆ¬å– |
| æ•°æ®æ¨¡å‹ | Pydantic v2 | ç»“æ„åŒ–è¾“å‡ºæ ¡éªŒ |
| å¼‚æ­¥æ¡†æ¶ | asyncio | å¹¶å‘æ‰§è¡Œæœç´¢ |

### 1.4 é…ç½®ä¾èµ–

æ‰€æœ‰ API Key æ¥è‡ªé¡¹ç›®æ ¹ç›®å½• `.env` æ–‡ä»¶ï¼š

```
DASHSCOPE_API_KEY=sk-xxxx          # é˜¿é‡Œäº‘ DashScope API Key
DASHSCOPE_BASE_URL=https://dashscope.aliyuncs.com/compatible-mode/v1
TAVILY_API_KEY=tvly-xxxx           # Tavily Search API Key
```

---

## äºŒã€æ¶æ„è®¾è®¡

### 2.1 æ•´ä½“æµç¨‹

```
                          è¾“å…¥: symbol, name, industry
                                    â”‚
                                    â–¼
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚   å®šä¹‰ 5 ä¸ªæœç´¢ä¸»é¢˜ (topics)    â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â”‚
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â–¼         â–¼           â–¼           â–¼         â–¼
          â”Œâ”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”
          â”‚ T1    â”‚ â”‚ T2    â”‚ â”‚ T3    â”‚ â”‚ T4    â”‚ â”‚ T5    â”‚
          â”‚è¿‘æœŸ   â”‚ â”‚ç«äº‰   â”‚ â”‚è¡Œä¸š   â”‚ â”‚é£é™©   â”‚ â”‚æœºæ„   â”‚
          â”‚æ–°é—»   â”‚ â”‚ä¼˜åŠ¿   â”‚ â”‚å‰æ™¯   â”‚ â”‚äº‹ä»¶   â”‚ â”‚è§‚ç‚¹   â”‚
          â””â”€â”€â”€â”¬â”€â”€â”€â”˜ â””â”€â”€â”€â”¬â”€â”€â”€â”˜ â””â”€â”€â”€â”¬â”€â”€â”€â”˜ â””â”€â”€â”€â”¬â”€â”€â”€â”˜ â””â”€â”€â”€â”¬â”€â”€â”€â”˜
              â”‚         â”‚         â”‚         â”‚         â”‚
              â–¼         â–¼         â–¼         â–¼         â–¼
          deep_research(breadth=3, depth=2)  Ã—5 å¹¶è¡Œ
              â”‚
              â”‚  æ¯ä¸ªä¸»é¢˜å†…éƒ¨é€’å½’ï¼š
              â”‚  â”Œâ”€ ç¬¬1å±‚: breadth=3, ç”Ÿæˆ3ä¸ªæŸ¥è¯¢ â†’ Tavilyæœç´¢ â†’ æå–learnings
              â”‚  â””â”€ ç¬¬2å±‚: breadth=1, æ¯åˆ†æ”¯1ä¸ªæŸ¥è¯¢ â†’ Tavilyæœç´¢ â†’ æå–learnings
              â”‚
              â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  åˆå¹¶å»é‡æ‰€æœ‰ learnings      â”‚
    â”‚  (~50-70 ä¸ªç‹¬ç‰¹çŸ¥è¯†ç‚¹)       â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  report_agent (ChatAgent)   â”‚
    â”‚  ç»¼åˆç”Ÿæˆç»“æ„åŒ–æ‘˜è¦æŠ¥å‘Š       â”‚
    â”‚  â†’ WebResearchResult        â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  å¼ºåˆ¶é™çº§æ£€æŸ¥                â”‚
    â”‚  å¦‚æœ learnings < 5:        â”‚
    â”‚  search_confidence = "ä½"   â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â–¼
         è¿”å› Pydantic æ¨¡å‹å¯¹è±¡
         (è°ƒç”¨æ–¹å¯é€‰æ‹©æ˜¯å¦ä¿å­˜ä¸º JSON)
```

### 2.2 æ ¸å¿ƒç»„ä»¶

| ç»„ä»¶ | ç±»å‹ | èŒè´£ |
|------|------|------|
| `query_agent` | MAF `ChatAgent` | æ ¹æ®ç ”ç©¶ä¸»é¢˜ + å·²æœ‰ learnings ç”Ÿæˆæœç´¢æŸ¥è¯¢ |
| `extract_agent` | MAF `ChatAgent` | ä» Tavily æœç´¢ç»“æœä¸­æå– learnings å’Œ follow-up questions |
| `report_agent` | MAF `ChatAgent` | å°†æ‰€æœ‰ learnings æ•´åˆä¸ºç»“æ„åŒ–æŠ•èµ„ç ”ç©¶æŠ¥å‘Š |
| `tavily_search()` | å¼‚æ­¥å‡½æ•° | å°è£… Tavily API è°ƒç”¨ |
| `deep_research()` | å¼‚æ­¥é€’å½’å‡½æ•° | æ ¸å¿ƒé€’å½’é€»è¾‘ï¼Œç¼–æ’æœç´¢-æå–-æ·±å…¥çš„å¾ªç¯ |
| `run_web_research()` | å¼‚æ­¥å…¥å£å‡½æ•° | æ¨¡å—Bå¯¹å¤–å…¥å£ï¼Œç¼–æ’5ä¸ªä¸»é¢˜çš„å¹¶è¡Œ Deep Research |

### 2.3 è®¾è®¡åŸåˆ™

1. **Agent èŒè´£å•ä¸€**ï¼šæ¯ä¸ª ChatAgent åªåšä¸€ä»¶äº‹ï¼ˆç”ŸæˆæŸ¥è¯¢ / æå–çŸ¥è¯† / å†™æŠ¥å‘Šï¼‰
2. **ä¸ä½¿ç”¨ Agent å·¥å…·è°ƒç”¨**ï¼šDeep Research çš„é€’å½’é€»è¾‘ç”± Python ä»£ç æ§åˆ¶ï¼ŒAgent åªè´Ÿè´£æ–‡æœ¬æ¨ç†ï¼ŒTavily æœç´¢ç”±ä»£ç ç›´æ¥è°ƒç”¨
3. **æ‰€æœ‰ Agent å…±äº«åŒä¸€ä¸ª LLM Client**ï¼šå¤ç”¨ `OpenAIChatClient` å®ä¾‹ï¼Œå‡å°‘è¿æ¥å¼€é”€
4. **ç»“æ„åŒ–è¾“å‡º**ï¼šæ‰€æœ‰ Agent è¾“å‡ºå‡é€šè¿‡ `response_format: {"type": "json_object"}` + Pydantic æ ¡éªŒ

### 2.4 å¹¶å‘æ§åˆ¶ç­–ç•¥

æœ¬æ¨¡å—é‡‡ç”¨**ä¸¤çº§å¹¶å‘æ§åˆ¶**ï¼š

#### ä¸»é¢˜çº§åˆ«ï¼ˆæ˜¾å¼æ§åˆ¶ï¼‰

```python
TOPIC_CONCURRENCY_LIMIT = 3  # æœ€å¤š 3 ä¸ªä¸»é¢˜åŒæ—¶æ‰§è¡Œ
```

- ä½¿ç”¨ `asyncio.Semaphore` æ˜¾å¼æ§åˆ¶
- é¿å… 5 ä¸ªä¸»é¢˜åŒæ—¶è°ƒç”¨ Tavily å¯¼è‡´é™æµ

#### æŸ¥è¯¢çº§åˆ«ï¼ˆéšå¼æ§åˆ¶ï¼‰

```python
DEFAULT_BREADTH = 3  # ç¬¬1å±‚ï¼šæ¯ä¸»é¢˜ 3 ä¸ªæŸ¥è¯¢å¹¶å‘
DEFAULT_DEPTH = 2    # ç¬¬2å±‚ï¼šbreadth å‡åŠä¸º 1
```

- **ä¸ä½¿ç”¨** Semaphoreï¼Œä¾èµ– `breadth` å‚æ•°è‡ªç„¶æ§åˆ¶
- ç¬¬1å±‚ï¼š3 ä¸ªæŸ¥è¯¢å¹¶å‘ï¼ˆæ¯ä¸»é¢˜ï¼‰
- ç¬¬2å±‚ï¼š1 ä¸ªæŸ¥è¯¢/åˆ†æ”¯ï¼ˆbreadth å‡åŠï¼‰
- **å®é™…å¹¶å‘å³°å€¼**ï¼š3 ä¸ªä¸»é¢˜ Ã— 3 ä¸ªæŸ¥è¯¢ = **9 ä¸ª Tavily è°ƒç”¨**

#### ä¸ºä»€ä¹ˆä¸éœ€è¦æŸ¥è¯¢çº§åˆ« Semaphoreï¼Ÿ

| ç†ç”± | è¯´æ˜ |
|------|------|
| âœ… breadth è‡ªç„¶é€’å‡ | 3â†’1â†’1ï¼Œè¶Šæ·±å¹¶å‘è¶Šå°‘ |
| âœ… ä¸»é¢˜çº§åˆ«å·²é™æµ | æœ€å¤š 3 ä¸ªä¸»é¢˜å¹¶è¡Œ |
| âœ… å³°å€¼å¯æ§ | 9 ä¸ªå¹¶å‘è°ƒç”¨åœ¨ Tavily æ‰¿å—èŒƒå›´å†… |
| âœ… ç®€åŒ–ä»£ç  | é¿å… Semaphore åœ¨é€’å½’ä¸­ä¼ é€’ |

**å¦‚æœé‡åˆ° Tavily é™æµï¼š** å¯é™ä½ `TOPIC_CONCURRENCY_LIMIT` æˆ– `DEFAULT_BREADTH`ã€‚

---

## ä¸‰ã€é…ç½®æ¨¡å—

### 3.1 config.py

```python
"""æ¨¡å—Bé…ç½®"""
import os
from dotenv import load_dotenv

load_dotenv()

# â”€â”€ DashScope / Qwen â”€â”€
DASHSCOPE_API_KEY: str = os.getenv("DASHSCOPE_API_KEY", "")
DASHSCOPE_BASE_URL: str = os.getenv(
    "DASHSCOPE_BASE_URL",
    "https://dashscope.aliyuncs.com/compatible-mode/v1",
)

# â”€â”€ Tavily â”€â”€
TAVILY_API_KEY: str = os.getenv("TAVILY_API_KEY", "")

# â”€â”€ æ¨¡å‹é€‰æ‹© â”€â”€
MODEL_QUERY_AGENT: str = "qwen-plus"     # ç”Ÿæˆæœç´¢æŸ¥è¯¢
MODEL_EXTRACT_AGENT: str = "qwen-plus"   # æå–çŸ¥è¯†ç‚¹
MODEL_REPORT_AGENT: str = "qwen-plus"    # ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š

# â”€â”€ Deep Research å‚æ•° â”€â”€
DEFAULT_BREADTH: int = 3     # æ¯è½®å¹¶è¡ŒæŸ¥è¯¢æ•°ï¼ˆç¬¬1å±‚ï¼‰
DEFAULT_DEPTH: int = 2       # é€’å½’æ·±åº¦ï¼ˆå±‚æ•°ï¼‰
TAVILY_MAX_RESULTS: int = 5  # æ¯æ¬¡ Tavily æœç´¢è¿”å›çš„æœ€å¤§ç»“æœæ•°

# â”€â”€ è¶…æ—¶ä¸å¹¶å‘ â”€â”€
API_TIMEOUT: float = 120.0           # LLM è°ƒç”¨è¶…æ—¶ï¼ˆç§’ï¼‰
TAVILY_TIMEOUT: float = 30.0         # Tavily è°ƒç”¨è¶…æ—¶ï¼ˆç§’ï¼‰
TOPIC_CONCURRENCY_LIMIT: int = 3     # ä¸»é¢˜å¹¶è¡Œæ•°ä¸Šé™ï¼ˆé¿å… Tavily é™æµï¼‰
# æ³¨ï¼šæŸ¥è¯¢çº§åˆ«ä¸éœ€è¦é¢å¤–é™æµï¼Œå› ä¸ºï¼š
#   1. breadth è‡ªç„¶é€’å‡ï¼ˆ3â†’1â†’1ï¼‰æ§åˆ¶äº†æ¯å±‚æŸ¥è¯¢æ•°
#   2. TOPIC_CONCURRENCY_LIMIT é™åˆ¶äº†ä¸»é¢˜å¹¶è¡Œæ•°
#   3. å®é™…å¹¶å‘å³°å€¼ = 3ä¸»é¢˜ Ã— 3æŸ¥è¯¢ = 9ä¸ª Tavily è°ƒç”¨ï¼ˆå¯æ¥å—ï¼‰

# â”€â”€ æ—¥å¿—é…ç½® â”€â”€
LOG_LEVEL_CONSOLE: str = "INFO"      # æ§åˆ¶å°æ—¥å¿—çº§åˆ«
LOG_LEVEL_FILE: str = "DEBUG"        # æ–‡ä»¶æ—¥å¿—çº§åˆ«
LOG_FILE_PATH: str = "logs/stock_analyzer.log"  # æ—¥å¿—æ–‡ä»¶è·¯å¾„
```

### 3.2 æ—¥å¿—é…ç½®

```python
"""æ—¥å¿—é…ç½® â€” ç»Ÿä¸€çš„æ—¥å¿—ç®¡ç†å™¨"""
import logging
import sys
from pathlib import Path
from stock_analyzer.config import (
    LOG_LEVEL_CONSOLE,
    LOG_LEVEL_FILE,
    LOG_FILE_PATH,
)


def setup_logger(name: str = "stock_analyzer") -> logging.Logger:
    """
    é…ç½®æ¨¡å—ç»Ÿä¸€çš„æ—¥å¿—è®°å½•å™¨ã€‚
    
    ç‰¹æ€§ï¼š
    - æ§åˆ¶å°è¾“å‡ºï¼šINFO çº§åˆ«ï¼ˆå¯é…ç½®ï¼‰ï¼Œæ ¼å¼ç®€æ´
    - æ–‡ä»¶è¾“å‡ºï¼šDEBUG çº§åˆ«ï¼ˆå¯é…ç½®ï¼‰ï¼Œæ ¼å¼è¯¦ç»†ï¼ŒåŒ…å«æ—¶é—´ã€æ¨¡å—ã€è¡Œå·
    - è‡ªåŠ¨åˆ›å»ºæ—¥å¿—ç›®å½•
    
    Args:
        name: Logger åç§°ï¼Œé»˜è®¤ "stock_analyzer"
    
    Returns:
        é…ç½®å¥½çš„ Logger å®ä¾‹
    """
    logger = logging.getLogger(name)
    
    # é¿å…é‡å¤é…ç½®
    if logger.handlers:
        return logger
    
    logger.setLevel(logging.DEBUG)  # Logger æœ¬èº«è®¾ä¸ºæœ€ä½çº§åˆ«
    logger.propagate = False
    
    # â”€â”€ æ§åˆ¶å° Handlerï¼ˆINFO çº§åˆ«ï¼‰â”€â”€
    console_handler = logging.StreamHandler(sys.stdout)
    console_handler.setLevel(getattr(logging, LOG_LEVEL_CONSOLE.upper()))
    console_formatter = logging.Formatter(
        fmt="%(levelname)s - %(name)s - %(message)s",
        datefmt="%H:%M:%S",
    )
    console_handler.setFormatter(console_formatter)
    logger.addHandler(console_handler)
    
    # â”€â”€ æ–‡ä»¶ Handlerï¼ˆDEBUG çº§åˆ«ï¼‰â”€â”€
    log_file = Path(LOG_FILE_PATH)
    log_file.parent.mkdir(parents=True, exist_ok=True)
    
    file_handler = logging.FileHandler(log_file, encoding="utf-8")
    file_handler.setLevel(getattr(logging, LOG_LEVEL_FILE.upper()))
    file_formatter = logging.Formatter(
        fmt="%(asctime)s - %(levelname)s - %(name)s - %(module)s:%(lineno)d - %(message)s",
        datefmt="%Y-%m-%d %H:%M:%S",
    )
    file_handler.setFormatter(file_formatter)
    logger.addHandler(file_handler)
    
    return logger


# æ¨¡å—çº§åˆ«çš„é»˜è®¤ Logger
logger = setup_logger()
```

### 3.3 LLM Client åˆå§‹åŒ–

```python
"""LLM Client å·¥å‚"""
from openai import AsyncOpenAI
from agent_framework.openai import OpenAIChatClient
from stock_analyzer.config import (
    DASHSCOPE_API_KEY, DASHSCOPE_BASE_URL, API_TIMEOUT,
    MODEL_QUERY_AGENT, MODEL_EXTRACT_AGENT, MODEL_REPORT_AGENT,
)


def create_openai_client() -> AsyncOpenAI:
    """åˆ›å»ºå…±äº«çš„ AsyncOpenAI å®¢æˆ·ç«¯ï¼ˆDashScope å…¼å®¹æ¨¡å¼ï¼‰"""
    return AsyncOpenAI(
        api_key=DASHSCOPE_API_KEY,
        base_url=DASHSCOPE_BASE_URL,
        timeout=API_TIMEOUT,
    )


def create_chat_client(
    openai_client: AsyncOpenAI,
    model_id: str,
) -> OpenAIChatClient:
    """åˆ›å»º MAF OpenAIChatClient"""
    return OpenAIChatClient(
        model_id=model_id,
        async_client=openai_client,
    )
```

---

## å››ã€Pydantic æ•°æ®æ¨¡å‹

### 4.1 å†…éƒ¨ä¸­é—´æ¨¡å‹

```python
"""Deep Research è¿‡ç¨‹ä¸­ä½¿ç”¨çš„ä¸­é—´æ•°æ®æ¨¡å‹"""
from pydantic import BaseModel, Field


class SerpQuery(BaseModel):
    """LLM ç”Ÿæˆçš„å•ä¸ªæœç´¢æŸ¥è¯¢"""
    query: str = Field(description="æœç´¢å¼•æ“æŸ¥è¯¢è¯ï¼Œåº”å…·ä½“ä¸”é€‚åˆæœç´¢å¼•æ“")
    research_goal: str = Field(description="è¯¥æŸ¥è¯¢çš„ç ”ç©¶ç›®æ ‡ï¼Œè¯´æ˜æœŸæœ›å‘ç°ä»€ä¹ˆä¿¡æ¯")


class SerpQueryList(BaseModel):
    """generate_serp_queries çš„è¾“å‡º"""
    queries: list[SerpQuery] = Field(description="æœç´¢æŸ¥è¯¢åˆ—è¡¨")


class ProcessedResult(BaseModel):
    """process_serp_result çš„è¾“å‡ºï¼šä»æœç´¢ç»“æœä¸­æå–çš„çŸ¥è¯†"""
    learnings: list[str] = Field(
        description="ä»æœç´¢ç»“æœä¸­æå–çš„çŸ¥è¯†ç‚¹ï¼Œåº”ä¿¡æ¯å¯†é›†ï¼ŒåŒ…å«å®ä½“ã€æ•°å­—ã€æ—¥æœŸ"
    )
    follow_up_questions: list[str] = Field(
        description="å€¼å¾—ç»§ç»­æ·±å…¥æœç´¢çš„è¿½é—®æ–¹å‘"
    )


class ResearchResult(BaseModel):
    """å•ä¸ªä¸»é¢˜çš„ Deep Research æœ€ç»ˆç»“æœ"""
    learnings: list[str] = Field(default_factory=list)
    visited_urls: list[str] = Field(default_factory=list)
```

### 4.2 æ¨¡å—è¾“å‡ºæ¨¡å‹

```python
"""æ¨¡å—Bæœ€ç»ˆè¾“å‡ºæ¨¡å‹ï¼ˆä¸æ¦‚è¦è®¾è®¡ v3.1 å¯¹é½ï¼‰"""
from pydantic import BaseModel, Field
from typing import Literal


class NewsItem(BaseModel):
    """å•æ¡æ–°é—»"""
    title: str = Field(description="æ–°é—»æ ‡é¢˜")
    summary: str = Field(description="æ–°é—»å†…å®¹æ‘˜è¦ï¼Œ50-100å­—")
    source: str = Field(description="ä¿¡æ¯æ¥æº")
    date: str = Field(description="æ–°é—»æ—¥æœŸï¼Œæ ¼å¼ YYYY-MM-DD")
    importance: Literal["é«˜", "ä¸­", "ä½"] = Field(description="é‡è¦æ€§ç­‰çº§")


class NewsSummary(BaseModel):
    """æ–°é—»æ‘˜è¦ï¼Œæ­£é¢å’Œè´Ÿé¢åˆ†å¼€"""
    positive: list[NewsItem] = Field(default_factory=list, description="æ­£é¢æ–°é—»")
    negative: list[NewsItem] = Field(default_factory=list, description="è´Ÿé¢æ–°é—»")
    neutral: list[NewsItem] = Field(default_factory=list, description="ä¸­æ€§æ–°é—»")


class CompetitiveAdvantage(BaseModel):
    """å…¬å¸ç«äº‰ä¼˜åŠ¿"""
    description: str = Field(
        max_length=500, 
        description="ç«äº‰ä¼˜åŠ¿ç»¼è¿°ï¼ˆé™çº§æŠ¥å‘Šé‡‡ç”¨ç»Ÿä¸€æˆªæ–­ç­–ç•¥ç¡®ä¿ä¸è¶…è¿‡æ­¤é™åˆ¶ï¼‰"
    )
    moat_type: str = Field(description="æŠ¤åŸæ²³ç±»å‹ï¼Œå¦‚ï¼šå“ç‰Œ+æ¸ é“+ç§‘æŠ€")
    market_position: str = Field(description="å¸‚åœºåœ°ä½æè¿°")


class IndustryOutlook(BaseModel):
    """è¡Œä¸šå‰æ™¯"""
    industry: str = Field(description="è¡Œä¸šåç§°")
    outlook: str = Field(description="å‰æ™¯åˆ¤æ–­ï¼šä¹è§‚/ä¸­æ€§åç§¯æ/ä¸­æ€§/ä¸­æ€§åæ¶ˆæ/æ‚²è§‚")
    key_drivers: list[str] = Field(description="ä¸»è¦é©±åŠ¨å› ç´ ")
    key_risks: list[str] = Field(description="ä¸»è¦é£é™©å› ç´ ")


class RiskEvents(BaseModel):
    """é£é™©äº‹ä»¶"""
    regulatory: str = Field(description="ç›‘ç®¡å¤„ç½šæƒ…å†µ")
    litigation: str = Field(description="è¯‰è®¼æƒ…å†µ")
    management: str = Field(description="ç®¡ç†å±‚å˜åŠ¨æƒ…å†µ")
    other: str = Field(default="", description="å…¶ä»–é£é™©")


class AnalystReport(BaseModel):
    """å•æ¡æœºæ„ç ”æŠ¥"""
    broker: str = Field(description="åˆ¸å•†/æœºæ„åç§°")
    rating: str = Field(description="è¯„çº§ï¼šä¹°å…¥/å¢æŒ/ä¸­æ€§/å‡æŒ/å–å‡º")
    target_price: float | None = Field(default=None, description="ç›®æ ‡ä»·")
    date: str = Field(description="ç ”æŠ¥æ—¥æœŸ")


class AnalystOpinions(BaseModel):
    """æœºæ„è§‚ç‚¹æ±‡æ€»"""
    buy_count: int = Field(default=0, description="ä¹°å…¥/å¢æŒè¯„çº§æ•°é‡")
    hold_count: int = Field(default=0, description="ä¸­æ€§/æŒæœ‰è¯„çº§æ•°é‡")
    sell_count: int = Field(default=0, description="å‡æŒ/å–å‡ºè¯„çº§æ•°é‡")
    average_target_price: float | None = Field(default=None, description="å¹³å‡ç›®æ ‡ä»·")
    recent_reports: list[AnalystReport] = Field(
        default_factory=list, description="è¿‘æœŸç ”æŠ¥åˆ—è¡¨"
    )


class SearchConfig(BaseModel):
    """æœç´¢å‚æ•°é…ç½®ï¼ˆç»“æ„åŒ–ï¼Œé¿å…è£¸ dict çš„ KeyError é£é™©ï¼‰"""
    topics_count: int = Field(description="æœç´¢ä¸»é¢˜æ€»æ•°")
    breadth: int = Field(description="æ¯ä¸»é¢˜æ¯è½®æŸ¥è¯¢æ•°")
    depth: int = Field(description="æ¯ä¸»é¢˜é€’å½’æ·±åº¦")
    successful_topics: int = Field(description="æˆåŠŸä¸»é¢˜æ•°ï¼ˆæœ‰ learnings çš„ä¸»é¢˜ï¼‰")


class SearchMeta(BaseModel):
    """æœç´¢å…ƒä¿¡æ¯"""
    symbol: str
    name: str
    search_time: str
    search_config: SearchConfig = Field(description="æœç´¢å‚æ•°é…ç½®")
    total_learnings: int = Field(description="å»é‡åçš„çŸ¥è¯†ç‚¹æ€»æ•°")
    total_sources_consulted: int = Field(description="è®¿é—®çš„ä¿¡æ¯æºæ€»æ•°")
    raw_learnings: list[str] | None = Field(
        default=None,
        description="åŸå§‹ learnings åˆ—è¡¨ï¼ˆä»…åœ¨é™çº§æŠ¥å‘Šæ—¶å¡«å……ï¼Œä¾¿äºåç»­äººå·¥åˆ†æï¼‰"
    )


class WebResearchResult(BaseModel):
    """æ¨¡å—Bæœ€ç»ˆè¾“å‡ºï¼šç½‘ç»œæ·±åº¦æœç´¢ç ”ç©¶æŠ¥å‘Š"""
    meta: SearchMeta
    news_summary: NewsSummary
    competitive_advantage: CompetitiveAdvantage
    industry_outlook: IndustryOutlook
    risk_events: RiskEvents
    analyst_opinions: AnalystOpinions
    search_confidence: Literal["é«˜", "ä¸­", "ä½"] = Field(
        description="æœç´¢ä¿¡æ¯çš„æ•´ä½“å¯ä¿¡åº¦"
    )
```

---

## äº”ã€æ ¸å¿ƒç»„ä»¶è¯¦ç»†è®¾è®¡

### 5.1 Tavily Search å°è£…

```python
"""Tavily Search API å°è£…

âš ï¸ Tavily å¼‚å¸¸å¯¼å…¥è§„èŒƒï¼ˆé¿å… NameErrorï¼‰ï¼š

ã€é€šç”¨è§„åˆ™ã€‘é€‚ç”¨äºå¤§å¤šæ•° Tavily å¼‚å¸¸ï¼š
- âœ… æ­£ç¡®ï¼šfrom tavily import InvalidAPIKeyError  â†’  ä½¿ç”¨ InvalidAPIKeyError
- âŒ é”™è¯¯ï¼šfrom tavily import InvalidAPIKeyError  â†’  ä½¿ç”¨ tavily.InvalidAPIKeyErrorï¼ˆNameErrorï¼‰

ã€ç‰¹ä¾‹è§„åˆ™ã€‘ä»…ç”¨äºå‘½åå†²çªï¼ˆå¦‚ TimeoutError ä¸æ ‡å‡†åº“é‡åï¼‰ï¼š
- ä½¿ç”¨å‘½åç©ºé—´å¯¼å…¥ï¼šfrom tavily import errors as tavily_errors  â†’  ä½¿ç”¨ tavily_errors.TimeoutError
- æ³¨æ„ï¼šæœ¬é¡¹ç›® TimeoutError å·²ç¡®è®¤ä¸å¯ç”¨ï¼Œæ­¤è§„åˆ™ä»…ä¾›å‚è€ƒ

ğŸ“– ä¾æ®ï¼šå®˜æ–¹æºç  v0.7.21 https://github.com/tavily-ai/tavily-python/blob/38627afb7b88d8a57bad29380896210a9ae7badd/tavily/__init__.py
"""
import asyncio
import httpx  # â† é¡¹ç›®ä¾èµ– httpxï¼Œå¿…é¡»å¯¼å…¥
from tavily import AsyncTavilyClient
# Tavily å¼‚å¸¸ç±»å¯¼å…¥ï¼ˆâœ… å·²ç¡®è®¤å¯ç”¨ï¼ŒåŸºäºå®˜æ–¹æºç  v0.7.21ï¼‰
# å¯¼å…¥è‡ª tavily.__init__.pyï¼ˆå·²é‡æ–°å¯¼å‡ºè‡ª tavily.errorsï¼‰
# âš ï¸ æ³¨æ„ï¼šä»£ç ä¸­ä½¿ç”¨æ—¶ç›´æ¥ç”¨ç±»åï¼Œä¸è¦åŠ  tavily. å‰ç¼€
from tavily import (
    InvalidAPIKeyError,
    MissingAPIKeyError, 
    BadRequestError,
    ForbiddenError,
    UsageLimitExceededError,
)

from stock_analyzer.config import TAVILY_API_KEY, TAVILY_MAX_RESULTS, TAVILY_TIMEOUT
from stock_analyzer.exceptions import TavilySearchError
from stock_analyzer.logger import logger

# ============================================================
# å¼‚å¸¸åˆ†ç±»ï¼šå¯é‡è¯• vs ä¸å¯é‡è¯•
# ============================================================
# 
# è®¾è®¡ç›®æ ‡ï¼šæ‰€æœ‰ç½‘ç»œå±‚ç¬æ—¶æ•…éšœåº”é‡è¯•ï¼Œé…ç½®/é€»è¾‘é”™è¯¯ä¸é‡è¯•
# 
# å®æ–½è¦æ±‚ï¼š
# 1. æ ¹æ®é¡¹ç›®å®é™…ä¾èµ–çš„ HTTP åº“å¯ç”¨ç›¸åº”å¼‚å¸¸ï¼ˆä¸è¦ç•™"å¯é€‰"æ³¨é‡Šï¼‰
# 2. å¦‚æœ Tavily SDK æä¾›äº†ç‰¹å®šå¼‚å¸¸ç±»å‹ï¼Œå¿…é¡»æ˜ç¡®æ·»åŠ 
# 3. å®šæœŸå®¡æŸ¥ï¼šæ–°å¢ä¾èµ–æ—¶åŒæ­¥æ›´æ–°æ­¤åˆ—è¡¨
# ============================================================

# å¯é‡è¯•å¼‚å¸¸ï¼šç½‘ç»œå±‚ç¬æ—¶æ•…éšœï¼ˆä¼šè‡ªåŠ¨é‡è¯• max_retries æ¬¡ï¼‰
# 
# æ³¨æ„ï¼šasyncio.TimeoutError å•ç‹¬å¤„ç†ï¼ˆæœ‰ä¸“é—¨çš„ except åˆ†æ”¯ï¼‰ï¼Œä¸åœ¨æ­¤å…ƒç»„ä¸­
# è¿™æ ·å¯ä»¥ä¸ºè¶…æ—¶å¼‚å¸¸æä¾›æ›´æ¸…æ™°çš„æ—¥å¿—ï¼Œå¹¶é¿å…é‡å¤åŒ¹é…
RETRYABLE_EXCEPTIONS = (
    # Python æ ‡å‡†åº“ç½‘ç»œå¼‚å¸¸
    ConnectionError,       # è¿æ¥å¤±è´¥ï¼ˆå« ConnectionRefusedError, ConnectionResetError ç­‰ï¼‰
    OSError,               # åº•å±‚ I/O é”™è¯¯ï¼ˆå«éƒ¨åˆ†ç½‘ç»œé”™è¯¯ï¼‰
    
    # ============================================================
    # é¡¹ç›®å®é™…ä¾èµ–ï¼šhttpx (pyproject.toml å·²ç¡®è®¤)
    # ============================================================
    # 
    # âœ… å¿…é¡»å¯ç”¨ï¼šé¡¹ç›®ä¾èµ– httpx>=0.28.1
    httpx.ConnectError,         # httpx è¿æ¥é”™è¯¯ï¼ˆå« ConnectTimeoutï¼‰
    httpx.NetworkError,         # httpx ç½‘ç»œé”™è¯¯ï¼ˆæ‰€æœ‰ç½‘ç»œå±‚é”™è¯¯çš„åŸºç±»ï¼‰
    httpx.TimeoutException,     # httpx è¶…æ—¶ï¼ˆReadTimeout, WriteTimeout, PoolTimeoutï¼‰
    httpx.RemoteProtocolError,  # è¿œç¨‹åè®®é”™è¯¯ï¼ˆå¦‚ HTTP/2 é”™è¯¯ï¼‰
    
    # ============================================================
    # Tavily SDK ç½‘ç»œ/è¶…æ—¶å¼‚å¸¸ï¼ˆâœ… å·²ç¡®è®¤ï¼ŒåŸºäºå®˜æ–¹æºç  v0.7.21ï¼‰
    # æºç ï¼šgithub.com/tavily-ai/tavily-python/blob/38627afb7b88d8a57bad29380896210a9ae7badd/tavily/errors.py
    # ============================================================
    # 
    # âš ï¸ å½“å‰ä¿æŒæ³¨é‡Šæ€ï¼šTavily TimeoutError ä¸æ ‡å‡†åº“ TimeoutError é‡å
    # 
    # å¦‚éœ€å¯ç”¨ï¼Œå»ºè®®ä½¿ç”¨å‘½åç©ºé—´å¯¼å…¥ï¼ˆâš ï¸ ä»…æ­¤ç‰¹ä¾‹ï¼Œä¸æ”¹å˜å…¶ä»–å¼‚å¸¸çš„é€šç”¨å¯¼å…¥è§„èŒƒï¼‰ï¼š
    # from tavily import errors as tavily_errors
    # tavily_errors.TimeoutError,     # Tavily è¯·æ±‚è¶…æ—¶ï¼ˆå‘½åç©ºé—´å†™æ³•é¿å…ä¸æ ‡å‡†åº“å†²çªï¼‰
    # 
    # ğŸ’¡ ç‰¹ä¾‹è¯´æ˜ï¼šè¯¥å‘½åç©ºé—´å†™æ³•ä»…ç”¨äºé‡åå†²çªåœºæ™¯ï¼Œå…¶ä»– Tavily å¼‚å¸¸ä»ä¼˜å…ˆç›´æ¥å¯¼å…¥ç±»å
    # 
    # âŒ NetworkErrorï¼šåœ¨ Tavily SDK ä¸­ä¸å­˜åœ¨ï¼ˆå·²ä»å®˜æ–¹æºç ç¡®è®¤ï¼Œä¸è¦æ·»åŠ ï¼‰
)

# ä¸å¯é‡è¯•å¼‚å¸¸ï¼šé…ç½®é”™è¯¯ã€API é”™è¯¯ï¼ˆç«‹å³å¤±è´¥ï¼Œä¸é‡è¯•ï¼‰
# 
# ç”¨é€”ï¼šæä¾›æ›´æ˜ç¡®çš„é”™è¯¯æ—¥å¿—å’Œåˆ†ç±»
# 
# ============================================================
# å¼‚å¸¸æ¥æºå·²ç¡®è®¤ï¼ˆåŸºäº Tavily SDK v0.7.21 å®˜æ–¹æºç ï¼‰
# æºç ä½ç½®ï¼šgithub.com/tavily-ai/tavily-python/blob/38627afb7b88d8a57bad29380896210a9ae7badd/tavily/errors.py
# å¯¼å…¥æ–¹å¼ï¼šfrom tavily import InvalidAPIKeyError, BadRequestError, ...
# ============================================================
NON_RETRYABLE_EXCEPTIONS = (
    # HTTP å±‚ API é”™è¯¯
    httpx.HTTPStatusError,  # HTTP 4xx/5xx çŠ¶æ€ç é”™è¯¯ï¼ˆé¡¹ç›®ä¾èµ– httpxï¼‰
    
    # Tavily SDK ç‰¹å®š API é”™è¯¯ï¼ˆâœ… å·²ç¡®è®¤å¯ç”¨ï¼ŒåŸºäºå®˜æ–¹æºç  v0.7.21ï¼‰
    # âš ï¸ æ³¨æ„ï¼šå¿…é¡»ä½¿ç”¨ç›´æ¥å¯¼å…¥çš„ç±»åï¼ˆè§é¡¶éƒ¨ import è¯­å¥ï¼‰ï¼Œä¸è¦ä½¿ç”¨ tavily.å¼‚å¸¸å
    InvalidAPIKeyError,       # API å¯†é’¥æ— æ•ˆ
    MissingAPIKeyError,       # ç¼ºå°‘ API å¯†é’¥
    BadRequestError,          # è¯·æ±‚æ ¼å¼é”™è¯¯
    ForbiddenError,           # æƒé™ä¸è¶³
    UsageLimitExceededError,  # ä½¿ç”¨é™åˆ¶è¶…å‡ºï¼ˆé…é¢ä¸è¶³ï¼‰
)

# æ¨¡å—çº§åˆ«å•ä¾‹
_tavily_client: AsyncTavilyClient | None = None


def get_tavily_client() -> AsyncTavilyClient:
    """è·å– Tavily å®¢æˆ·ç«¯å•ä¾‹"""
    global _tavily_client
    if _tavily_client is None:
        _tavily_client = AsyncTavilyClient(api_key=TAVILY_API_KEY)
    return _tavily_client


async def tavily_search(
    query: str,
    max_results: int = TAVILY_MAX_RESULTS,
    max_retries: int = 1,
) -> list[dict]:
    """
    è°ƒç”¨ Tavily Search API æ‰§è¡Œå•æ¬¡æœç´¢ï¼Œå¸¦è¶…æ—¶å’Œé‡è¯•ã€‚

    Args:
        query: æœç´¢æŸ¥è¯¢è¯
        max_results: è¿”å›ç»“æœæ•°ä¸Šé™
        max_retries: å¤±è´¥åçš„æœ€å¤§é‡è¯•æ¬¡æ•°ï¼ˆé»˜è®¤1æ¬¡ï¼‰

    Returns:
        æœç´¢ç»“æœåˆ—è¡¨ï¼Œæ¯é¡¹åŒ…å«:
        - title: str   é¡µé¢æ ‡é¢˜
        - url: str     é¡µé¢URL
        - content: str é¡µé¢å†…å®¹æ‘˜è¦ï¼ˆTavily æå–ï¼‰
        - score: float ç›¸å…³æ€§è¯„åˆ†

    Raises:
        TavilySearchError: æœç´¢å¤±è´¥ï¼ˆå«é‡è¯•åä»å¤±è´¥çš„æƒ…å†µï¼‰
    """
    client = get_tavily_client()
    last_error = None
    
    for attempt in range(max_retries + 1):
        try:
            # ä½¿ç”¨ asyncio.wait_for å®ç°è¶…æ—¶æ§åˆ¶
            response = await asyncio.wait_for(
                client.search(
                    query=query,
                    max_results=max_results,
                    search_depth="advanced",
                    include_answer=False,
                    include_raw_content=False,
                ),
                timeout=TAVILY_TIMEOUT,
            )
            results = response.get("results", [])
            
            if attempt > 0:
                logger.info(
                    f"Tavily search '{query[:50]}...' succeeded on retry {attempt}, "
                    f"returned {len(results)} results"
                )
            else:
                logger.info(
                    f"Tavily search '{query[:50]}...' returned {len(results)} results"
                )
            
            return results
            
        except asyncio.TimeoutError as e:
            # å•ç‹¬å¤„ç†è¶…æ—¶å¼‚å¸¸ï¼Œæä¾›æ›´æ¸…æ™°çš„æ—¥å¿—
            # ï¼ˆä¸åœ¨ RETRYABLE_EXCEPTIONS ä¸­ï¼Œé¿å…é‡å¤åŒ¹é…ï¼‰
            last_error = e
            if attempt < max_retries:
                logger.warning(
                    f"Tavily search '{query[:50]}...' timeout "
                    f"(attempt {attempt + 1}/{max_retries + 1}), retrying..."
                )
                await asyncio.sleep(2)  # é‡è¯•å‰ç­‰å¾…2ç§’
            else:
                logger.error(
                    f"Tavily search '{query[:50]}...' timeout after {max_retries + 1} attempts"
                )
                
        except RETRYABLE_EXCEPTIONS as e:
            # å¯é‡è¯•çš„ç½‘ç»œé”™è¯¯ï¼ˆä½¿ç”¨æ¨¡å—çº§å¸¸é‡ï¼Œä¾¿äºç»Ÿä¸€ç®¡ç†ï¼‰
            last_error = e
            if attempt < max_retries:
                logger.warning(
                    f"Tavily search '{query[:50]}...' network error: {type(e).__name__} "
                    f"(attempt {attempt + 1}/{max_retries + 1}), retrying..."
                )
                await asyncio.sleep(2)
            else:
                logger.error(
                    f"Tavily search '{query[:50]}...' failed after {max_retries + 1} attempts: {e}"
                )
        
        except NON_RETRYABLE_EXCEPTIONS as e:
            # å·²çŸ¥çš„ä¸å¯é‡è¯•å¼‚å¸¸ï¼ˆAPI é”™è¯¯ã€é…ç½®é”™è¯¯ï¼‰
            # ç«‹å³å¤±è´¥ï¼Œæä¾›æ˜ç¡®çš„é”™è¯¯åˆ†ç±»
            logger.error(
                f"Tavily search '{query[:50]}...' non-retryable API/config error: "
                f"{type(e).__name__}: {e}"
            )
            raise TavilySearchError(
                query=query,
                attempts=attempt + 1,
                cause=e
            ) from e
        
        except Exception as e:
            # å…¶ä»–æœªé¢„æœŸå¼‚å¸¸ï¼ˆç¼–ç¨‹é”™è¯¯ç­‰ï¼‰ä¸é‡è¯•ï¼Œç«‹å³å¤±è´¥
            # åŒ…æ‹¬ä½†ä¸é™äºï¼š
            # - ç¼–ç¨‹é”™è¯¯ï¼šKeyError, AttributeError, TypeError, ValueError
            # - å…¶ä»–æœªåˆ†ç±»å¼‚å¸¸
            #
            # å…³é”®ï¼šå°è£…ä¸º TavilySearchError å¯¹å¤–æŠ›å‡ºï¼ŒåŸå§‹å¼‚å¸¸é€šè¿‡ cause ä¿ç•™
            logger.error(
                f"Tavily search '{query[:50]}...' unexpected error: "
                f"{type(e).__name__}: {e}"
            )
            raise TavilySearchError(
                query=query,
                attempts=attempt + 1,
                cause=e  # â† ä¿ç•™åŸå§‹å¼‚å¸¸ï¼Œä¾¿äºè°ƒè¯•
            ) from e
    
    # æ‰€æœ‰é‡è¯•éƒ½å¤±è´¥
    raise TavilySearchError(
        query=query,
        cause=last_error,
        attempts=max_retries + 1,
    )
```

### 5.1.1 è‡ªå®šä¹‰å¼‚å¸¸ç±»

åœ¨ `stock_analyzer/exceptions.py` ä¸­å®šä¹‰ï¼š

```python
"""æ¨¡å—Bè‡ªå®šä¹‰å¼‚å¸¸"""

class TavilySearchError(Exception):
    """Tavily æœç´¢å¼‚å¸¸"""
    def __init__(self, query: str, cause: Exception, attempts: int = 1):
        self.query = query
        self.cause = cause
        self.attempts = attempts
        super().__init__(
            f"Tavily search failed for '{query}' after {attempts} attempts: {cause}"
        )


class AgentCallError(Exception):
    """Agent è°ƒç”¨å¼‚å¸¸ï¼ˆåŒ…å« JSON è§£æå’Œ Pydantic æ ¡éªŒå¤±è´¥ï¼‰"""
    def __init__(self, agent_name: str, cause: Exception):
        self.agent_name = agent_name
        self.cause = cause
        super().__init__(f"Agent '{agent_name}' call failed: {cause}")


class ReportGenerationError(Exception):
    """æŠ¥å‘Šç”Ÿæˆå¼‚å¸¸"""
    def __init__(self, symbol: str, cause: Exception, learnings_count: int):
        self.symbol = symbol
        self.cause = cause
        self.learnings_count = learnings_count
        super().__init__(
            f"Failed to generate report for {symbol} with {learnings_count} learnings: {cause}"
        )


class WebResearchError(Exception):
    """Web Research æ•´ä½“æµç¨‹å¼‚å¸¸"""
    pass
```

### 5.2 Agent å®šä¹‰

ä¸‰ä¸ª ChatAgent å®ä¾‹ï¼š`query_agent`ï¼ˆç”ŸæˆæŸ¥è¯¢ï¼‰ã€`extract_agent`ï¼ˆæå–çŸ¥è¯†ï¼‰ã€`report_agent`ï¼ˆç”ŸæˆæŠ¥å‘Šï¼‰ã€‚

#### 5.2.1 query_agent â€” ç”Ÿæˆæœç´¢æŸ¥è¯¢

```python
from agent_framework import ChatAgent
from agent_framework.openai import OpenAIChatClient

def create_query_agent(chat_client: OpenAIChatClient) -> ChatAgent:
    """
    åˆ›å»ºæœç´¢æŸ¥è¯¢ç”Ÿæˆ Agentã€‚

    èŒè´£ï¼šæ ¹æ®ç ”ç©¶ä¸»é¢˜å’Œå·²æœ‰ learningsï¼Œç”Ÿæˆé€‚åˆæœç´¢å¼•æ“çš„æŸ¥è¯¢è¯ã€‚
    è¾“å…¥ï¼šç”¨æˆ·æ¶ˆæ¯ï¼ˆåŒ…å«ä¸»é¢˜å’Œ learnings çš„åŠ¨æ€æ‹¼æ¥æç¤ºè¯ï¼‰
    è¾“å‡ºï¼šJSON æ ¼å¼çš„ SerpQueryList
    """
    return ChatAgent(
        chat_client=chat_client,
        name="query_generator",
        instructions=QUERY_AGENT_SYSTEM_PROMPT,
        default_options={
            "temperature": 0.5,
            "response_format": {"type": "json_object"},
        },
    )
```

**ç³»ç»Ÿæç¤ºè¯ï¼ˆ`QUERY_AGENT_SYSTEM_PROMPT`ï¼‰ï¼š**

```python
QUERY_AGENT_SYSTEM_PROMPT = """\
ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„ A è‚¡é‡‘èç ”ç©¶å‘˜åŠ©æ‰‹ã€‚ä½ çš„å”¯ä¸€ä»»åŠ¡æ˜¯ï¼šæ ¹æ®ç»™å®šçš„ç ”ç©¶ä¸»é¢˜å’Œå·²æœ‰çŸ¥è¯†ï¼Œ
ç”Ÿæˆé€‚åˆæœç´¢å¼•æ“çš„æŸ¥è¯¢è¯ã€‚

## è§„åˆ™

1. ç”Ÿæˆçš„æŸ¥è¯¢è¯å¿…é¡»æ˜¯ä¸­æ–‡ï¼Œé€‚åˆåœ¨ Google / Bing ç­‰æœç´¢å¼•æ“ä¸­ä½¿ç”¨
2. æ¯ä¸ªæŸ¥è¯¢è¯åº”å…·ä½“ã€ç²¾å‡†ï¼Œé¿å…è¿‡äºç¬¼ç»Ÿ
3. å¦‚æœæä¾›äº†å·²æœ‰çŸ¥è¯†ç‚¹ï¼ˆlearningsï¼‰ï¼Œä½ åº”è¯¥ï¼š
   - é¿å…æœç´¢å·²çŸ¥ä¿¡æ¯
   - é’ˆå¯¹çŸ¥è¯†ç‚¹ä¸­çš„ç¼ºå¤±ã€ä¸ç¡®å®šæˆ–å€¼å¾—æ·±å…¥çš„æ–¹å‘ç”ŸæˆæŸ¥è¯¢
   - æŸ¥è¯¢åº”æ¯”å‰ä¸€è½®æ›´å…·ä½“ã€æ›´æœ‰æ·±åº¦
4. æ¯ä¸ªæŸ¥è¯¢å¿…é¡»é™„å¸¦ research_goalï¼Œè¯´æ˜è¿™æ¡æŸ¥è¯¢æœŸæœ›å‘ç°ä»€ä¹ˆ

## è¾“å‡ºæ ¼å¼

ä¸¥æ ¼è¾“å‡º JSONï¼Œæ ¼å¼å¦‚ä¸‹ï¼š
```json
{
  "queries": [
    {
      "query": "æœç´¢å¼•æ“æŸ¥è¯¢è¯",
      "research_goal": "ç ”ç©¶ç›®æ ‡è¯´æ˜"
    }
  ]
}
```

ä¸è¦è¾“å‡ºä»»ä½• JSON ä¹‹å¤–çš„å†…å®¹ã€‚
"""
```

#### 5.2.2 extract_agent â€” æå–çŸ¥è¯†ç‚¹

```python
def create_extract_agent(chat_client: OpenAIChatClient) -> ChatAgent:
    """
    åˆ›å»ºçŸ¥è¯†ç‚¹æå– Agentã€‚

    èŒè´£ï¼šä» Tavily æœç´¢ç»“æœä¸­æå–ç»“æ„åŒ–çš„çŸ¥è¯†ç‚¹å’Œè¿½é—®æ–¹å‘ã€‚
    è¾“å…¥ï¼šç”¨æˆ·æ¶ˆæ¯ï¼ˆåŒ…å«æœç´¢æŸ¥è¯¢å’Œæœç´¢ç»“æœå†…å®¹ï¼‰
    è¾“å‡ºï¼šJSON æ ¼å¼çš„ ProcessedResult
    """
    return ChatAgent(
        chat_client=chat_client,
        name="knowledge_extractor",
        instructions=EXTRACT_AGENT_SYSTEM_PROMPT,
        default_options={
            "temperature": 0.2,
            "response_format": {"type": "json_object"},
        },
    )
```

**ç³»ç»Ÿæç¤ºè¯ï¼ˆ`EXTRACT_AGENT_SYSTEM_PROMPT`ï¼‰ï¼š**

```python
EXTRACT_AGENT_SYSTEM_PROMPT = """\
ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„ A è‚¡é‡‘èç ”ç©¶å‘˜åŠ©æ‰‹ã€‚ä½ çš„å”¯ä¸€ä»»åŠ¡æ˜¯ï¼šä»æœç´¢ç»“æœä¸­æå–é«˜è´¨é‡çš„çŸ¥è¯†ç‚¹ï¼Œ
å¹¶æå‡ºå€¼å¾—ç»§ç»­æ·±å…¥çš„è¿½é—®æ–¹å‘ã€‚

## çŸ¥è¯†ç‚¹ï¼ˆlearningsï¼‰æå–è§„åˆ™

1. æ¯ä¸ªçŸ¥è¯†ç‚¹å¿…é¡»**ä¿¡æ¯å¯†é›†**ï¼ŒåŒ…å«å…·ä½“çš„ï¼š
   - å®ä½“åç§°ï¼ˆå…¬å¸åã€äº§å“åã€äººåï¼‰
   - æ•°å­—ï¼ˆé‡‘é¢ã€ç™¾åˆ†æ¯”ã€æ•°é‡ï¼‰
   - æ—¥æœŸï¼ˆå…·ä½“åˆ°æœˆä»½æˆ–å­£åº¦ï¼‰
2. çŸ¥è¯†ç‚¹ä¹‹é—´ä¸åº”é‡å¤
3. ä¼˜å…ˆæå–ä¸æŠ•èµ„å†³ç­–ç›¸å…³çš„ä¿¡æ¯
4. å¿½ç•¥å¹¿å‘Šã€æ¨å¹¿ã€æ— å®è´¨å†…å®¹çš„ä¿¡æ¯
5. æ¯æ¬¡æœ€å¤šæå– **5 ä¸ª**çŸ¥è¯†ç‚¹

## è¿½é—®æ–¹å‘ï¼ˆfollow_up_questionsï¼‰è§„åˆ™

1. è¿½é—®åº”æŒ‡å‘æœç´¢ç»“æœæœªå……åˆ†è¦†ç›–ä½†æœ‰ä»·å€¼çš„æ–¹å‘
2. è¿½é—®åº”æ¯”å½“å‰æœç´¢æ›´æ·±å…¥ã€æ›´å…·ä½“
3. æ¯æ¬¡æœ€å¤šæå‡º **3 ä¸ª**è¿½é—®

## è¾“å‡ºæ ¼å¼

ä¸¥æ ¼è¾“å‡º JSONï¼Œæ ¼å¼å¦‚ä¸‹ï¼š
```json
{
  "learnings": [
    "çŸ¥è¯†ç‚¹1ï¼šåŒ…å«å…·ä½“å®ä½“ã€æ•°å­—ã€æ—¥æœŸ",
    "çŸ¥è¯†ç‚¹2ï¼šä¿¡æ¯å¯†é›†ä¸”ä¸é‡å¤"
  ],
  "follow_up_questions": [
    "è¿½é—®æ–¹å‘1",
    "è¿½é—®æ–¹å‘2"
  ]
}
```

ä¸è¦è¾“å‡ºä»»ä½• JSON ä¹‹å¤–çš„å†…å®¹ã€‚
"""
```

#### 5.2.3 report_agent â€” ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š

```python
def create_report_agent(chat_client: OpenAIChatClient) -> ChatAgent:
    """
    åˆ›å»ºæŠ¥å‘Šç”Ÿæˆ Agentã€‚

    èŒè´£ï¼šå°†æ‰€æœ‰ learnings æ•´åˆä¸ºç»“æ„åŒ–çš„æŠ•èµ„ç ”ç©¶æŠ¥å‘Šã€‚
    è¾“å…¥ï¼šç”¨æˆ·æ¶ˆæ¯ï¼ˆåŒ…å«è‚¡ç¥¨ä¿¡æ¯å’Œå…¨éƒ¨ learningsï¼‰
    è¾“å‡ºï¼šJSON æ ¼å¼çš„ WebResearchResultï¼ˆä¸å« metaï¼Œmeta ç”±ä»£ç å¡«å……ï¼‰
    """
    return ChatAgent(
        chat_client=chat_client,
        name="report_generator",
        instructions=REPORT_AGENT_SYSTEM_PROMPT,
        default_options={
            "temperature": 0.3,
            "response_format": {"type": "json_object"},
        },
    )
```

**ç³»ç»Ÿæç¤ºè¯ï¼ˆ`REPORT_AGENT_SYSTEM_PROMPT`ï¼‰ï¼š**

```python
REPORT_AGENT_SYSTEM_PROMPT = """\
ä½ æ˜¯ä¸€ä½èµ„æ·±çš„ A è‚¡é¦–å¸­ç ”ç©¶å‘˜ã€‚ä½ çš„ä»»åŠ¡æ˜¯ï¼šå°†å¤šè½®æ·±åº¦ç½‘ç»œæœç´¢ä¸­ç§¯ç´¯çš„çŸ¥è¯†ç‚¹ï¼Œ
æ•´åˆä¸ºä¸€ä»½ç»“æ„åŒ–çš„æŠ•èµ„ç ”ç©¶æ‘˜è¦æŠ¥å‘Šã€‚

## æŠ¥å‘Šè¦æ±‚

1. **æ–°é—»åˆ†ç±»**ï¼šå°†æ–°é—»ç±»çŸ¥è¯†ç‚¹æŒ‰æ­£é¢/è´Ÿé¢/ä¸­æ€§åˆ†ç±»ï¼Œæ¯æ¡åŒ…å«æ ‡é¢˜ã€æ‘˜è¦ã€æ¥æºã€æ—¥æœŸã€é‡è¦æ€§
2. **ç«äº‰ä¼˜åŠ¿**ï¼šç»¼åˆæ‰€æœ‰ç›¸å…³çŸ¥è¯†ç‚¹ï¼Œæè¿°å…¬å¸çš„æ ¸å¿ƒç«äº‰åŠ›ã€æŠ¤åŸæ²³ç±»å‹ã€å¸‚åœºåœ°ä½
3. **è¡Œä¸šå‰æ™¯**ï¼šç»¼åˆè¡Œä¸šç›¸å…³çŸ¥è¯†ç‚¹ï¼Œç»™å‡ºå‰æ™¯åˆ¤æ–­ã€é©±åŠ¨å› ç´ ã€é£é™©å› ç´ 
4. **é£é™©äº‹ä»¶**ï¼šæ•´ç†ç›‘ç®¡å¤„ç½šã€è¯‰è®¼ã€ç®¡ç†å±‚å˜åŠ¨ç­‰é£é™©ä¿¡æ¯
5. **æœºæ„è§‚ç‚¹**ï¼šæ•´ç†åˆ¸å•†è¯„çº§ã€ç›®æ ‡ä»·ç­‰ä¿¡æ¯
6. **å¯ä¿¡åº¦è¯„ä¼°**ï¼šæ ¹æ®çŸ¥è¯†ç‚¹çš„æ¥æºè´¨é‡å’Œä¸€è‡´æ€§ï¼Œç»™å‡ºæ•´ä½“å¯ä¿¡åº¦ï¼ˆé«˜/ä¸­/ä½ï¼‰

## æ•°æ®å¤„ç†åŸåˆ™

- å¦‚æœæŸä¸ªå­—æ®µç¼ºä¹è¶³å¤Ÿçš„çŸ¥è¯†ç‚¹æ”¯æ’‘ï¼Œåº”å¦‚å®è¯´æ˜"ä¿¡æ¯ä¸è¶³"ï¼Œä¸è¦ç¼–é€ 
- æ—¥æœŸå°½é‡ç²¾ç¡®åˆ°å¤©ï¼Œæ— æ³•ç¡®å®šæ—¶å†™åˆ°æœˆä»½
- æ¥æºå†™åª’ä½“/ç½‘ç«™åç§°ï¼Œä¸è¦å†™ URL
- æ•°å­—ä¿ç•™åŸå§‹ç²¾åº¦ï¼Œä¸è¦å››èˆäº”å…¥

## è¾“å‡ºæ ¼å¼

ä¸¥æ ¼è¾“å‡º JSONï¼Œç»“æ„å¦‚ä¸‹ï¼ˆä¸åŒ…å« meta å­—æ®µï¼Œmeta ç”±ç³»ç»Ÿè‡ªåŠ¨å¡«å……ï¼‰ï¼š

```json
{
  "news_summary": {
    "positive": [{"title": "", "summary": "", "source": "", "date": "", "importance": "é«˜/ä¸­/ä½"}],
    "negative": [],
    "neutral": []
  },
  "competitive_advantage": {
    "description": "",
    "moat_type": "",
    "market_position": ""
  },
  "industry_outlook": {
    "industry": "",
    "outlook": "",
    "key_drivers": [],
    "key_risks": []
  },
  "risk_events": {
    "regulatory": "",
    "litigation": "",
    "management": "",
    "other": ""
  },
  "analyst_opinions": {
    "buy_count": 0,
    "hold_count": 0,
    "sell_count": 0,
    "average_target_price": null,
    "recent_reports": []
  },
  "search_confidence": "é«˜/ä¸­/ä½"
}
```

**æ³¨æ„ï¼š** é™çº§æŠ¥å‘Šæ—¶ï¼Œ`meta.raw_learnings` ä¼šåŒ…å«æ‰€æœ‰åŸå§‹çŸ¥è¯†ç‚¹åˆ—è¡¨ï¼Œä¾¿äºåç»­äººå·¥åˆ†æã€‚

ä¸è¦è¾“å‡ºä»»ä½• JSON ä¹‹å¤–çš„å†…å®¹ã€‚
"""
```

### 5.3 LLM è°ƒç”¨è¾…åŠ©å‡½æ•°

æ‰€æœ‰ Agent çš„è°ƒç”¨ç»Ÿä¸€å°è£…ä¸ºè¾…åŠ©å‡½æ•°ï¼Œè´Ÿè´£è°ƒç”¨ Agentã€æå– JSONã€æ ¡éªŒ Pydantic æ¨¡å‹ã€‚

**å…³é”®æ”¹è¿›ï¼š**
- `extract_json_str()` ä½¿ç”¨"å€™é€‰é›† + é€ä¸ªéªŒè¯"ç­–ç•¥ï¼Œæ”¯æŒå¤šç§å¤æ‚åœºæ™¯
- ä¼˜å…ˆéªŒè¯æ ‡è®°ä¸º `json` çš„ä»£ç å—ï¼Œç„¶åæ˜¯æ— æ ‡ç­¾å—ï¼Œæœ€åæ˜¯æ•´æ®µæ–‡æœ¬
- ä»åå¾€å‰éªŒè¯ï¼ˆLLM é€šå¸¸æŠŠæœ€ç»ˆç»“æœæ”¾åœ¨æœ€åï¼‰ï¼Œä½¿ç”¨ `itertools.chain()` æ­£ç¡®è¿æ¥ä¸¤ä¸ª `reversed()` è¿­ä»£å™¨
- æ— éœ€å¼ºåˆ¶ LLM ä»…è¾“å‡ºçº¯ JSONï¼Œå¤§å¹…æå‡äº†é²æ£’æ€§

```python
"""
Agent è°ƒç”¨è¾…åŠ©å‡½æ•°

æ³¨æ„ï¼šå¼‚å¸¸ç±»å¿…é¡»ä» exceptions.py å¯¼å…¥ï¼Œç¡®ä¿ç¤ºä¾‹ä»£ç å¯ç›´æ¥è¿è¡Œ
"""
from pydantic import BaseModel, ValidationError
from agent_framework import ChatAgent
from stock_analyzer.logger import logger
from stock_analyzer.exceptions import AgentCallError  # â† å¿…é¡»å¯¼å…¥ï¼Œå¦åˆ™è¿è¡Œæ—¶ NameError


def extract_json_str(text: str) -> str:
    """
    ä» LLM å“åº”æ–‡æœ¬ä¸­æå– JSON å­—ç¬¦ä¸²ã€‚
    
    ç­–ç•¥ï¼ˆæŒ‰ä¼˜å…ˆçº§ï¼‰ï¼š
    1. å°è¯•æ•´æ®µæ–‡æœ¬ä½œä¸º JSON è§£æï¼ˆæœ€å¿«è·¯å¾„ï¼‰
    2. æå–æ‰€æœ‰ fenced code blocksï¼Œä¼˜å…ˆéªŒè¯ ```json æ ‡ç­¾çš„ï¼Œç„¶åéªŒè¯æ— æ ‡ç­¾çš„
    3. ä»åå¾€å‰éªŒè¯ï¼ˆLLM é€šå¸¸æŠŠæœ€ç»ˆç»“æœæ”¾åœ¨æœ€åï¼‰
    4. å¦‚æœéƒ½å¤±è´¥ï¼ŒæŠ›å‡º ValueError
    
    æ”¯æŒçš„æ ¼å¼ï¼š
    - ç›´æ¥ JSON: {"key": "value"}
    - Markdown åŒ…è£¹: ```json\n{"key": "value"}\n```
    - å‰ç½®æ–‡å­—: Here is...\n```json\n{"key": "value"}\n```
    - å¤šä¸ªä»£ç å—: å–æœ€åä¸€ä¸ªæœ‰æ•ˆçš„ JSON å—
    
    æ³¨æ„ï¼šä½¿ç”¨ itertools.chain è¿æ¥ä¸¤ä¸ª reversed è¿­ä»£å™¨ï¼Œ
    å› ä¸º Python ä¸­ reversed() è¿”å›çš„è¿­ä»£å™¨å¯¹è±¡ä¸æ”¯æŒ + è¿ç®—ç¬¦ã€‚
    """
    import json
    import re
    from itertools import chain
    
    text = text.strip()
    
    # 1. å°è¯•æ•´æ®µæ–‡æœ¬ï¼ˆæœ€å¿«è·¯å¾„ï¼‰
    try:
        json.loads(text)
        return text
    except Exception:
        pass
    
    # 2. æå–æ‰€æœ‰ fenced code blocks
    FENCE_RE = re.compile(r"```(?P<lang>[a-zA-Z0-9_-]*)\s*\n(?P<body>[\s\S]*?)```", re.MULTILINE)
    blocks = list(FENCE_RE.finditer(text))
    
    if blocks:
        json_blocks = []  # æ˜ç¡®æ ‡è®°ä¸º json çš„å—
        plain_blocks = []  # æ— æ ‡ç­¾çš„å—
        
        for match in blocks:
            lang = (match.group("lang") or "").strip().lower()
            body = match.group("body").strip()
            
            if lang == "json":
                json_blocks.append(body)
            elif lang == "":
                plain_blocks.append(body)
        
        # ä»åå¾€å‰éªŒè¯ï¼ˆä¼˜å…ˆ json æ ‡ç­¾ï¼Œå†å°è¯• plainï¼‰
        # ä½¿ç”¨ itertools.chain è¿æ¥ä¸¤ä¸ª reversed è¿­ä»£å™¨
        for candidate in chain(reversed(json_blocks), reversed(plain_blocks)):
            try:
                json.loads(candidate)
                return candidate
            except Exception:
                continue
    
    # 3. å¦‚æœéƒ½å¤±è´¥ï¼ŒæŠ›å‡ºå¼‚å¸¸
    raise ValueError("No valid JSON found in model output")


# ============================================================
# âš ï¸ å…³é”®å®æ–½è¦ç‚¹ï¼šitertools.chain çš„ä½¿ç”¨
# ============================================================
# 1. reversed() è¿”å›çš„æ˜¯è¿­ä»£å™¨å¯¹è±¡ï¼Œä¸æ”¯æŒ + è¿ç®—ç¬¦
# 2. å¿…é¡»ä½¿ç”¨ itertools.chain() æ¥è¿æ¥å¤šä¸ªè¿­ä»£å™¨
# 3. é”™è¯¯ç¤ºä¾‹ï¼šfor x in reversed(list1) + reversed(list2)  # âŒ TypeError
# 4. æ­£ç¡®ç¤ºä¾‹ï¼šfor x in chain(reversed(list1), reversed(list2))  # âœ…
# ============================================================


async def call_agent_with_model[T: BaseModel](
    agent: ChatAgent,
    message: str,
    model_cls: type[T],
) -> T:
    """
    è°ƒç”¨ ChatAgent å¹¶è§£æä¸º Pydantic æ¨¡å‹ã€‚
    
    æ³¨æ„ï¼šä½¿ç”¨ PEP 695 æ³›å‹è¯­æ³•ï¼ˆPython 3.12+ï¼‰ï¼Œ
    ä¸é¡¹ç›® pyproject.toml çš„ requires-python = ">=3.12" è¦æ±‚ä¸€è‡´ã€‚

    Args:
        agent: MAF ChatAgent å®ä¾‹
        message: ç”¨æˆ·æ¶ˆæ¯
        model_cls: æœŸæœ›çš„ Pydantic è¾“å‡ºæ¨¡å‹

    Returns:
        è§£æåçš„ Pydantic æ¨¡å‹å®ä¾‹

    Raises:
        AgentCallError: Agent è°ƒç”¨æˆ–è§£æå¤±è´¥
    """
    thread = agent.get_new_thread()

    try:
        response = await agent.run(
            message=message,
            thread=thread,
        )
        raw_text = response.text
        json_str = extract_json_str(raw_text)
        result = model_cls.model_validate_json(json_str)
        return result

    except ValidationError as e:
        # Pydantic v2: model_validate_json() å¯¹æ‰€æœ‰éªŒè¯å¤±è´¥éƒ½æŠ›å‡º ValidationError
        # åŒ…æ‹¬ï¼šJSON æ ¼å¼é”™è¯¯ã€å­—æ®µç¼ºå¤±ã€ç±»å‹ä¸åŒ¹é…ã€çº¦æŸè¿åç­‰
        logger.error(
            f"Agent '{agent.name}' validation failed: {e.error_count()} errors\n"
            f"First error: {e.errors()[0] if e.errors() else 'unknown'}"
        )
        raise AgentCallError(agent_name=agent.name, cause=e) from e
    except Exception as e:
        logger.error(f"Agent '{agent.name}' call failed: {e}")
        raise AgentCallError(agent_name=agent.name, cause=e) from e
```

### 5.4 generate_serp_queries â€” ç”Ÿæˆæœç´¢æŸ¥è¯¢

```python
async def generate_serp_queries(
    query_agent: ChatAgent,
    query: str,
    num_queries: int,
    learnings: list[str],
) -> list[SerpQuery]:
    """
    è°ƒç”¨ query_agent ç”Ÿæˆæœç´¢æŸ¥è¯¢ã€‚

    æç¤ºè¯åŠ¨æ€æ‹¼æ¥é€»è¾‘ï¼š
    - å§‹ç»ˆåŒ…å«ç ”ç©¶ä¸»é¢˜ï¼ˆqueryï¼‰
    - å§‹ç»ˆåŒ…å«æ‰€éœ€æŸ¥è¯¢æ•°é‡ï¼ˆnum_queriesï¼‰
    - æœ‰ learnings æ—¶è¿½åŠ å†å²çŸ¥è¯†ä¸Šä¸‹æ–‡ï¼Œå¼•å¯¼ Agent ç”Ÿæˆæ›´æ·±å…¥çš„æŸ¥è¯¢
    - æ—  learnings æ—¶ï¼ˆç¬¬ä¸€è½®ï¼‰ï¼ŒAgent è‡ªç”±å‘æ•£

    Args:
        query_agent: MAF ChatAgent (query_generator)
        query: å½“å‰ç ”ç©¶ä¸»é¢˜æˆ–è¿½é—®æ–¹å‘
        num_queries: éœ€è¦ç”Ÿæˆçš„æŸ¥è¯¢æ•°é‡
        learnings: å·²æœ‰çš„çŸ¥è¯†ç‚¹åˆ—è¡¨ï¼ˆå‰å‡ è½®ç´¯ç§¯ï¼‰

    Returns:
        æœç´¢æŸ¥è¯¢åˆ—è¡¨
    """
    # â”€â”€ åŠ¨æ€æ‹¼æ¥ç”¨æˆ·æ¶ˆæ¯ â”€â”€
    user_message = f"è¯·ä¸ºä»¥ä¸‹ç ”ç©¶ä¸»é¢˜ç”Ÿæˆ {num_queries} ä¸ªæœç´¢æŸ¥è¯¢ã€‚\n\n"
    user_message += f"<topic>\n{query}\n</topic>\n"

    if learnings:
        user_message += (
            "\nä»¥ä¸‹æ˜¯å‰å‡ è½®ç ”ç©¶ä¸­å·²è·å¾—çš„çŸ¥è¯†ç‚¹ï¼Œ"
            "è¯·æ®æ­¤ç”Ÿæˆæ›´æœ‰é’ˆå¯¹æ€§çš„æŸ¥è¯¢ï¼Œé¿å…æœç´¢å·²çŸ¥ä¿¡æ¯ï¼š\n"
            "<learnings>\n"
        )
        for learning in learnings:
            user_message += f"- {learning}\n"
        user_message += "</learnings>\n"

    # â”€â”€ è°ƒç”¨ Agent â”€â”€
    result = await call_agent_with_model(
        agent=query_agent,
        message=user_message,
        model_cls=SerpQueryList,
    )

    # æˆªæ–­åˆ°è¯·æ±‚æ•°é‡ï¼ˆLLM å¯èƒ½å¤šç”Ÿæˆï¼‰
    return result.queries[:num_queries]
```

**æç¤ºè¯æ¼”è¿›ç¤ºä¾‹ï¼š**

| è½®æ¬¡ | learnings æ•°é‡ | ç”¨æˆ·æ¶ˆæ¯å…³é”®å†…å®¹ |
|------|---------------|-----------------|
| ç¬¬1è½® | 0 | `<topic>å¹³å®‰é“¶è¡Œ è¿‘æœŸæ–°é—»...</topic>` |
| ç¬¬2è½® | ~9 | `<topic>Previous research goal: ...\nFollow-up: ...</topic>\n<learnings>9æ¡</learnings>` |
| ç¬¬3è½® | ~18 | `<topic>Previous research goal: ...\nFollow-up: ...</topic>\n<learnings>18æ¡</learnings>` |

### 5.5 process_serp_result â€” æå–çŸ¥è¯†ç‚¹

```python
async def process_serp_result(
    extract_agent: ChatAgent,
    query: str,
    search_results: list[dict],
) -> ProcessedResult:
    """
    è°ƒç”¨ extract_agent ä» Tavily æœç´¢ç»“æœä¸­æå–çŸ¥è¯†ç‚¹å’Œè¿½é—®æ–¹å‘ã€‚

    Args:
        extract_agent: MAF ChatAgent (knowledge_extractor)
        query: æœ¬æ¬¡æœç´¢ä½¿ç”¨çš„æŸ¥è¯¢è¯
        search_results: Tavily è¿”å›çš„æœç´¢ç»“æœåˆ—è¡¨

    Returns:
        ProcessedResultï¼ˆlearnings + follow_up_questionsï¼‰
    """
    # â”€â”€ å°†æœç´¢ç»“æœæ ¼å¼åŒ–ä¸ºæç¤ºè¯ â”€â”€
    if not search_results:
        return ProcessedResult(learnings=[], follow_up_questions=[])

    contents_parts: list[str] = []
    for r in search_results:
        content = r.get("content", "")
        url = r.get("url", "")
        title = r.get("title", "")
        if content:
            contents_parts.append(
                f'<source title="{title}" url="{url}">\n{content}\n</source>'
            )

    if not contents_parts:
        return ProcessedResult(learnings=[], follow_up_questions=[])

    contents_text = "\n\n".join(contents_parts)

    user_message = (
        f"ä»¥ä¸‹æ˜¯é’ˆå¯¹æŸ¥è¯¢ <query>{query}</query> çš„æœç´¢ç»“æœã€‚\n"
        f"è¯·ä»ä¸­æå–å…³é”®çŸ¥è¯†ç‚¹å’Œå€¼å¾—è¿½é—®çš„æ–¹å‘ã€‚\n\n"
        f"<search_results>\n{contents_text}\n</search_results>"
    )

    # â”€â”€ è°ƒç”¨ Agent â”€â”€
    result = await call_agent_with_model(
        agent=extract_agent,
        message=user_message,
        model_cls=ProcessedResult,
    )

    return result
```

### 5.6 deep_research â€” æ ¸å¿ƒé€’å½’å‡½æ•°

é€’å½’çš„ Deep Research é€»è¾‘å®ç°ï¼šæ¯æ¬¡è°ƒç”¨ç”Ÿæˆ breadth ä¸ªæŸ¥è¯¢ï¼Œæ‰§è¡Œæœç´¢å’ŒçŸ¥è¯†æå–ï¼Œç„¶åå¯¹æ¯ä¸ª follow-up é€’å½’è°ƒç”¨ã€‚

**è¿”å›å€¼è¯´æ˜ï¼š**
- æ€»æ˜¯è¿”å› `ResearchResult` å¯¹è±¡
- å¦‚æœæ‰€æœ‰æŸ¥è¯¢éƒ½å¤±è´¥ï¼ˆæŸ¥è¯¢ç”Ÿæˆå¤±è´¥ã€æœç´¢å¤±è´¥ã€æå–å¤±è´¥ç­‰ï¼‰ï¼Œä¼šè¿”å›ç©ºçš„ `learnings` åˆ—è¡¨
- è°ƒç”¨æ–¹ï¼ˆ`run_web_research()`ï¼‰ä¼šé€šè¿‡ `len(result.learnings) > 0` åˆ¤æ–­ä¸»é¢˜æ˜¯å¦çœŸæ­£æˆåŠŸ

```python
import asyncio
from stock_analyzer.logger import logger


async def deep_research(
    query_agent: ChatAgent,
    extract_agent: ChatAgent,
    query: str,
    breadth: int,
    depth: int,
    learnings: list[str] | None = None,
    visited_urls: list[str] | None = None,
) -> ResearchResult:
    """
    å¯¹å•ä¸ªç ”ç©¶ä¸»é¢˜æ‰§è¡Œé€’å½’æ·±åº¦æœç´¢ã€‚

    ç®—æ³•æµç¨‹ï¼š
    1. è°ƒç”¨ query_agent ç”Ÿæˆ breadth ä¸ªæœç´¢æŸ¥è¯¢
    2. å¯¹æ¯ä¸ªæŸ¥è¯¢ï¼š
       a. è°ƒç”¨ tavily_search æ‰§è¡Œæœç´¢
       b. è°ƒç”¨ extract_agent æå– learnings å’Œ follow_up_questions
       c. å¦‚æœ depth > 1ï¼šæ„é€ æ–°çš„ queryï¼ˆåŸºäº follow_up_questionsï¼‰ï¼Œé€’å½’è°ƒç”¨
    3. åˆå¹¶æ‰€æœ‰åˆ†æ”¯çš„ learnings å¹¶å»é‡

    å‚æ•°é€’å‡è§„åˆ™ï¼š
    - breadth: æ¯æ·±å…¥ä¸€å±‚å‡åŠ â†’ max(1, breadth // 2)
    - depth: æ¯æ·±å…¥ä¸€å±‚å‡1 â†’ depth - 1
    - å½“ depth == 1 æ—¶ä¸ºæœ€åä¸€å±‚ï¼Œä¸å†é€’å½’

    Args:
        query_agent: æœç´¢æŸ¥è¯¢ç”Ÿæˆ Agent
        extract_agent: çŸ¥è¯†ç‚¹æå– Agent
        query: ç ”ç©¶ä¸»é¢˜æˆ–è¿½é—®æ–¹å‘
        breadth: å½“å‰å±‚çš„å¹¶è¡ŒæŸ¥è¯¢æ•°
        depth: å‰©ä½™é€’å½’æ·±åº¦
        learnings: å‰å‡ è½®ç´¯ç§¯çš„çŸ¥è¯†ç‚¹
        visited_urls: å‰å‡ è½®å·²è®¿é—®çš„ URL

    Returns:
        ResearchResultï¼ˆåˆå¹¶å»é‡åçš„ learnings + visited_urlsï¼‰
        æ³¨æ„ï¼šå¦‚æœæ‰€æœ‰æŸ¥è¯¢éƒ½å¤±è´¥ï¼Œå¯èƒ½è¿”å›ç©ºçš„ learnings åˆ—è¡¨ï¼Œ
        è°ƒç”¨æ–¹éœ€é€šè¿‡ len(result.learnings) > 0 åˆ¤æ–­ä¸»é¢˜æ˜¯å¦çœŸæ­£æˆåŠŸ
    """
    if learnings is None:
        learnings = []
    if visited_urls is None:
        visited_urls = []

    logger.info(
        f"deep_research: depth={depth}, breadth={breadth}, "
        f"existing_learnings={len(learnings)}, query='{query[:80]}...'"
    )

    # â”€â”€ Step 1: ç”Ÿæˆæœç´¢æŸ¥è¯¢ â”€â”€
    try:
        serp_queries = await generate_serp_queries(
            query_agent=query_agent,
            query=query,
            num_queries=breadth,
            learnings=learnings,
        )
    except AgentCallError:
        logger.warning("Failed to generate SERP queries, returning current learnings")
        # æ³¨æ„ï¼šå¦‚æœ learnings ä¸ºç©ºï¼Œrun_web_research() ä¼šå°†æ­¤ä¸»é¢˜æ ‡è®°ä¸ºå¤±è´¥
        return ResearchResult(learnings=learnings, visited_urls=visited_urls)

    all_learnings = list(learnings)
    all_urls = list(visited_urls)

    # â”€â”€ Step 2: å¯¹æ¯ä¸ªæŸ¥è¯¢æ‰§è¡Œæœç´¢å’Œæå– â”€â”€
    #     åŒå±‚æŸ¥è¯¢å¹¶è¡Œæ‰§è¡Œï¼ˆæ— éœ€é¢å¤–é™æµï¼Œbreadth è‡ªç„¶æ§åˆ¶å¹¶å‘æ•°ï¼‰

    async def process_single_query(serp_query: SerpQuery) -> ResearchResult:
        """å¤„ç†å•ä¸ªæœç´¢æŸ¥è¯¢ï¼ˆæœç´¢ â†’ æå– â†’ å¯èƒ½é€’å½’ï¼‰"""
        branch_learnings = list(all_learnings)
        branch_urls = list(all_urls)

        # 2a. Tavily æœç´¢
        try:
            search_results = await tavily_search(serp_query.query)
        except TavilySearchError:
            logger.warning(f"Tavily search failed for '{serp_query.query}', skipping")
            # è¿”å›å½“å‰åˆ†æ”¯çš„ learningsï¼ˆå¯èƒ½ä¸ºç©ºï¼‰
            return ResearchResult(learnings=branch_learnings, visited_urls=branch_urls)

        # æ”¶é›† URL
        for r in search_results:
            url = r.get("url", "")
            if url:
                branch_urls.append(url)

        # 2b. æå–çŸ¥è¯†ç‚¹
        try:
            processed = await process_serp_result(
                extract_agent=extract_agent,
                query=serp_query.query,
                search_results=search_results,
            )
            branch_learnings.extend(processed.learnings)
        except AgentCallError:
            logger.warning(f"Knowledge extraction failed for '{serp_query.query}'")
            processed = ProcessedResult(learnings=[], follow_up_questions=[])

        # â”€â”€ Step 3: åˆ¤æ–­æ˜¯å¦ç»§ç»­æ·±å…¥ â”€â”€
        new_depth = depth - 1
        new_breadth = max(1, breadth // 2)

        if new_depth > 0 and processed.follow_up_questions:
            # Step 4: é€’å½’ â€” æ„é€ æ–°æŸ¥è¯¢ï¼ŒåŸºäº follow-up questions
            next_query = (
                f"Previous research goal: {serp_query.research_goal}\n"
                f"Follow-up research directions:\n"
                + "\n".join(f"- {q}" for q in processed.follow_up_questions)
            )

            deeper_result = await deep_research(
                query_agent=query_agent,
                extract_agent=extract_agent,
                query=next_query,
                breadth=new_breadth,
                depth=new_depth,
                learnings=branch_learnings,
                visited_urls=branch_urls,
            )
            return deeper_result

        return ResearchResult(learnings=branch_learnings, visited_urls=branch_urls)

    # å¹¶è¡Œæ‰§è¡Œæ‰€æœ‰åˆ†æ”¯ï¼ˆæ— éœ€ Semaphoreï¼Œbreadth å‚æ•°å·²æ§åˆ¶å¹¶å‘æ•°ï¼‰
    tasks = [process_single_query(sq) for sq in serp_queries]
    branch_results = await asyncio.gather(*tasks, return_exceptions=True)

    # â”€â”€ Step 5: åˆå¹¶æ‰€æœ‰åˆ†æ”¯ç»“æœ â”€â”€
    merged_learnings: set[str] = set()
    merged_urls: set[str] = set()

    for result in branch_results:
        if isinstance(result, ResearchResult):
            merged_learnings.update(result.learnings)
            merged_urls.update(result.visited_urls)
        elif isinstance(result, asyncio.CancelledError):
            # å–æ¶ˆå¼‚å¸¸å¿…é¡»å‘ä¸Šä¼ æ’­ï¼Œä¸èƒ½è¢«åæ‰
            logger.warning("Branch task was cancelled, propagating CancelledError")
            raise result
        elif isinstance(result, Exception):
            logger.error(f"Branch failed with exception: {result}")
        # è·³è¿‡å¤±è´¥çš„åˆ†æ”¯

    logger.info(
        f"deep_research complete: depth={depth}, "
        f"total_learnings={len(merged_learnings)}, total_urls={len(merged_urls)}"
    )

    return ResearchResult(
        learnings=list(merged_learnings),
        visited_urls=list(merged_urls),
    )
```

**é€’å½’æ‰§è¡Œæ ‘ï¼ˆå•ä¸»é¢˜ï¼Œbreadth=3, depth=2ï¼‰ï¼š**

```
ç¬¬1å±‚ (depth=2, breadth=3)
â”œâ”€â”€ æŸ¥è¯¢A â”€â”€â†’ Tavily â”€â”€â†’ æå– learnings + follow_ups
â”‚   â””â”€â”€ ç¬¬2å±‚ (depth=1, breadth=1)
â”‚       â””â”€â”€ æŸ¥è¯¢A-1 â”€â”€â†’ Tavily â”€â”€â†’ æå– (depth=0, åœæ­¢)
â”œâ”€â”€ æŸ¥è¯¢B â”€â”€â†’ Tavily â”€â”€â†’ æå– learnings + follow_ups
â”‚   â””â”€â”€ ç¬¬2å±‚ (depth=1, breadth=1)
â”‚       â””â”€â”€ æŸ¥è¯¢B-1 â”€â”€â†’ Tavily â”€â”€â†’ æå– (depth=0, åœæ­¢)
â””â”€â”€ æŸ¥è¯¢C â”€â”€â†’ Tavily â”€â”€â†’ æå– learnings + follow_ups
    â””â”€â”€ ç¬¬2å±‚ (depth=1, breadth=1)
        â””â”€â”€ æŸ¥è¯¢C-1 â”€â”€â†’ Tavily â”€â”€â†’ æå– (depth=0, åœæ­¢)

Tavily æ€»è°ƒç”¨: 3 + 3 = 6
LLM æ€»è°ƒç”¨: 1(ç”ŸæˆæŸ¥è¯¢) + 3(æå–) + 3(ç”ŸæˆæŸ¥è¯¢) + 3(æå–) = 10
```

### 5.7 generate_report â€” æœ€ç»ˆæŠ¥å‘Šç”Ÿæˆ

```python
async def generate_report(
    report_agent: ChatAgent,
    symbol: str,
    name: str,
    industry: str,
    learnings: list[str],
) -> dict:
    """
    è°ƒç”¨ report_agent å°†æ‰€æœ‰ learnings æ•´åˆä¸ºç»“æ„åŒ–æŠ¥å‘Šã€‚

    Args:
        report_agent: æŠ¥å‘Šç”Ÿæˆ Agent
        symbol: è‚¡ç¥¨ä»£ç 
        name: è‚¡ç¥¨åç§°
        industry: æ‰€å±è¡Œä¸š
        learnings: å»é‡åçš„å…¨éƒ¨çŸ¥è¯†ç‚¹

    Returns:
        æŠ¥å‘Š JSON dictï¼ˆä¸å« meta å­—æ®µï¼‰
    """
    user_message = (
        f"è¯·ä¸ºè‚¡ç¥¨ {symbol} {name}ï¼ˆ{industry}è¡Œä¸šï¼‰"
        f"ç”Ÿæˆç»“æ„åŒ–çš„ç½‘ç»œæ·±åº¦æœç´¢ç ”ç©¶æŠ¥å‘Šã€‚\n\n"
        f"ä»¥ä¸‹æ˜¯é€šè¿‡å¤šè½®æ·±åº¦æœç´¢ç§¯ç´¯çš„å…¨éƒ¨ {len(learnings)} ä¸ªçŸ¥è¯†ç‚¹ï¼š\n\n"
        f"<learnings>\n"
    )
    for i, learning in enumerate(learnings, 1):
        user_message += f"{i}. {learning}\n"
    user_message += "</learnings>\n"

    # è°ƒç”¨ Agent â€” è¿™é‡Œä½¿ç”¨æ ‡å‡† json.loads è€Œä¸æ˜¯ Pydantic æ¨¡å‹ï¼Œ
    # å› ä¸º report_agent è¾“å‡ºçš„ JSON ä¸å« meta å­—æ®µï¼Œ
    # meta ç”±è°ƒç”¨æ–¹å¡«å……åå†ç»„è£…ä¸º WebResearchResult
    try:
        thread = report_agent.get_new_thread()
        response = await report_agent.run(message=user_message, thread=thread)
        raw_text = response.text
        json_str = extract_json_str(raw_text)
        report_dict = json.loads(json_str)  # ä½¿ç”¨æ ‡å‡† json.loadsï¼Œä¸æ˜¯ Pydantic
        return report_dict
    except json.JSONDecodeError as e:
        # json.loads() æŠ›å‡º JSONDecodeErrorï¼ˆæ ‡å‡†åº“å¼‚å¸¸ï¼‰
        logger.error(f"Report generation failed: invalid JSON - {e}")
        raise ReportGenerationError(
            symbol=symbol, 
            cause=e,
            learnings_count=len(learnings)
        ) from e
    except Exception as e:
        logger.error(f"Report generation failed: {e}")
        raise ReportGenerationError(
            symbol=symbol,
            cause=e,
            learnings_count=len(learnings)
        ) from e
```

---

## å…­ã€æ¨¡å—å…¥å£ â€” run_web_research

**æ³¨æ„ï¼š** å¼‚å¸¸ç±»ç»Ÿä¸€å®šä¹‰äº `stock_analyzer/exceptions.py`ï¼Œå„æ¨¡å—ä»… importï¼Œä¸é‡å¤å®šä¹‰ã€‚

```python
import asyncio
import json
from datetime import datetime
from stock_analyzer.logger import logger
from stock_analyzer.exceptions import (
    WebResearchError,
    ReportGenerationError,
)


async def run_web_research(
    symbol: str,
    name: str,
    industry: str,
    breadth: int = DEFAULT_BREADTH,
    depth: int = DEFAULT_DEPTH,
) -> WebResearchResult:
    """
    æ¨¡å—B å¯¹å¤–å…¥å£ï¼šå¯¹æŒ‡å®šè‚¡ç¥¨æ‰§è¡Œç½‘ç»œæ·±åº¦æœç´¢ï¼Œè¿”å›ç»“æ„åŒ–ç ”ç©¶æŠ¥å‘Šã€‚

    æµç¨‹ï¼š
    1. åˆå§‹åŒ– LLM å®¢æˆ·ç«¯å’Œ 3 ä¸ª Agent
    2. å®šä¹‰ 5 ä¸ªæœç´¢ä¸»é¢˜
    3. å¹¶è¡Œæ‰§è¡Œ 5 ä¸ªä¸»é¢˜çš„ deep_research
    4. åˆå¹¶å»é‡æ‰€æœ‰ learnings
    5. è°ƒç”¨ report_agent ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Šï¼ˆå¤±è´¥æ—¶ä½¿ç”¨é™çº§æŠ¥å‘Šï¼‰
    6. ç»„è£… meta ä¿¡æ¯ï¼Œå¼ºåˆ¶é™çº§æ£€æŸ¥ï¼ˆlearnings < 5 æ—¶ç½®ä¿¡åº¦æ ‡è®°ä¸º"ä½"ï¼‰
    7. è¿”å› WebResearchResult

    Args:
        symbol: è‚¡ç¥¨ä»£ç ï¼ˆçº¯6ä½æ•°å­—ï¼‰
        name: è‚¡ç¥¨åç§°
        industry: æ‰€å±è¡Œä¸š
        breadth: æ¯ä¸»é¢˜æ¯è½®æŸ¥è¯¢æ•°ï¼ˆé»˜è®¤3ï¼‰
        depth: æ¯ä¸»é¢˜é€’å½’æ·±åº¦ï¼ˆé»˜è®¤2ï¼‰

    Returns:
        WebResearchResultï¼ˆPydantic æ¨¡å‹å¯¹è±¡ï¼‰
        
    æ³¨æ„ï¼š
        æœ¬å‡½æ•°åªè¿”å›æ¨¡å‹å¯¹è±¡ï¼Œä¸è‡ªåŠ¨ä¿å­˜ä¸ºJSONæ–‡ä»¶ã€‚
        å¦‚éœ€ä¿å­˜ï¼Œè°ƒç”¨æ–¹åº”ä½¿ç”¨ï¼š
        result.model_dump_json() æˆ– json.dumps(result.model_dump())
    """
    start_time = datetime.now()
    logger.info(f"Starting web research for {symbol} {name} ({industry})")

    # â”€â”€ Step 1: åˆå§‹åŒ– â”€â”€
    openai_client = create_openai_client()

    query_client = create_chat_client(openai_client, MODEL_QUERY_AGENT)
    extract_client = create_chat_client(openai_client, MODEL_EXTRACT_AGENT)
    report_client = create_chat_client(openai_client, MODEL_REPORT_AGENT)

    query_agent = create_query_agent(query_client)
    extract_agent = create_extract_agent(extract_client)
    report_agent = create_report_agent(report_client)

    # â”€â”€ Step 2: å®šä¹‰æœç´¢ä¸»é¢˜ â”€â”€
    topics = [
        (
            f"{name}ï¼ˆè‚¡ç¥¨ä»£ç {symbol}ï¼‰è¿‘æœŸé‡å¤§æ–°é—»ï¼Œ"
            f"åŒ…æ‹¬æ­£é¢å’Œè´Ÿé¢å½±å“è‚¡ä»·çš„äº‹ä»¶"
        ),
        (
            f"{name} æ ¸å¿ƒç«äº‰åŠ›åˆ†æï¼ŒæŠ¤åŸæ²³ç±»å‹ï¼Œ"
            f"åœ¨{industry}è¡Œä¸šä¸­çš„å¸‚åœºåœ°ä½å’Œç«äº‰ä¼˜åŠ¿"
        ),
        (
            f"{industry}è¡Œä¸šå‘å±•å‰æ™¯ï¼Œæ”¿ç­–ç¯å¢ƒï¼Œå¸‚åœºè¶‹åŠ¿ï¼Œ"
            f"è¡Œä¸šå¢é•¿é©±åŠ¨åŠ›å’Œä¸»è¦é£é™©"
        ),
        (
            f"{name} é£é™©äº‹ä»¶ï¼ŒåŒ…æ‹¬ç›‘ç®¡å¤„ç½šã€è¯‰è®¼çº çº·ã€"
            f"ç®¡ç†å±‚å˜åŠ¨ã€è´¢åŠ¡é£é™©ç­‰è´Ÿé¢ä¿¡æ¯"
        ),
        (
            f"{name} åˆ¸å•†ç ”æŠ¥ã€æœºæ„è¯„çº§ã€ç›®æ ‡ä»·ã€"
            f"åˆ†æå¸ˆå¯¹è¯¥è‚¡çš„æŠ•èµ„è§‚ç‚¹å’Œè¯„çº§å˜åŒ–"
        ),
    ]

    # â”€â”€ Step 3: å¹¶è¡Œæ‰§è¡Œ Deep Research â”€â”€
    semaphore = asyncio.Semaphore(TOPIC_CONCURRENCY_LIMIT)

    async def research_topic(topic: str) -> ResearchResult:
        async with semaphore:
            return await deep_research(
                query_agent=query_agent,
                extract_agent=extract_agent,
                query=topic,
                breadth=breadth,
                depth=depth,
            )

    tasks = [research_topic(topic) for topic in topics]
    results = await asyncio.gather(*tasks, return_exceptions=True)

    # â”€â”€ Step 4: åˆå¹¶å»é‡ + æˆåŠŸç‡æ£€æŸ¥ â”€â”€
    all_learnings: set[str] = set()
    all_urls: set[str] = set()

    for i, result in enumerate(results):
        if isinstance(result, ResearchResult):
            all_learnings.update(result.learnings)
            all_urls.update(result.visited_urls)
            logger.info(
                f"Topic {i+1} completed: "
                f"{len(result.learnings)} learnings, "
                f"{len(result.visited_urls)} urls"
            )
        elif isinstance(result, asyncio.CancelledError):
            # å–æ¶ˆå¼‚å¸¸å¿…é¡»å‘ä¸Šä¼ æ’­ï¼Œä¸èƒ½è¢«åæ‰
            logger.warning(f"Topic {i+1} was cancelled, propagating CancelledError")
            raise result
        elif isinstance(result, Exception):
            logger.error(f"Topic {i+1} failed: {result}")

    unique_learnings = list(all_learnings)
    unique_urls = list(all_urls)

    # æˆåŠŸä¸»é¢˜æ•°ç»Ÿè®¡ï¼ˆå¿…é¡»æ˜¯ ResearchResult ä¸”æœ‰æœ‰æ•ˆ learningsï¼‰
    successful_topics = sum(
        1 for r in results 
        if isinstance(r, ResearchResult) and len(r.learnings) > 0
    )
    
    # å…¨éƒ¨ä¸»é¢˜å¤±è´¥ä¿æŠ¤
    if successful_topics == 0:
        raise WebResearchError(
            f"All {len(topics)} topics failed, cannot generate report for {symbol}"
        )
    
    # çŸ¥è¯†ç‚¹æ•°é‡è¿‡å°‘è­¦å‘Šï¼ˆåç»­ä¼šåœ¨ Step 6 å¼ºåˆ¶æ ‡è®°ä¸ºä½ç½®ä¿¡åº¦ï¼‰
    if len(unique_learnings) < 5:
        logger.warning(
            f"Only {len(unique_learnings)} learnings collected "
            f"({successful_topics}/{len(topics)} topics succeeded), "
            f"report quality may be low"
        )

    logger.info(
        f"All topics done: {len(unique_learnings)} unique learnings, "
        f"{len(unique_urls)} unique urls, "
        f"{successful_topics}/{len(topics)} topics succeeded"
    )

    # â”€â”€ Step 5: ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Šï¼ˆå¸¦é™çº§å¤„ç†ï¼‰â”€â”€
    is_fallback = False
    try:
        report_dict = await generate_report(
            report_agent=report_agent,
            symbol=symbol,
            name=name,
            industry=industry,
            learnings=unique_learnings,
        )
    except ReportGenerationError as e:
        logger.error(f"Report generation failed, using fallback: {e}")
        # é™çº§ï¼šè¿”å›åŸºç¡€ç»“æ„ï¼Œæ ‡è®°ä¸ºä½å¯ä¿¡åº¦
        is_fallback = True
        report_dict = _create_fallback_report(
            learnings=unique_learnings,
            error_message=str(e.cause)
        )

    # â”€â”€ Step 6: ç»„è£… meta å¹¶æ„å»ºæœ€ç»ˆè¾“å‡º â”€â”€
    meta = SearchMeta(
        symbol=symbol,
        name=name,
        search_time=start_time.isoformat(),
        search_config=SearchConfig(
            topics_count=len(topics),
            breadth=breadth,
            depth=depth,
            successful_topics=successful_topics,  # è®°å½•æˆåŠŸä¸»é¢˜æ•°
        ),
        total_learnings=len(unique_learnings),
        total_sources_consulted=len(unique_urls),
        raw_learnings=unique_learnings if is_fallback else None,  # é™çº§æ—¶ä¿å­˜åŸå§‹ learnings
    )

    report_dict["meta"] = meta.model_dump()
    
    # å¼ºåˆ¶é™çº§é€»è¾‘ï¼šlearnings è¿‡å°‘æ—¶ï¼Œå¼ºåˆ¶æ ‡è®°ä¸ºä½ç½®ä¿¡åº¦
    if len(unique_learnings) < 5:
        logger.warning(f"Forcing search_confidence to 'ä½' due to insufficient learnings: {len(unique_learnings)}")
        report_dict["search_confidence"] = "ä½"
    
    try:
        final_result = WebResearchResult.model_validate(report_dict)
    except Exception as e:
        logger.error(f"Final result validation failed: {e}")
        raise WebResearchError(
            f"Failed to validate final report for {symbol}: {e}"
        ) from e

    elapsed = (datetime.now() - start_time).total_seconds()
    logger.info(f"Web research completed in {elapsed:.1f}s")

    return final_result


def _create_fallback_report(learnings: list[str], error_message: str) -> dict:
    """
    å½“æŠ¥å‘Šç”Ÿæˆå¤±è´¥æ—¶ï¼Œåˆ›å»ºé™çº§æŠ¥å‘Šã€‚
    
    è¿”å›æœ€å°åŒ–çš„æŠ¥å‘Šç»“æ„ã€‚åŸå§‹ learnings åˆ—è¡¨ä¸åœ¨æ­¤å¤„è¿”å›ï¼Œ
    è€Œæ˜¯ç”± run_web_research() åœ¨ç»„è£… meta æ—¶å¡«å……åˆ° meta.raw_learnings å­—æ®µã€‚
    
    æ³¨æ„ï¼š
    - é‡‡ç”¨"æ‹¼æ¥åç»Ÿä¸€æˆªæ–­"ç­–ç•¥ï¼Œç¡®ä¿ description æœ€ç»ˆé•¿åº¦ <= 500 å­—ç¬¦
    - é€»è¾‘ç®€å•æ¸…æ™°ï¼Œæ˜“äºæµ‹è¯•å’Œç»´æŠ¤
    """
    # æ‹¼æ¥ base_msg å’Œ error_message
    base_msg = f"æŠ¥å‘Šç”Ÿæˆå¤±è´¥ï¼Œå…±æ”¶é›†åˆ° {len(learnings)} æ¡ä¿¡æ¯ï¼Œè¯·æŸ¥çœ‹åŸå§‹æ•°æ®ã€‚é”™è¯¯ï¼š"
    description = base_msg + error_message
    
    # ç»Ÿä¸€æˆªæ–­ï¼šç¡®ä¿æœ€ç»ˆé•¿åº¦ä¸è¶…è¿‡ 500
    if len(description) > 500:
        description = description[:497] + "..."
    
    return {
        "news_summary": {
            "positive": [],
            "negative": [],
            "neutral": [],
        },
        "competitive_advantage": {
            "description": description,  # æœ€ç»ˆé•¿åº¦ä¿è¯ <= 500
            "moat_type": "æœªçŸ¥",
            "market_position": "æœªçŸ¥",
        },
        "industry_outlook": {
            "industry": "æœªçŸ¥",
            "outlook": "æœªçŸ¥",
            "key_drivers": [],
            "key_risks": [],
        },
        "risk_events": {
            "regulatory": "æŠ¥å‘Šç”Ÿæˆå¤±è´¥",
            "litigation": "æŠ¥å‘Šç”Ÿæˆå¤±è´¥",
            "management": "æŠ¥å‘Šç”Ÿæˆå¤±è´¥",
            "other": f"åŸå§‹learningsæ•°é‡: {len(learnings)}",
        },
        "analyst_opinions": {
            "buy_count": 0,
            "hold_count": 0,
            "sell_count": 0,
            "average_target_price": None,
            "recent_reports": [],
        },
        "search_confidence": "ä½",
    }
```

---

## ä¸ƒã€å®Œæ•´æ‰§è¡Œç¤ºä¾‹

### 7.1 åœºæ™¯è®¾ç½®

**è¾“å…¥ï¼š** `symbol="000001"`, `name="å¹³å®‰é“¶è¡Œ"`, `industry="é“¶è¡Œ"`
**å‚æ•°ï¼š** `breadth=3, depth=2`

### 7.2 ä¸»é¢˜ T1ï¼ˆè¿‘æœŸæ–°é—»ï¼‰æ‰§è¡Œè¿‡ç¨‹

#### ç¬¬1å±‚ (depth=2, breadth=3)

**generate_serp_queries è¾“å…¥ï¼š**

```
è¯·ä¸ºä»¥ä¸‹ç ”ç©¶ä¸»é¢˜ç”Ÿæˆ 3 ä¸ªæœç´¢æŸ¥è¯¢ã€‚

<topic>
å¹³å®‰é“¶è¡Œï¼ˆè‚¡ç¥¨ä»£ç 000001ï¼‰è¿‘æœŸé‡å¤§æ–°é—»ï¼ŒåŒ…æ‹¬æ­£é¢å’Œè´Ÿé¢å½±å“è‚¡ä»·çš„äº‹ä»¶
</topic>
```

**generate_serp_queries è¾“å‡ºï¼š**

```json
{
  "queries": [
    {
      "query": "å¹³å®‰é“¶è¡Œ 2025å¹´ æœ€æ–°æ¶ˆæ¯ ä¸šç»©",
      "research_goal": "äº†è§£å¹³å®‰é“¶è¡Œæœ€æ–°çš„ä¸šç»©è¡¨ç°å’Œç»è¥åŠ¨æ€"
    },
    {
      "query": "å¹³å®‰é“¶è¡Œ åˆ©å¥½ åˆ©ç©º é‡å¤§äº‹ä»¶ 2025",
      "research_goal": "æœç´¢è¿‘æœŸå½±å“è‚¡ä»·çš„æ­£é¢å’Œè´Ÿé¢äº‹ä»¶"
    },
    {
      "query": "å¹³å®‰é“¶è¡Œ è‚¡ä»· å¼‚åŠ¨ å…¬å‘Š",
      "research_goal": "æŸ¥æ‰¾è¿‘æœŸè‚¡ä»·å¼‚åŠ¨ç›¸å…³å…¬å‘Šå’Œå¸‚åœºååº”"
    }
  ]
}
```

**Tavily æœç´¢ï¼ˆ3æ¬¡å¹¶è¡Œï¼‰â†’ extract_agent æå–ï¼ˆ3æ¬¡å¹¶è¡Œï¼‰**

**æå–ç»“æœï¼ˆç¤ºä¾‹ï¼ŒæŸ¥è¯¢Aï¼‰ï¼š**

```json
{
  "learnings": [
    "å¹³å®‰é“¶è¡Œ2024å¹´å…¨å¹´å‡€åˆ©æ¶¦445.1äº¿å…ƒï¼ŒåŒæ¯”å¢é•¿2.1%ï¼Œè¥æ”¶1600äº¿å…ƒåŒæ¯”ä¸‹é™8.5%",
    "å¹³å®‰é“¶è¡Œ2024å¹´é›¶å”®AUMçªç ´4.2ä¸‡äº¿å…ƒï¼Œä¿¡ç”¨å¡æµé€šå¡é‡è¶…6500ä¸‡å¼ ",
    "å¹³å®‰é“¶è¡Œ2024Q4ä¸è‰¯è´·æ¬¾ç‡1.06%ï¼Œè¾ƒQ3ä¸‹é™2ä¸ªåŸºç‚¹ï¼Œæ‹¨å¤‡è¦†ç›–ç‡261%"
  ],
  "follow_up_questions": [
    "å¹³å®‰é“¶è¡Œè¥æ”¶ä¸‹é™8.5%çš„ä¸»è¦åŸå› æ˜¯ä»€ä¹ˆï¼Ÿ",
    "å¹³å®‰é“¶è¡Œé›¶å”®è½¬å‹å¯¹åˆ©æ¶¦è´¡çŒ®å¦‚ä½•ï¼Ÿ",
    "å¹³å®‰é“¶è¡Œæˆ¿åœ°äº§è´·æ¬¾æ•å£è§„æ¨¡åŠé£é™©çŠ¶å†µï¼Ÿ"
  ]
}
```

**ç¬¬1å±‚ learnings ç´¯è®¡ï¼š** ~9 ä¸ªï¼ˆ3 ä¸ªæŸ¥è¯¢ Ã— ~3 ä¸ª learningsï¼‰

#### ç¬¬2å±‚ (depth=1, breadth=1)

3 ä¸ªåˆ†æ”¯å„è‡ªç‹¬ç«‹é€’å½’ï¼Œæ¯ä¸ªåˆ†æ”¯ç”Ÿæˆ 1 ä¸ªæ›´ç²¾å‡†çš„æŸ¥è¯¢ã€‚

**åˆ†æ”¯A â†’ generate_serp_queries è¾“å…¥ï¼ˆå¸¦ learningsï¼‰ï¼š**

```
è¯·ä¸ºä»¥ä¸‹ç ”ç©¶ä¸»é¢˜ç”Ÿæˆ 1 ä¸ªæœç´¢æŸ¥è¯¢ã€‚

<topic>
Previous research goal: äº†è§£å¹³å®‰é“¶è¡Œæœ€æ–°çš„ä¸šç»©è¡¨ç°å’Œç»è¥åŠ¨æ€
Follow-up research directions:
- å¹³å®‰é“¶è¡Œè¥æ”¶ä¸‹é™8.5%çš„ä¸»è¦åŸå› æ˜¯ä»€ä¹ˆï¼Ÿ
- å¹³å®‰é“¶è¡Œé›¶å”®è½¬å‹å¯¹åˆ©æ¶¦è´¡çŒ®å¦‚ä½•ï¼Ÿ
- å¹³å®‰é“¶è¡Œæˆ¿åœ°äº§è´·æ¬¾æ•å£è§„æ¨¡åŠé£é™©çŠ¶å†µï¼Ÿ
</topic>

ä»¥ä¸‹æ˜¯å‰å‡ è½®ç ”ç©¶ä¸­å·²è·å¾—çš„çŸ¥è¯†ç‚¹ï¼Œè¯·æ®æ­¤ç”Ÿæˆæ›´æœ‰é’ˆå¯¹æ€§çš„æŸ¥è¯¢ï¼Œé¿å…æœç´¢å·²çŸ¥ä¿¡æ¯ï¼š
<learnings>
- å¹³å®‰é“¶è¡Œ2024å¹´å…¨å¹´å‡€åˆ©æ¶¦445.1äº¿å…ƒï¼ŒåŒæ¯”å¢é•¿2.1%ï¼Œè¥æ”¶1600äº¿å…ƒåŒæ¯”ä¸‹é™8.5%
- å¹³å®‰é“¶è¡Œ2024å¹´é›¶å”®AUMçªç ´4.2ä¸‡äº¿å…ƒï¼Œä¿¡ç”¨å¡æµé€šå¡é‡è¶…6500ä¸‡å¼ 
- ... (å…±çº¦9æ¡)
</learnings>
```

**generate_serp_queries è¾“å‡ºï¼š**

```json
{
  "queries": [
    {
      "query": "å¹³å®‰é“¶è¡Œ 2024å¹´æŠ¥ è¥æ”¶ä¸‹é™åŸå›  å‡€æ¯å·® æ‰‹ç»­è´¹æ”¶å…¥",
      "research_goal": "æ·±å…¥äº†è§£è¥æ”¶ä¸‹é™çš„å…·ä½“æ„æˆå› ç´ "
    }
  ]
}
```

**ç¬¬2å±‚ learnings ç´¯è®¡ï¼š** ~18 ä¸ªï¼ˆå»é‡å ~15 ä¸ªï¼‰

**depth é™ä¸º 0ï¼Œåœæ­¢é€’å½’ã€‚**

### 7.3 æ‰€æœ‰ä¸»é¢˜å®Œæˆååˆå¹¶

| ä¸»é¢˜ | åŸå§‹ learnings | å»é‡å |
|------|---------------|--------|
| T1 è¿‘æœŸæ–°é—» | ~18 | ~15 |
| T2 ç«äº‰ä¼˜åŠ¿ | ~18 | ~14 |
| T3 è¡Œä¸šå‰æ™¯ | ~18 | ~16 |
| T4 é£é™©äº‹ä»¶ | ~18 | ~12 |
| T5 æœºæ„è§‚ç‚¹ | ~18 | ~13 |
| **åˆå¹¶** | ~90 | **~60** |

### 7.4 ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š

**report_agent è¾“å…¥ï¼ˆèŠ‚é€‰ï¼‰ï¼š**

```
è¯·ä¸ºè‚¡ç¥¨ 000001 å¹³å®‰é“¶è¡Œï¼ˆé“¶è¡Œè¡Œä¸šï¼‰ç”Ÿæˆç»“æ„åŒ–çš„ç½‘ç»œæ·±åº¦æœç´¢ç ”ç©¶æŠ¥å‘Šã€‚

ä»¥ä¸‹æ˜¯é€šè¿‡å¤šè½®æ·±åº¦æœç´¢ç§¯ç´¯çš„å…¨éƒ¨ 60 ä¸ªçŸ¥è¯†ç‚¹ï¼š

<learnings>
1. å¹³å®‰é“¶è¡Œ2024å¹´å…¨å¹´å‡€åˆ©æ¶¦445.1äº¿å…ƒï¼ŒåŒæ¯”å¢é•¿2.1%ï¼Œè¥æ”¶1600äº¿å…ƒåŒæ¯”ä¸‹é™8.5%
2. å¹³å®‰é“¶è¡Œ2024å¹´é›¶å”®AUMçªç ´4.2ä¸‡äº¿å…ƒï¼Œä¿¡ç”¨å¡æµé€šå¡é‡è¶…6500ä¸‡å¼ 
3. å¹³å®‰é“¶è¡Œ2024Q4ä¸è‰¯è´·æ¬¾ç‡1.06%ï¼Œè¾ƒQ3ä¸‹é™2ä¸ªåŸºç‚¹ï¼Œæ‹¨å¤‡è¦†ç›–ç‡261%
... å…±60æ¡ ...
</learnings>
```

**report_agent è¾“å‡ºï¼š** ç»“æ„åŒ– JSON â†’ è§£æä¸º `WebResearchResult`

---

## å…«ã€å¼‚å¸¸å¤„ç†è®¾è®¡

### 8.1 å¼‚å¸¸åˆ†ç±»ä¸å¤„ç†ç­–ç•¥

| å¼‚å¸¸ç±»å‹ | æ¥æº | å¤„ç†ç­–ç•¥ | è¯´æ˜ |
|---------|------|---------|------|
| `TavilySearchError` | Tavily API è°ƒç”¨å¤±è´¥ | è·³è¿‡è¯¥æŸ¥è¯¢åˆ†æ”¯ï¼Œç»§ç»­å…¶ä»–åˆ†æ”¯ | å°è£…æ‰€æœ‰ Tavily ç›¸å…³å¼‚å¸¸ |
| **å¯é‡è¯•å¼‚å¸¸** | | | **å¼‚å¸¸åˆ†æ´¾ä¼˜åŒ–** |
| `asyncio.TimeoutError` | `asyncio.wait_for()` è¶…æ—¶ | **å•ç‹¬ except åˆ†æ”¯**ï¼Œè‡ªåŠ¨é‡è¯•1æ¬¡ | âš ï¸ ä¸åœ¨ `RETRYABLE_EXCEPTIONS` å…ƒç»„ä¸­ï¼ˆé¿å…é‡å¤ï¼‰ |
| `ConnectionError` | ç½‘ç»œè¿æ¥å¤±è´¥ï¼ˆæ ‡å‡†åº“ï¼‰ | åœ¨ `tavily_search()` å†…è‡ªåŠ¨é‡è¯•1æ¬¡ | âœ… åœ¨ `RETRYABLE_EXCEPTIONS` ä¸­ |
| `OSError` | ç½‘ç»œ I/O é”™è¯¯ï¼ˆæ ‡å‡†åº“ï¼‰ | åœ¨ `tavily_search()` å†…è‡ªåŠ¨é‡è¯•1æ¬¡ | âœ… åœ¨ `RETRYABLE_EXCEPTIONS` ä¸­ |
| `httpx.NetworkError` | HTTP ç½‘ç»œé”™è¯¯ | åœ¨ `tavily_search()` å†…è‡ªåŠ¨é‡è¯•1æ¬¡ | âœ… **å·²å¯ç”¨**ï¼Œé¡¹ç›®ä¾èµ– httpx>=0.28.1 |
| `httpx.ConnectError` | HTTP è¿æ¥é”™è¯¯ | åœ¨ `tavily_search()` å†…è‡ªåŠ¨é‡è¯•1æ¬¡ | âœ… **å·²å¯ç”¨**ï¼Œé¡¹ç›®ä¾èµ– httpx>=0.28.1 |
| `httpx.TimeoutException` | HTTP è¶…æ—¶ | åœ¨ `tavily_search()` å†…è‡ªåŠ¨é‡è¯•1æ¬¡ | âœ… **å·²å¯ç”¨**ï¼Œé¡¹ç›®ä¾èµ– httpx>=0.28.1 |
| `httpx.RemoteProtocolError` | HTTP/2 åè®®é”™è¯¯ | åœ¨ `tavily_search()` å†…è‡ªåŠ¨é‡è¯•1æ¬¡ | âœ… **å·²å¯ç”¨**ï¼Œé¡¹ç›®ä¾èµ– httpx>=0.28.1 |
| **ä¸å¯é‡è¯•å¼‚å¸¸ï¼ˆNON_RETRYABLE_EXCEPTIONSï¼‰** | | | **ç‹¬ç«‹åˆ†æ”¯å¤„ç†** |
| `httpx.HTTPStatusError` | HTTP 4xx/5xx çŠ¶æ€ç é”™è¯¯ | ä¸é‡è¯•ï¼Œç‹¬ç«‹åˆ†æ”¯è®°å½•æ—¥å¿—å¹¶ç«‹å³å¤±è´¥ | âœ… **å·²å¯ç”¨**ï¼Œé¡¹ç›®ä¾èµ– httpx>=0.28.1 |
| `InvalidAPIKeyError` | API å¯†é’¥é”™è¯¯ | ä¸é‡è¯•ï¼Œç‹¬ç«‹åˆ†æ”¯è®°å½•æ—¥å¿—å¹¶ç«‹å³å¤±è´¥ | âœ… **å·²å¯ç”¨**ï¼Œv0.7.21 @ 38627af |
| `MissingAPIKeyError` | ç¼ºå°‘ API å¯†é’¥ | ä¸é‡è¯•ï¼Œç‹¬ç«‹åˆ†æ”¯è®°å½•æ—¥å¿—å¹¶ç«‹å³å¤±è´¥ | âœ… **å·²å¯ç”¨**ï¼Œv0.7.21 @ 38627af |
| `BadRequestError` | è¯·æ±‚æ ¼å¼é”™è¯¯ | ä¸é‡è¯•ï¼Œç‹¬ç«‹åˆ†æ”¯è®°å½•æ—¥å¿—å¹¶ç«‹å³å¤±è´¥ | âœ… **å·²å¯ç”¨**ï¼Œv0.7.21 @ 38627af |
| `ForbiddenError` | æƒé™ä¸è¶³ | ä¸é‡è¯•ï¼Œç‹¬ç«‹åˆ†æ”¯è®°å½•æ—¥å¿—å¹¶ç«‹å³å¤±è´¥ | âœ… **å·²å¯ç”¨**ï¼Œv0.7.21 @ 38627af |
| `UsageLimitExceededError` | ä½¿ç”¨é™åˆ¶è¶…å‡º | ä¸é‡è¯•ï¼Œç‹¬ç«‹åˆ†æ”¯è®°å½•æ—¥å¿—å¹¶ç«‹å³å¤±è´¥ | âœ… **å·²å¯ç”¨**ï¼Œv0.7.21 @ 38627af |
| **å·²ç¡®è®¤ä½†ä¸å¯ç”¨çš„å¼‚å¸¸** | | | **ä¿æŒæ³¨é‡Šæ€** |
| ~~`TimeoutError`~~ (Tavily) | ~~è¶…æ—¶é”™è¯¯~~ | ä¿æŒæ³¨é‡Šæ€ | âš ï¸ **å·²ç¡®è®¤ä½†ä¸å¯ç”¨**ï¼Œä¸æ ‡å‡†åº“é‡å |
| ~~`NetworkError`~~ (Tavily) | ~~ç½‘ç»œé”™è¯¯~~ | N/A | âŒ **ä¸å­˜åœ¨**ï¼Œåœ¨ Tavily SDK v0.7.21 æºç ä¸­æœªæ‰¾åˆ° |
| **å…¶ä»–å¼‚å¸¸ï¼ˆé€šç”¨ Exception åˆ†æ”¯ï¼‰** | | | |
| ç¼–ç¨‹é”™è¯¯ï¼ˆ`KeyError` ç­‰ï¼‰ | ä»£ç ç¼ºé™· | N/A | **ä¸é‡è¯•**ï¼Œé€šç”¨åˆ†æ”¯å¤„ç† |
| å…¶ä»–æœªé¢„æœŸå¼‚å¸¸ | æœªåˆ†ç±» | N/A | **ä¸é‡è¯•**ï¼Œé€šç”¨åˆ†æ”¯å¤„ç† |

**å›¾ä¾‹è¯´æ˜ï¼š**
- âœ… **å·²å¯ç”¨/å·²ç¡®è®¤å¯ç”¨**ï¼šå·²åœ¨ä»£ç ä¸­çš„å¼‚å¸¸å¤„ç†é…ç½®ï¼ˆ`RETRYABLE_EXCEPTIONS` æˆ– `NON_RETRYABLE_EXCEPTIONS`ï¼‰æ˜ç¡®å®šä¹‰ï¼Œé¡¹ç›®ä¾èµ–å·²æ»¡è¶³ï¼Œ**ä¸è¦åˆ é™¤**
- âŒ **ä¸å­˜åœ¨/ä¸å¯ç”¨**ï¼šç»å®˜æ–¹æºç ç¡®è®¤ä¸å­˜åœ¨è¯¥å¼‚å¸¸ç±»å‹ï¼Œæˆ–å› å‘½åå†²çªç­‰åŸå› ä¸å¯ç”¨
- ~~åˆ é™¤çº¿~~ï¼šè¡¨ç¤ºè¯¥å¼‚å¸¸ä¸å¯ç”¨æˆ–ä¸åº”ä½¿ç”¨

**\* å®æ–½è¯´æ˜ï¼ˆhttpx å¼‚å¸¸ - å·²å¼ºåˆ¶å¯ç”¨ï¼‰ï¼š**
1. âœ… **é¡¹ç›®ä¾èµ–å·²æ»¡è¶³**ï¼š`httpx>=0.28.1`ï¼ˆè§ `pyproject.toml`ï¼‰
2. âœ… **ä»£ç ä¸­å·²å¯¼å…¥**ï¼š`import httpx`ï¼ˆå¿…é¡»ä¿ç•™ï¼‰
3. âœ… **å¼‚å¸¸é…ç½®å·²å®Œæˆ**ï¼š
   - `RETRYABLE_EXCEPTIONS` ä¸­å·²åŒ…å« `httpx.ConnectError`, `httpx.NetworkError`, `httpx.TimeoutException`, `httpx.RemoteProtocolError`
   - `NON_RETRYABLE_EXCEPTIONS` ä¸­å·²åŒ…å« `httpx.HTTPStatusError`
4. âš ï¸ **ç»´æŠ¤æ³¨æ„**ï¼šä¸è¦åˆ é™¤ httpx ç›¸å…³å¼‚å¸¸ï¼Œå¦åˆ™ç½‘ç»œé”™è¯¯ä¸ä¼šé‡è¯•æˆ– API é”™è¯¯å¤„ç†ä¸å½“

**\*\* å®æ–½è¯´æ˜ï¼ˆTavily SDK å¼‚å¸¸ - âœ… å·²å¯ç”¨å»ºè®®å¯ç”¨çš„å­é›†ï¼‰ï¼š**

**ç¡®è®¤æ¥æºï¼š** åŸºäº [Tavily SDK v0.7.21 å®˜æ–¹æºç ](https://github.com/tavily-ai/tavily-python/blob/38627afb7b88d8a57bad29380896210a9ae7badd/tavily/errors.py)ï¼ˆcommit `38627af`ï¼Œ2026-01-28ï¼‰

**å¼‚å¸¸é…ç½®å·²å®Œæˆï¼š**
1. âœ… **å¯¼å…¥è¯­å¥**ï¼šå·²æ·»åŠ  `from tavily import InvalidAPIKeyError, BadRequestError, ForbiddenError, MissingAPIKeyError, UsageLimitExceededError`ï¼ˆå…±5ä¸ªï¼‰
2. âœ… **NON_RETRYABLE_EXCEPTIONS å…ƒç»„**ï¼šå·²å¯ç”¨5ä¸ªå»ºè®®å¯ç”¨çš„ Tavily å¼‚å¸¸ï¼ˆè§ä¸Šè¡¨ï¼‰
3. âš ï¸ **å·²ç¡®è®¤ä½†ä¸å¯ç”¨**ï¼šTimeoutErrorï¼ˆä¸æ ‡å‡†åº“é‡åï¼Œä½¿ç”¨ `asyncio.TimeoutError` æ›¿ä»£ï¼‰
4. âœ… **ç‹¬ç«‹ except åˆ†æ”¯**ï¼šåœ¨ `tavily_search()` ä¸­å­˜åœ¨ `except NON_RETRYABLE_EXCEPTIONS` åˆ†æ”¯ï¼Œä½äº `except Exception` ä¹‹å‰

**ğŸ”’ ç¡¬æ€§è§„åˆ™ï¼ˆé¿å…å®æ–½æ­§ä¹‰ï¼‰ï¼š**
- **è‹¥å®˜æ–¹æºç ç¡®è®¤å­˜åœ¨ ä¸” æ¨èå¯ç”¨**ï¼š
  - æ·»åŠ åˆ° `RETRYABLE_EXCEPTIONS` æˆ– `NON_RETRYABLE_EXCEPTIONS` å…ƒç»„
  - è‹¥ä½¿ç”¨æ³¨é‡Šæ¨¡æ¿åˆ™å–æ¶ˆæ³¨é‡Šï¼Œç›´æ¥å®æ–½æ—¶å¯è·³è¿‡æ³¨é‡Šæ­¥éª¤
  - ç¡®ä¿åœ¨æ–‡ä»¶é¡¶éƒ¨æ·»åŠ ç›¸åº”çš„ `import` æˆ– `from ... import` è¯­å¥
- **è‹¥å®˜æ–¹æºç ç¡®è®¤å­˜åœ¨ ä½† ä¸å»ºè®®å¯ç”¨**ï¼ˆå¦‚ä¸æ ‡å‡†åº“é‡åï¼‰ï¼š
  - **ä¿æŒæ³¨é‡Šæ€**ï¼Œåœ¨æ³¨é‡Šä¸­è¯´æ˜ä¸å¯ç”¨åŸå› ï¼ˆè§ `TimeoutError` ç¤ºä¾‹ï¼‰
  - ä¸æ·»åŠ åˆ°å¼‚å¸¸å…ƒç»„ï¼Œä¸å¯¼å…¥
- **è‹¥å®˜æ–¹æºç ç¡®è®¤ä¸å­˜åœ¨è¯¥å¼‚å¸¸ç±»**ï¼š
  - **åˆ é™¤æ³¨é‡Š**ï¼ˆæˆ–æ ‡æ³¨ âŒ ä¸å­˜åœ¨ï¼‰
  - **ä¸æ–°å¢æ•è·åˆ†æ”¯**ï¼Œç»Ÿä¸€ç”±é€šç”¨ `except Exception` åˆ†æ”¯å…œåº•
- **ç¦æ­¢"çŒœæµ‹æ€§æ·»åŠ "**ï¼šæ‰€æœ‰å¼‚å¸¸ç±»å¿…é¡»ç»å®˜æ–¹æ–‡æ¡£æˆ–æºç ç¡®è®¤åæ‰èƒ½å¯ç”¨ï¼Œä¸”éœ€è¯„ä¼°æ˜¯å¦é€‚åˆå¯ç”¨

**å‘½åè§„èŒƒè¯´æ˜ï¼ˆâœ… åŸºäºå®˜æ–¹æ–‡æ¡£ v0.7.21ï¼‰ï¼š**

**é€šç”¨è§„åˆ™ï¼ˆé€‚ç”¨äºå¤§å¤šæ•° Tavily å¼‚å¸¸ï¼‰ï¼š**
- âœ… **æ¨èå†™æ³•**ï¼š`from tavily import InvalidAPIKeyError` + ç›´æ¥ä½¿ç”¨ `InvalidAPIKeyError`
- âŒ **ä¸æ¨è**ï¼š`import tavily` + ä½¿ç”¨ `tavily.InvalidAPIKeyError`ï¼ˆä¼šå¯¼è‡´ä»£ç å†—é•¿ï¼‰
- âŒ **é”™è¯¯å†™æ³•**ï¼š`from tavily import InvalidAPIKeyError` + ä½¿ç”¨ `tavily.InvalidAPIKeyError`ï¼ˆä¼šå¯¼è‡´ `NameError`ï¼‰

**âš ï¸ ç‰¹ä¾‹è§„åˆ™ï¼ˆä»…ç”¨äºå‘½åå†²çªåœºæ™¯ï¼‰ï¼š**
- å½“å¼‚å¸¸åä¸ Python æ ‡å‡†åº“é‡åï¼ˆå¦‚ `TimeoutError`ï¼‰æ—¶ï¼Œä½¿ç”¨å‘½åç©ºé—´å†™æ³•é¿å…æ­§ä¹‰ï¼š
  ```python
  from tavily import errors as tavily_errors  # å¯¼å…¥å‘½åç©ºé—´
  # ä½¿ç”¨æ—¶ï¼štavily_errors.TimeoutErrorï¼ˆåŒºåˆ«äºæ ‡å‡†åº“ TimeoutErrorï¼‰
  ```
- **é€‚ç”¨åœºæ™¯**ï¼šä»… `TimeoutError` ä¸€ä¸ªå¼‚å¸¸ï¼ˆå…¶ä»– Tavily å¼‚å¸¸æ— é‡åé—®é¢˜ï¼‰
- **æ³¨æ„**ï¼šæœ¬é¡¹ç›®å·²ç¡®è®¤ `TimeoutError` ä¸å¯ç”¨ï¼Œæ­¤è§„åˆ™ä»…ä¾›å‚è€ƒ

**ğŸ“– å®˜æ–¹ä¾æ®ï¼š**
- [tavily/__init__.py @ 38627af](https://github.com/tavily-ai/tavily-python/blob/38627afb7b88d8a57bad29380896210a9ae7badd/tavily/__init__.py)

**ç‰¹åˆ«è¯´æ˜ï¼ˆä¸å­˜åœ¨æˆ–ä¸å¯ç”¨çš„å¼‚å¸¸ï¼‰ï¼š**
- âš ï¸ `TimeoutError`ï¼ˆTavily SDKï¼‰ï¼šè™½å­˜åœ¨ä½†ä¸æ ‡å‡†åº“é‡åï¼Œ**ä¸å¯ç”¨**ï¼ˆä½¿ç”¨ `asyncio.TimeoutError` å³å¯ï¼‰
  - ğŸ“„ æºç ç¡®è®¤ï¼š[tavily/errors.py @ 38627af, L21-L23](https://github.com/tavily-ai/tavily-python/blob/38627afb7b88d8a57bad29380896210a9ae7badd/tavily/errors.py#L21-L23)
  - ğŸ’¡ **ç‰¹ä¾‹è¯´æ˜**ï¼šå¦‚éœ€å¯ç”¨æ­¤å¼‚å¸¸ï¼Œåº”ä½¿ç”¨å‘½åç©ºé—´å¯¼å…¥ï¼ˆ`from tavily import errors as tavily_errors` + `tavily_errors.TimeoutError`ï¼‰ï¼Œè¿™æ˜¯å”¯ä¸€éœ€è¦å‘½åç©ºé—´çš„å¼‚å¸¸ï¼Œå…¶ä»– Tavily å¼‚å¸¸ä»éµå¾ªé€šç”¨è§„èŒƒï¼ˆç›´æ¥å¯¼å…¥ç±»åï¼‰
- âŒ `NetworkError`ï¼ˆTavily SDKï¼‰ï¼š**ä¸å­˜åœ¨**äºå®˜æ–¹æºç ä¸­ï¼ˆå·²é€šè¿‡å®Œæ•´æºç ç¡®è®¤ï¼‰ï¼Œä¸è¦æ·»åŠ ï¼ˆä½¿ç”¨ httpx å¼‚å¸¸å³å¯ï¼‰
  - ğŸ“„ æºç ç¡®è®¤ï¼š[tavily/errors.py @ 38627af](https://github.com/tavily-ai/tavily-python/blob/38627afb7b88d8a57bad29380896210a9ae7badd/tavily/errors.py)ï¼ˆå®Œæ•´æ–‡ä»¶æ— æ­¤ç±»ï¼‰

---

### 8.1.1 LLM ä¸ç»“æœç»„è£…å¼‚å¸¸

| å¼‚å¸¸ç±»å‹ | è§¦å‘åœºæ™¯ | å¤„ç†ç­–ç•¥ | å¤‡æ³¨ |
|---------|---------|---------|------|
| `pydantic.ValidationError` | LLM è¿”å›çš„ JSON æ— æ•ˆæˆ–ä¸ç¬¦åˆæ¨¡å‹ | åœ¨ `call_agent_with_model()` å†…æ•è·ï¼ŒæŠ›å‡º `AgentCallError` | Pydantic v2 ç»Ÿä¸€å¼‚å¸¸ï¼ˆå« JSON æ ¼å¼é”™è¯¯ï¼‰ |
| `json.JSONDecodeError` | æŠ¥å‘Šç”Ÿæˆæ—¶ JSON æ ¼å¼é”™è¯¯ | æŠ›å‡º `ReportGenerationError`ï¼Œè§¦å‘é™çº§æŠ¥å‘Š | `generate_report()` ç‰¹ä¾‹ï¼Œä½¿ç”¨ `json.loads()` |
| `AgentCallError` | LLM è°ƒç”¨å¤±è´¥ï¼ˆæ±‡æ€»å¼‚å¸¸ï¼‰ | è·³è¿‡è¯¥æ­¥éª¤ï¼Œä½¿ç”¨ç©ºç»“æœ | ç”± `call_agent_with_model()` æŠ›å‡º |
| `ReportGenerationError` | æŠ¥å‘Šç”Ÿæˆå¤±è´¥ | ä½¿ç”¨é™çº§æŠ¥å‘Šï¼ˆ`_create_fallback_report`ï¼‰ | ä¿ç•™å·²æ”¶é›†çš„ learnings |
| `WebResearchError` | æœ€ç»ˆæ ¡éªŒå¤±è´¥ | æŠ›å‡ºå¼‚å¸¸ï¼Œç»ˆæ­¢æµç¨‹ | æ— æ³•é™çº§ï¼ˆå¦‚æ‰€æœ‰ä¸»é¢˜å¤±è´¥ï¼‰ |

**è¯´æ˜ï¼š**
1. **`json.JSONDecodeError`** ä»…å‡ºç°åœ¨ `generate_report()` ä¸­ï¼Œå› ä¸ºè¯¥å‡½æ•°ä½¿ç”¨æ ‡å‡†åº“ `json.loads()` è€Œé Pydanticã€‚å…¶ä»–æ‰€æœ‰ LLM è°ƒç”¨å‡ä½¿ç”¨ `call_agent_with_model()` + `model_validate_json()`ï¼Œåªä¼šæŠ›å‡º `ValidationError`ï¼ˆPydantic v2ï¼‰ã€‚
2. **`RETRYABLE_EXCEPTIONS`** å…ƒç»„å®šä¹‰åœ¨ `tavily_client.py` ä¸­ï¼Œå¯æ ¹æ®é¡¹ç›®å®é™…ä½¿ç”¨çš„ HTTP åº“ï¼ˆhttpxã€aiohttp ç­‰ï¼‰çµæ´»è°ƒæ•´ã€‚
3. **å¼‚å¸¸å±‚æ¬¡**ï¼šåº•å±‚å¼‚å¸¸ï¼ˆå¦‚ `ValidationError`ï¼‰è¢«å°è£…ä¸ºä¸šåŠ¡å¼‚å¸¸ï¼ˆå¦‚ `AgentCallError`ï¼‰ï¼Œä¾¿äºä¸Šå±‚ç»Ÿä¸€å¤„ç†ã€‚

### 8.2 æ ¸å¿ƒåŸåˆ™

1. **ä¸ä¸­æ–­æ•´ä½“æµç¨‹**ï¼šå•ä¸ªæŸ¥è¯¢æˆ–åˆ†æ”¯çš„å¤±è´¥ä¸åº”å¯¼è‡´æ•´ä¸ªæ¨¡å—å¤±è´¥
2. **å°½é‡è¿”å›éƒ¨åˆ†ç»“æœ**ï¼šå³ä½¿åªæœ‰ 2/5 ä¸ªä¸»é¢˜æˆåŠŸï¼Œä¹Ÿåº”ç”ŸæˆæŠ¥å‘Š
3. **æŠ¥å‘Šç”Ÿæˆé™çº§**ï¼šå¦‚æœ LLM ç”ŸæˆæŠ¥å‘Šå¤±è´¥ï¼Œä½¿ç”¨ `_create_fallback_report()` è¿”å›åŸºç¡€ç»“æ„
4. **è‡ªåŠ¨é‡è¯•**ï¼šTavily æœç´¢å¤±è´¥æ—¶è‡ªåŠ¨é‡è¯• 1 æ¬¡ï¼ˆå«è¶…æ—¶ï¼‰ï¼Œ2 ç§’é—´éš”
5. **ç»Ÿä¸€å¼‚å¸¸å¤„ç†**ï¼šPydantic v2 ç»Ÿä¸€ä½¿ç”¨ `ValidationError`ï¼ˆåŒ…å« JSON æ ¼å¼é”™è¯¯ï¼‰ï¼Œé€šè¿‡ `error_type` åŒºåˆ†å…·ä½“åŸå› 
6. **å–æ¶ˆå¼‚å¸¸ä¼ æ’­**ï¼š`asyncio.CancelledError` å¿…é¡»å‘ä¸Šä¼ æ’­ï¼Œä¸åº”è¢«å½“ä½œæ™®é€šå¤±è´¥åæ‰ï¼ˆå…³é”®è¯­ä¹‰ï¼šç”¨æˆ·å–æ¶ˆ vs ä»»åŠ¡å¤±è´¥ï¼‰
7. **è¯¦ç»†æ—¥å¿—**ï¼šæ‰€æœ‰å¼‚å¸¸è®°å½•åˆ° `logs/stock_analyzer.log`ï¼ŒåŒ…å«é‡è¯•ä¿¡æ¯å’Œ `error_type`
8. **æˆåŠŸç‡è®°å½•**ï¼šåœ¨ `meta.search_config.successful_topics` ä¸­è®°å½•æˆåŠŸä¸»é¢˜æ•°ï¼ˆç”¨äºè´¨é‡è¯„ä¼°ï¼‰

### 8.2.1 Pydantic v2 å¼‚å¸¸å¤„ç†è¯´æ˜

**é‡è¦ï¼š** Pydantic v2 çš„ `model_validate_json()` å¯¹æ‰€æœ‰éªŒè¯å¤±è´¥**ç»Ÿä¸€æŠ›å‡º `ValidationError`**ï¼ŒåŒ…æ‹¬ï¼š

| å¤±è´¥åœºæ™¯ | ç¤ºä¾‹è¾“å…¥ | ValidationError.errors()[0]['type'] |
|---------|---------|-------------------------------------|
| JSON æ ¼å¼é”™è¯¯ | `{invalid json` | `'json_invalid'` |
| å­—æ®µç¼ºå¤± | `{}` ç¼ºå°‘å¿…å¡«å­—æ®µ | `'missing'` |
| ç±»å‹ä¸åŒ¹é… | `{"score": "abc"}` åº”ä¸º float | `'float_parsing'` |
| çº¦æŸè¿å | `{"score": 15}` ä½†é™åˆ¶ `le=10` | `'less_than_equal'` |

**ä¸ Pydantic v1 çš„åŒºåˆ«ï¼š**
- âŒ **Pydantic v1**ï¼šJSON æ ¼å¼é”™è¯¯æ—¶æŠ›å‡º `json.JSONDecodeError`
- âœ… **Pydantic v2**ï¼šæ‰€æœ‰é”™è¯¯ç»Ÿä¸€ä¸º `ValidationError`ï¼Œé€šè¿‡ `error_type` åŒºåˆ†

**ç‰¹æ®Šæƒ…å†µï¼š`generate_report()` å‡½æ•°**

è¯¥å‡½æ•°ä½¿ç”¨æ ‡å‡†åº“ `json.loads()` è€Œé Pydantic çš„ `model_validate_json()`ï¼Œå› ä¸ºå…¶è¿”å›çš„ JSON ä¸å« `meta` å­—æ®µï¼Œéœ€è¦ç”±è°ƒç”¨æ–¹å¡«å……åå†ç»„è£…ä¸ºå®Œæ•´çš„ `WebResearchResult`ã€‚å› æ­¤è¿™é‡Œ**ä»ç„¶éœ€è¦æ•è· `JSONDecodeError`**ï¼š

```python
# generate_report() â€” ç‰¹æ®Šæƒ…å†µï¼Œä½¿ç”¨ json.loads
try:
    json_str = extract_json_str(raw_text)
    report_dict = json.loads(json_str)  # â† ä¸æ˜¯ Pydanticï¼Œä»éœ€æ•è· JSONDecodeError
    return report_dict
except json.JSONDecodeError as e:
    raise ReportGenerationError(...) from e
except Exception as e:
    raise ReportGenerationError(...) from e
```

**å…¶ä»– Agent è°ƒç”¨ï¼ˆcall_agent_with_modelï¼‰ï¼š**

```python
from pydantic import ValidationError

try:
    result = model_cls.model_validate_json(json_str)  # â† Pydantic v2
except ValidationError as e:
    # ç»Ÿä¸€æ•è·ï¼Œé€šè¿‡ error_type åŒºåˆ†å…·ä½“é”™è¯¯
    logger.error(
        f"Validation failed: {e.error_count()} errors, "
        f"first error type: {e.errors()[0]['type']}"
    )
```

**æ—¥å¿—ç¤ºä¾‹ï¼š**

```
# JSON æ ¼å¼é”™è¯¯
ERROR - Agent 'query_generator' validation failed: 1 errors
First error: {'type': 'json_invalid', 'loc': (), 'msg': 'Invalid JSON'}

# å­—æ®µç±»å‹é”™è¯¯
ERROR - Agent 'knowledge_extractor' validation failed: 2 errors
First error: {'type': 'float_parsing', 'loc': ('score',), 'msg': 'Input should be a valid number'}

# å­—æ®µç¼ºå¤±
ERROR - Agent 'report_generator' validation failed: 5 errors
First error: {'type': 'missing', 'loc': ('news_summary',), 'msg': 'Field required'}
```

**ä¼˜åŠ¿ï¼š**
- âœ… é”™è¯¯ä¿¡æ¯æ›´ç»“æ„åŒ–ï¼ˆ`type`ã€`loc`ã€`msg`ï¼‰
- âœ… ä¾¿äºç»Ÿè®¡å’Œç›‘æ§ï¼ˆæŒ‰ `error_type` åˆ†ç±»ï¼‰
- âœ… ä¾¿äºé’ˆå¯¹æ€§ä¼˜åŒ–æç¤ºè¯

**æ³¨æ„ï¼š** `generate_report()` ä½¿ç”¨æ ‡å‡†åº“ `json.loads()` è€Œé Pydanticï¼Œå› æ­¤ä»éœ€æ•è· `json.JSONDecodeError`ã€‚

### 8.3 é™çº§ç­–ç•¥è¯¦è§£

#### 8.3.1 ä¸»é¢˜çº§åˆ«é™çº§ï¼ˆå·²åœ¨ä¸»æµç¨‹å®ç°ï¼‰

**ä½ç½®ï¼š** `run_web_research()` Step 4

```python
# ç»Ÿè®¡æˆåŠŸä¸»é¢˜æ•°ï¼ˆå¿…é¡»æ˜¯ ResearchResult ä¸”æœ‰æœ‰æ•ˆ learningsï¼‰
successful_topics = sum(
    1 for r in results 
    if isinstance(r, ResearchResult) and len(r.learnings) > 0
)

# å…¨éƒ¨ä¸»é¢˜å¤±è´¥ä¿æŠ¤ â† å…³é”®ä¿æŠ¤é€»è¾‘
if successful_topics == 0:
    raise WebResearchError(
        f"All {len(topics)} topics failed, cannot generate report for {symbol}"
    )

# çŸ¥è¯†ç‚¹æ•°é‡è¿‡å°‘è­¦å‘Š
if len(unique_learnings) < 5:
    logger.warning(
        f"Only {len(unique_learnings)} learnings collected "
        f"({successful_topics}/{len(topics)} topics succeeded), "
        f"report quality may be low"
    )
    # ä»ç„¶å°è¯•ç”ŸæˆæŠ¥å‘Šï¼Œåç»­ä¼šåœ¨ Step 6 å¼ºåˆ¶æ ‡è®° search_confidence = "ä½"
```

**æˆåŠŸåˆ¤å®šæ ‡å‡†ï¼š**
- ä¸»é¢˜å¿…é¡»åŒæ—¶æ»¡è¶³ï¼š
  1. è¿”å› `ResearchResult` å¯¹è±¡ï¼ˆç±»å‹æ­£ç¡®ï¼‰
  2. `len(r.learnings) > 0`ï¼ˆæœ‰æœ‰æ•ˆçŸ¥è¯†ç‚¹ï¼‰
- **å…³é”®ä¿®å¤**ï¼šé¿å…æŠŠ"è¿”å›ç©ºç»“æœçš„å¤±è´¥ä¸»é¢˜"è¯¯åˆ¤ä¸ºæˆåŠŸ

**é€»è¾‘è¯´æ˜ï¼š**
- âœ… **å…¨éƒ¨å¤±è´¥ï¼ˆ0/5ï¼‰**ï¼šæ‰€æœ‰ä¸»é¢˜è¿”å›ç©º learningsï¼ŒæŠ›å‡º `WebResearchError`ï¼Œç»ˆæ­¢æµç¨‹
- âš ï¸ **éƒ¨åˆ†å¤±è´¥ï¼ˆ1-4/5ï¼‰**ï¼šç»§ç»­ï¼Œä½†è®°å½•æ—¥å¿—å’ŒæˆåŠŸç‡
- âš ï¸ **çŸ¥è¯†ç‚¹è¿‡å°‘ï¼ˆ< 5 æ¡ï¼‰**ï¼šè®°å½•è­¦å‘Šï¼Œç»§ç»­ç”ŸæˆæŠ¥å‘Šï¼Œåœ¨ Step 6 å¼ºåˆ¶è¦†ç›–ç½®ä¿¡åº¦ä¸º "ä½"

**è®°å½•åˆ° metaï¼š** `search_config.successful_topics` å­—æ®µ

#### 8.3.2 æŠ¥å‘Šç”Ÿæˆçº§åˆ«é™çº§ï¼ˆå·²åœ¨ä¸»æµç¨‹å®ç°ï¼‰

**ä½ç½®ï¼š** `run_web_research()` Step 5 + Step 6

```python
# Step 5: generate_report å¤±è´¥æ—¶çš„é™çº§å¤„ç†
is_fallback = False
try:
    report_dict = await generate_report(
        report_agent=report_agent,
        symbol=symbol,
        name=name,
        industry=industry,
        learnings=unique_learnings,
    )
except ReportGenerationError as e:
    logger.error(f"Report generation failed, using fallback: {e}")
    is_fallback = True
    # ä½¿ç”¨é™çº§æŠ¥å‘Šï¼ˆè¿”å›æœ€å°åŒ–ç»“æ„ï¼‰
    report_dict = _create_fallback_report(
        learnings=unique_learnings,
        error_message=str(e.cause)
    )

# Step 6: ç»„è£… meta æ—¶ï¼Œé™çº§æŠ¥å‘Šéœ€è¦å¡«å…… raw_learnings
meta = SearchMeta(
    # ...
    raw_learnings=unique_learnings if is_fallback else None,  # ä»…é™çº§æ—¶ä¿å­˜
)
```

**é™çº§æ•°æ®æµå‘ï¼š**
```
é™çº§è§¦å‘ â†’ _create_fallback_report() è¿”å›æœ€å°åŒ–ç»“æ„
          â†“
          è®¾ç½® is_fallback = True
          â†“
          meta.raw_learnings = unique_learningsï¼ˆ60æ¡åŸå§‹çŸ¥è¯†ç‚¹ï¼‰
          â†“
          æœ€ç»ˆ JSON ä¸­ meta.raw_learnings åŒ…å«å®Œæ•´æ•°æ®
```

**é™çº§æŠ¥å‘Šç‰¹ç‚¹ï¼š**
- ä¿ç•™æ‰€æœ‰æ”¶é›†åˆ°çš„ learnings åœ¨ `meta.raw_learnings` å­—æ®µï¼ˆä¾¿äºåç»­äººå·¥åˆ†æï¼‰
- è¿”å›æœ€å°åŒ–çš„ç»“æ„åŒ–æ•°æ®ï¼ˆ**ç©ºåˆ—è¡¨/é»˜è®¤å€¼ï¼Œå¦‚ `news_summary.positive = []`**ï¼‰
- `search_confidence` æ ‡è®°ä¸º "ä½"
- åœ¨ `competitive_advantage.description` ä¸­è¯´æ˜é™çº§åŸå› ï¼ˆå·²æˆªæ–­åˆ° 500 å­—ç¬¦ï¼‰
- **å…³é”®è¯†åˆ«æ ‡å¿—**ï¼š`meta.raw_learnings is not None`

#### 8.3.3 ç½®ä¿¡åº¦å¼ºåˆ¶é™çº§ï¼ˆå·²åœ¨ä¸»æµç¨‹å®ç°ï¼‰

**ä½ç½®ï¼š** `run_web_research()` Step 6ï¼ˆmeta ç»„è£…åï¼‰

```python
# åœ¨ meta ç»„è£…å®Œæˆã€æœ€ç»ˆæ ¡éªŒä¹‹å‰
report_dict["meta"] = meta.model_dump()

# å¼ºåˆ¶é™çº§é€»è¾‘ï¼šlearnings è¿‡å°‘æ—¶ï¼Œå¼ºåˆ¶æ ‡è®°ä¸ºä½ç½®ä¿¡åº¦
if len(unique_learnings) < 5:
    logger.warning(f"Forcing search_confidence to 'ä½' due to insufficient learnings: {len(unique_learnings)}")
    report_dict["search_confidence"] = "ä½"

# æœ€ç»ˆ Pydantic æ ¡éªŒ
final_result = WebResearchResult.model_validate(report_dict)
```

**è§¦å‘æ¡ä»¶ï¼š**
- `len(unique_learnings) < 5`ï¼ˆæ— è®º Agent è¿”å›çš„ç½®ä¿¡åº¦æ˜¯ä»€ä¹ˆï¼‰

**ä½œç”¨ï¼š**
- å¼ºåˆ¶è¦†ç›– `search_confidence` ä¸º "ä½"ï¼Œç¡®ä¿æ•°æ®è´¨é‡æ ‡è¯†å‡†ç¡®
- é¿å… LLM åŸºäºå°‘é‡æ•°æ®è‡ªä¿¡åœ°ç»™å‡º"é«˜"æˆ–"ä¸­"ç½®ä¿¡åº¦

#### 8.3.4 æœ€ç»ˆæ ¡éªŒå¤±è´¥ï¼ˆå·²åœ¨ä¸»æµç¨‹å®ç°ï¼‰

**ä½ç½®ï¼š** `run_web_research()` Step 6

```python
# Pydantic æ ¡éªŒå¤±è´¥æ—¶æ— æ³•é™çº§ï¼Œç›´æ¥æŠ›å‡ºå¼‚å¸¸
try:
    final_result = WebResearchResult.model_validate(report_dict)
except Exception as e:
    logger.error(f"Final result validation failed: {e}")
    raise WebResearchError(
        f"Failed to validate final report for {symbol}: {e}"
    ) from e
```

**åŸå› ï¼š** å¦‚æœè¿é™çº§æŠ¥å‘Šéƒ½æ— æ³•é€šè¿‡ Pydantic æ ¡éªŒï¼Œè¯´æ˜æ•°æ®ç»“æ„ä¸¥é‡é”™è¯¯ï¼Œæ— æ³•ç»§ç»­ã€‚

---

### 8.4 è¶…æ—¶ä¸é‡è¯•ç­–ç•¥

#### 8.4.1 Tavily æœç´¢è¶…æ—¶æ§åˆ¶

**å®ç°ä½ç½®ï¼š** `tavily_search()` å‡½æ•°

```python
# ä½¿ç”¨ asyncio.wait_for å®ç°è¶…æ—¶æ§åˆ¶
response = await asyncio.wait_for(
    client.search(...),
    timeout=TAVILY_TIMEOUT,  # 30ç§’
)
```

**è¶…æ—¶é…ç½®ï¼š** `TAVILY_TIMEOUT = 30.0` ç§’ï¼ˆconfig.pyï¼‰

#### 8.4.2 è‡ªåŠ¨é‡è¯•æœºåˆ¶

**ç­–ç•¥ï¼š** å¤±è´¥åè‡ªåŠ¨é‡è¯• 1 æ¬¡ï¼ˆå…± 2 æ¬¡å°è¯•ï¼‰

**å¯é‡è¯•å¼‚å¸¸ï¼ˆRetryableï¼‰- ç½‘ç»œå±‚ç¬æ—¶æ•…éšœï¼š**

**è®¾è®¡ç›®æ ‡ï¼š** æ‰€æœ‰ç½‘ç»œå±‚ç¬æ—¶æ•…éšœéƒ½åº”é‡è¯•ï¼Œç¡®ä¿ç³»ç»Ÿå¯¹ä¸´æ—¶ç½‘ç»œé—®é¢˜çš„é²æ£’æ€§ã€‚

**å®æ–½è¦æ±‚ï¼š** 
1. âœ… æ ¹æ®é¡¹ç›®å®é™…ä¾èµ–æ˜ç¡®å¯ç”¨ç›¸åº”å¼‚å¸¸ï¼ˆhttpx å¼‚å¸¸å·²å…¨éƒ¨å¯ç”¨ï¼‰
2. âœ… Tavily SDK å¼‚å¸¸å·²ç¡®è®¤å¹¶å¯ç”¨**å»ºè®®å¯ç”¨çš„å­é›†**ï¼ˆåŸºäºå®˜æ–¹æºç  v0.7.21ï¼‰
   - å·²å¯ç”¨ï¼š5ä¸ªå¼‚å¸¸ï¼ˆInvalidAPIKeyError, MissingAPIKeyError, BadRequestError, ForbiddenError, UsageLimitExceededErrorï¼‰
   - å·²ç¡®è®¤ä½†ä¸å¯ç”¨ï¼šTimeoutErrorï¼ˆä¸æ ‡å‡†åº“é‡åï¼‰
3. âš ï¸ æ–°å¢ä¾èµ–æ—¶åŠæ—¶æ›´æ–°æ­¤åˆ—è¡¨

| å¼‚å¸¸ç±»å‹ | æ¥æº | è¯´æ˜ | å®æ–½çŠ¶æ€ |
|---------|------|------|---------|
| `asyncio.TimeoutError` | `asyncio.wait_for()` | asyncio è¶…æ—¶ | âœ… **å•ç‹¬å¤„ç†**ï¼ˆä¸åœ¨ RETRYABLE_EXCEPTIONS ä¸­ï¼‰ |
| `ConnectionError` | æ ‡å‡†åº“ | ç½‘ç»œè¿æ¥å¤±è´¥ | âœ… å·²å¯ç”¨ |
| `OSError` | æ ‡å‡†åº“ | ç½‘ç»œ I/O é”™è¯¯ | âœ… å·²å¯ç”¨ |
| `httpx.ConnectError` | httpx åº“ | HTTP è¿æ¥é”™è¯¯ | âœ… **å·²å¯ç”¨**ï¼ˆé¡¹ç›®ä¾èµ– httpx>=0.28.1ï¼‰ |
| `httpx.NetworkError` | httpx åº“ | HTTP ç½‘ç»œé”™è¯¯ | âœ… **å·²å¯ç”¨**ï¼ˆé¡¹ç›®ä¾èµ– httpx>=0.28.1ï¼‰ |
| `httpx.TimeoutException` | httpx åº“ | HTTP è¶…æ—¶ | âœ… **å·²å¯ç”¨**ï¼ˆé¡¹ç›®ä¾èµ– httpx>=0.28.1ï¼‰ |

**ä¸å¯é‡è¯•å¼‚å¸¸ï¼ˆNon-Retryableï¼‰- API ä¸šåŠ¡é”™è¯¯æˆ–ç¼–ç¨‹é”™è¯¯ï¼š**

| å¼‚å¸¸ç±»å‹ | æ¥æº | åŸå›  | çŠ¶æ€ |
|---------|------|------|------|
| `httpx.HTTPStatusError` | httpx åº“ | HTTP 4xx/5xx çŠ¶æ€ç é”™è¯¯ | âœ… å·²å¯ç”¨ |
| `InvalidAPIKeyError` | Tavily SDK | API å¯†é’¥æ— æ•ˆï¼ˆé…ç½®é—®é¢˜ï¼‰ | âœ… å·²ç¡®è®¤å¯ç”¨ï¼ˆv0.7.21 @ 38627afï¼‰ |
| `MissingAPIKeyError` | Tavily SDK | ç¼ºå°‘ API å¯†é’¥ï¼ˆé…ç½®é—®é¢˜ï¼‰ | âœ… å·²ç¡®è®¤å¯ç”¨ï¼ˆv0.7.21 @ 38627afï¼‰ |
| `BadRequestError` | Tavily SDK | è¯·æ±‚æ ¼å¼é”™è¯¯ï¼ˆä»£ç é—®é¢˜ï¼‰ | âœ… å·²ç¡®è®¤å¯ç”¨ï¼ˆv0.7.21 @ 38627afï¼‰ |
| `ForbiddenError` | Tavily SDK | æƒé™ä¸è¶³ | âœ… å·²ç¡®è®¤å¯ç”¨ï¼ˆv0.7.21 @ 38627afï¼‰ |
| `UsageLimitExceededError` | Tavily SDK | ä½¿ç”¨é™åˆ¶è¶…å‡ºï¼ˆé…é¢ä¸è¶³ï¼‰ | âœ… å·²ç¡®è®¤å¯ç”¨ï¼ˆv0.7.21 @ 38627afï¼‰ |
| `KeyError`, `AttributeError` | ä»£ç ç¼ºé™· | ç¼–ç¨‹é”™è¯¯ï¼ˆå¿…é¡»ä¿®å¤ï¼‰ | N/Aï¼ˆç¼–ç¨‹é”™è¯¯ï¼‰ |
| å…¶ä»– `Exception` | æœªé¢„æœŸ | éœ€è¦æ’æŸ¥æ ¹å›  | N/Aï¼ˆé€šç”¨æ•è·ï¼‰ |

**æ³¨ï¼š** 
1. **httpx å¼‚å¸¸**ï¼šé¡¹ç›®å·²ä¾èµ– `httpx>=0.28.1`ï¼ˆè§ `pyproject.toml`ï¼‰ï¼Œä»£ç ä¸­**å¿…é¡»** `import httpx` å¹¶å¯ç”¨ httpx å¼‚å¸¸ã€‚âœ… **å·²å®Œæˆ**
2. **Tavily SDK å¼‚å¸¸**ï¼šâœ… **å·²å¯ç”¨å»ºè®®å¯ç”¨çš„å­é›†**ï¼ˆåŸºäºå®˜æ–¹æºç  v0.7.21ï¼Œcommit `38627af`ï¼‰
   - **å·²å¯ç”¨ï¼ˆ5ä¸ªï¼‰**ï¼šInvalidAPIKeyError, BadRequestError, ForbiddenError, MissingAPIKeyError, UsageLimitExceededError
   - **å·²ç¡®è®¤ä½†ä¸å¯ç”¨ï¼ˆ1ä¸ªï¼‰**ï¼šTimeoutErrorï¼ˆä¸æ ‡å‡†åº“é‡åï¼Œä½¿ç”¨ `asyncio.TimeoutError` æ›¿ä»£ï¼‰
   - **æ ‡å‡†å¯¼å…¥æ–¹å¼**ï¼š
     ```python
     from tavily import InvalidAPIKeyError, BadRequestError, ForbiddenError, MissingAPIKeyError, UsageLimitExceededError
     ```
   - ğŸ“– **å¯è¿½æº¯æ€§é“¾æ¥**ï¼š[tavily/errors.py @ 38627af](https://github.com/tavily-ai/tavily-python/blob/38627afb7b88d8a57bad29380896210a9ae7badd/tavily/errors.py)

**é‡è¯•é—´éš”ï¼š** 2 ç§’ï¼ˆ`await asyncio.sleep(2)`ï¼‰

**è®¾è®¡åŸåˆ™ï¼š**
1. **ç½‘ç»œå±‚ç¬æ—¶æ•…éšœ**ï¼ˆè¿æ¥å¤±è´¥ã€è¶…æ—¶ï¼‰â†’ **é‡è¯•**ï¼ˆå¯èƒ½æ¢å¤ï¼‰
2. **API ä¸šåŠ¡é”™è¯¯**ï¼ˆå¯†é’¥é”™è¯¯ã€è¯·æ±‚æ ¼å¼é”™è¯¯ï¼‰â†’ **ä¸é‡è¯•**ï¼ˆéœ€è¦ä¿®æ”¹é…ç½®æˆ–ä»£ç ï¼‰
3. **ç¼–ç¨‹é”™è¯¯**ï¼ˆKeyError ç­‰ï¼‰â†’ **ä¸é‡è¯•**ï¼Œç«‹å³æŠ›å‡ºï¼ˆé¿å…æ©ç›–ç¼ºé™·ï¼‰

**ä»£ç å®ç°ï¼ˆæ¨èï¼‰ï¼š**

```python
# åœ¨æ–‡ä»¶å¼€å¤´å®šä¹‰å¯é‡è¯•å¼‚å¸¸å…ƒç»„ï¼ˆå¯æ ¹æ®å®é™…ä½¿ç”¨çš„åº“è°ƒæ•´ï¼‰
# 
# æ³¨æ„ï¼šasyncio.TimeoutError ä¸åœ¨æ­¤å…ƒç»„ä¸­ï¼Œå®ƒæœ‰ä¸“é—¨çš„ except åˆ†æ”¯ï¼Œ
# ä»¥ä¾¿æä¾›æ›´æ˜ç¡®çš„è¶…æ—¶æ—¥å¿—å’Œç‹¬ç«‹çš„é‡è¯•æ§åˆ¶
RETRYABLE_EXCEPTIONS = (
    ConnectionError,       # æ ‡å‡†åº“è¿æ¥é”™è¯¯
    OSError,               # æ ‡å‡†åº“ I/O é”™è¯¯ï¼ˆåŒ…å« TimeoutError å­ç±»ï¼‰
    # âœ… httpx å¼‚å¸¸ï¼ˆé¡¹ç›®ä¾èµ– httpx>=0.28.1ï¼Œå¿…é¡»å¯ç”¨ï¼‰
    httpx.ConnectError,
    httpx.NetworkError,
    httpx.TimeoutException,
    httpx.RemoteProtocolError,
)

# é‡è¯•å¾ªç¯
for attempt in range(max_retries + 1):
    try:
        response = await asyncio.wait_for(...)
        return results  # âœ… æˆåŠŸï¼Œç›´æ¥è¿”å›
    
    except asyncio.TimeoutError as e:
        # âœ… å•ç‹¬å¤„ç†è¶…æ—¶ï¼ˆæ¸…æ™°çš„æ—¥å¿—ï¼Œä¸åœ¨ RETRYABLE_EXCEPTIONS ä¸­ï¼‰
        last_error = e
        if attempt < max_retries:
            logger.warning(f"Timeout (attempt {attempt + 1}/{max_retries + 1}), retrying...")
            await asyncio.sleep(2)
        else:
            logger.error(f"Timeout after {max_retries + 1} attempts")
    
    except RETRYABLE_EXCEPTIONS as e:
        # ç½‘ç»œå±‚é”™è¯¯ï¼Œå¯é‡è¯•
        last_error = e
        if attempt < max_retries:
            logger.warning(f"Retryable error: {type(e).__name__}, retrying...")
            await asyncio.sleep(2)
        else:
            logger.error(f"Failed after {max_retries + 1} attempts: {e}")
    
    except NON_RETRYABLE_EXCEPTIONS as e:
        # âŒ å·²çŸ¥çš„ä¸å¯é‡è¯•å¼‚å¸¸ï¼ˆAPI é”™è¯¯ã€é…ç½®é”™è¯¯ï¼‰
        logger.error(f"Non-retryable API/config error: {type(e).__name__}: {e}")
        raise TavilySearchError(query=query, attempts=attempt + 1, cause=e) from e
    
    except Exception as e:
        # âŒ å…¶ä»–æœªé¢„æœŸå¼‚å¸¸ï¼ˆç¼–ç¨‹é”™è¯¯ç­‰ï¼‰
        logger.error(f"Unexpected error: {type(e).__name__}: {e}")
        raise TavilySearchError(query=query, attempts=attempt + 1, cause=e) from e

# å¾ªç¯ç»“æŸï¼Œæ‰€æœ‰é‡è¯•éƒ½å¤±è´¥
raise TavilySearchError(query=query, attempts=max_retries + 1, cause=last_error)
```

**å®æ–½å»ºè®®ï¼ˆå››å±‚å¼‚å¸¸å¤„ç†ç»“æ„ï¼‰ï¼š**
1. **ç¬¬ä¸€å±‚**ï¼š`except asyncio.TimeoutError` - è¶…æ—¶å¼‚å¸¸å•ç‹¬å¤„ç†ï¼Œæä¾›æ˜ç¡®çš„è¶…æ—¶æ—¥å¿—
2. **ç¬¬äºŒå±‚**ï¼š`except RETRYABLE_EXCEPTIONS` - ç½‘ç»œå±‚ç¬æ—¶æ•…éšœï¼Œå¯é‡è¯•
3. **ç¬¬ä¸‰å±‚**ï¼š`except NON_RETRYABLE_EXCEPTIONS` - API/é…ç½®é”™è¯¯ï¼Œä¸é‡è¯•ï¼Œæ˜ç¡®åˆ†ç±»
4. **ç¬¬å››å±‚**ï¼š`except Exception` - å…¶ä»–æœªé¢„æœŸå¼‚å¸¸ï¼ˆç¼–ç¨‹é”™è¯¯ï¼‰ï¼Œä¸é‡è¯•
5. âœ… æ ¹æ®é¡¹ç›®å®é™…ä½¿ç”¨çš„ HTTP åº“ï¼ˆhttpxï¼‰è°ƒæ•´ `RETRYABLE_EXCEPTIONS`ï¼ˆå·²å®Œæˆï¼‰
6. âœ… Tavily SDK å»ºè®®å¯ç”¨çš„å¼‚å¸¸ç±»ï¼ˆç»å®˜æ–¹æºç ç¡®è®¤çš„5ä¸ªï¼‰å·²æ·»åŠ åˆ° `NON_RETRYABLE_EXCEPTIONS` å…ƒç»„ä¸­
7. âœ… ä»£ç æ³¨é‡Šæ¸…æ™°ï¼Œè¯´æ˜äº†å“ªäº›å¼‚å¸¸å¯é‡è¯•ã€å“ªäº›ä¸å¯é‡è¯•åŠåŸå› 
8. ğŸ”’ **ç¡¬æ€§è§„åˆ™**ï¼šæ‰€æœ‰ç¬¬ä¸‰æ–¹ SDK å¼‚å¸¸å¿…é¡»ç»å®˜æ–¹æ–‡æ¡£/æºç ç¡®è®¤åæ‰èƒ½æ·»åŠ ï¼Œä¸”éœ€è¯„ä¼°æ˜¯å¦é€‚åˆå¯ç”¨ï¼ˆå¦‚é¿å…ä¸æ ‡å‡†åº“é‡åï¼‰

#### 8.4.3 å®æ–½å»ºè®®ï¼šæ ¹æ®å®é™…ä¾èµ–è°ƒæ•´å¼‚å¸¸ç±»å‹

**åŸºç¡€å®ç°ï¼ˆä»…æ ‡å‡†åº“å¼‚å¸¸ï¼‰ï¼š**
```python
# æ³¨æ„ï¼šasyncio.TimeoutError ä¸åœ¨æ­¤å…ƒç»„ä¸­ï¼ˆæœ‰ä¸“é—¨çš„ except åˆ†æ”¯ï¼‰
RETRYABLE_EXCEPTIONS = (
    ConnectionError,  # æ ‡å‡†åº“è¿æ¥é”™è¯¯
    OSError,          # æ ‡å‡†åº“ I/O é”™è¯¯ï¼ˆåŒ…å« TimeoutError å­ç±»ï¼‰
)
```

**é¡¹ç›®å½“å‰é…ç½®ï¼ˆå·²ä½¿ç”¨ httpx>=0.28.1ï¼Œä¼˜åŒ–åæ— é‡å¤ï¼‰ï¼š**
```python
import httpx  # â† å¿…é¡»å¯¼å…¥
# Tavily å¼‚å¸¸å¯¼å…¥ï¼ˆâœ… å®˜æ–¹æ¨èæ–¹å¼ï¼ŒåŸºäº v0.7.21ï¼‰
from tavily import (
    InvalidAPIKeyError,
    MissingAPIKeyError,
    BadRequestError,
    ForbiddenError,
    UsageLimitExceededError,
)

# æ³¨æ„ï¼šasyncio.TimeoutError ä¸åœ¨æ­¤å…ƒç»„ä¸­ï¼ˆæœ‰å•ç‹¬çš„ except åˆ†æ”¯ï¼‰
RETRYABLE_EXCEPTIONS = (
    ConnectionError,         # æ ‡å‡†åº“è¿æ¥é”™è¯¯
    OSError,                 # æ ‡å‡†åº“ I/O é”™è¯¯
    # âœ… httpx å¼‚å¸¸ï¼ˆå·²å¯ç”¨ï¼Œä¸è¦åˆ é™¤ï¼‰
    httpx.ConnectError,      # æ›´ç²¾ç¡®çš„è¿æ¥é”™è¯¯
    httpx.NetworkError,      # HTTP ç½‘ç»œé”™è¯¯ï¼ˆåŸºç±»ï¼‰
    httpx.TimeoutException,  # HTTP è¶…æ—¶
    httpx.RemoteProtocolError,  # HTTP/2 åè®®é”™è¯¯
)

# ä¸å¯é‡è¯•å¼‚å¸¸ï¼šAPI é”™è¯¯ã€é…ç½®é”™è¯¯ï¼ˆç«‹å³å¤±è´¥ï¼‰
# æ³¨æ„ï¼šæ­¤å…ƒç»„ä¼šåœ¨ä»£ç ä¸­ä½¿ç”¨ï¼Œæä¾›æ˜ç¡®çš„é”™è¯¯åˆ†ç±»
NON_RETRYABLE_EXCEPTIONS = (
    httpx.HTTPStatusError,   # HTTP 4xx/5xx çŠ¶æ€ç é”™è¯¯ï¼ˆå·²å¯ç”¨ï¼‰
    # Tavily SDK ç‰¹å®šå¼‚å¸¸ï¼ˆâœ… å·²ç¡®è®¤å¹¶å¯ç”¨ï¼ŒåŸºäºå®˜æ–¹æºç  v0.7.21ï¼‰
    # ğŸ”’ ç¡¬æ€§è§„åˆ™ï¼šä»…æ·»åŠ ç»å®˜æ–¹æºç ç¡®è®¤å­˜åœ¨çš„å¼‚å¸¸ç±»ï¼Œä¸å­˜åœ¨æ—¶ä¿æŒæ³¨é‡Šæ€æˆ–åˆ é™¤
    # âš ï¸ æ³¨æ„ï¼šä½¿ç”¨ç›´æ¥å¯¼å…¥çš„ç±»åï¼ˆè§ä¸Šæ–¹ importï¼‰ï¼Œä¸ä½¿ç”¨ tavily.å¼‚å¸¸å
    InvalidAPIKeyError,
    MissingAPIKeyError,
    BadRequestError,
    ForbiddenError,
    UsageLimitExceededError,
)
```

**å…³é”®è®¾è®¡åŸåˆ™ï¼š**
1. âš ï¸ **`asyncio.TimeoutError` ä¸åœ¨ `RETRYABLE_EXCEPTIONS` ä¸­**ï¼šä½¿ç”¨å•ç‹¬çš„ `except asyncio.TimeoutError` åˆ†æ”¯å¤„ç†ï¼Œæä¾›æ›´æ¸…æ™°çš„è¶…æ—¶æ—¥å¿—
2. âš ï¸ **ä¸åŒ…å«æ ‡å‡†åº“ `TimeoutError`**ï¼šå› ä¸ºå®ƒæ˜¯ `OSError` çš„å­ç±»ï¼ˆPython 3.3+ï¼‰ï¼Œå·²è¢« `OSError` è¦†ç›–ï¼Œæ— éœ€å•ç‹¬åˆ—å‡º
3. âœ… **åªåŒ…å«çœŸæ­£éœ€è¦é‡è¯•çš„ç½‘ç»œå±‚å¼‚å¸¸**ï¼šé¿å…è¿‡å®½çš„å¼‚å¸¸æ•è·

**å®æ–½æ­¥éª¤ï¼š**
1. âœ… **ç¡®è®¤é¡¹ç›®ä¾èµ–**ï¼šæ£€æŸ¥ `pyproject.toml` å·²å®‰è£… `httpx>=0.28.1`ï¼ˆå·²æ»¡è¶³ï¼‰
2. âœ… **ç¡®è®¤ä»£ç å¯¼å…¥**ï¼šç¡®è®¤ `tavily_client.py` ä¸­å·²å­˜åœ¨å¹¶ä¿ç•™ `import httpx`
3. âœ… **ç¡®è®¤å¼‚å¸¸å…ƒç»„é…ç½®**ï¼šéªŒè¯ `RETRYABLE_EXCEPTIONS` ä¸­ï¼š
   - âœ… åŒ…å«æ‰€æœ‰ httpx å¼‚å¸¸ï¼ˆ`httpx.ConnectError`, `httpx.NetworkError`, `httpx.TimeoutException`, `httpx.RemoteProtocolError`ï¼‰
   - âš ï¸ **ä¸åŒ…å«** `asyncio.TimeoutError`ï¼ˆå®ƒæœ‰å•ç‹¬çš„ `except` åˆ†æ”¯ï¼‰
4. âœ… **ç¡®è®¤å¼‚å¸¸å¤„ç†é¡ºåº**ï¼šä»£ç ä¸­æœ‰å•ç‹¬çš„ `except asyncio.TimeoutError` åˆ†æ”¯ï¼ˆåœ¨ `except RETRYABLE_EXCEPTIONS` ä¹‹å‰ï¼‰
5. âœ… **æµ‹è¯•éªŒè¯**ï¼šç¡®ä¿ç½‘ç»œé”™è¯¯ä¼šé‡è¯•ã€è¶…æ—¶ä¼šé‡è¯•ã€API é”™è¯¯ä¸é‡è¯•

**ğŸ”’ é€šç”¨ç¡¬æ€§è§„åˆ™ï¼ˆé¿å…å®æ–½æ­§ä¹‰ï¼‰ï¼š**
- **æ·»åŠ å¼‚å¸¸çš„å‰æ**ï¼šå¿…é¡»å…ˆæŸ¥çœ‹å®˜æ–¹æ–‡æ¡£æˆ–æºç ï¼Œç¡®è®¤å¼‚å¸¸ç±»å‹ç¡®å®å­˜åœ¨**ä¸”é€‚åˆå¯ç”¨**
- **ç¡®è®¤å­˜åœ¨ä½†ä¸å»ºè®®å¯ç”¨æ—¶**ï¼šä¿æŒæ³¨é‡Šæ€ï¼Œåœ¨æ³¨é‡Šä¸­è¯´æ˜åŸå› ï¼ˆå¦‚ä¸æ ‡å‡†åº“é‡åï¼‰
- **ç¡®è®¤ä¸å­˜åœ¨æ—¶çš„å¤„ç†**ï¼šåˆ é™¤æ³¨é‡Šè¡Œæˆ–æ ‡æ³¨ âŒ ä¸å­˜åœ¨ï¼Œä¾èµ–é€šç”¨ `except Exception` åˆ†æ”¯å…œåº•
- **ä¸¥ç¦çŒœæµ‹æ€§æ·»åŠ **ï¼šä¸è¦åŸºäº"å¯èƒ½æœ‰"ã€"åº”è¯¥æœ‰"æˆ–"å…¶ä»–åº“æœ‰"ç­‰ç†ç”±æ·»åŠ æœªç»ç¡®è®¤çš„å¼‚å¸¸ç±»
- **ç»´æŠ¤åŸåˆ™**ï¼šå®šæœŸå®¡æŸ¥ç¬¬ä¸‰æ–¹ SDK ç‰ˆæœ¬æ›´æ–°ï¼Œç¡®è®¤å¼‚å¸¸ç±»å‹å˜åŒ–åŠå¯ç”¨å»ºè®®

#### 8.4.4 æ—¥å¿—è¾“å‡ºç¤ºä¾‹

**é¦–æ¬¡æˆåŠŸï¼š**
```
INFO - Tavily search 'å¹³å®‰é“¶è¡Œ æœ€æ–°æ¶ˆæ¯...' returned 5 results
```

**é‡è¯•åæˆåŠŸï¼š**
```
WARNING - Tavily search 'å¹³å®‰é“¶è¡Œ æœ€æ–°æ¶ˆæ¯...' timeout (attempt 1/2), retrying...
INFO - Tavily search 'å¹³å®‰é“¶è¡Œ æœ€æ–°æ¶ˆæ¯...' succeeded on retry 1, returned 5 results
```

**å…¨éƒ¨å¤±è´¥ï¼š**
```
WARNING - Tavily search 'å¹³å®‰é“¶è¡Œ æœ€æ–°æ¶ˆæ¯...' failed: ConnectError (attempt 1/2), retrying...
ERROR - Tavily search 'å¹³å®‰é“¶è¡Œ æœ€æ–°æ¶ˆæ¯...' failed after 2 attempts: ConnectError
```

#### 8.4.4 LLM è°ƒç”¨è¶…æ—¶

**é…ç½®ï¼š** `API_TIMEOUT = 120.0` ç§’

LLM è°ƒç”¨çš„è¶…æ—¶ç”± `AsyncOpenAI` å®¢æˆ·ç«¯å†…ç½®å¤„ç†ï¼š

```python
AsyncOpenAI(
    api_key=DASHSCOPE_API_KEY,
    base_url=DASHSCOPE_BASE_URL,
    timeout=API_TIMEOUT,  # 120ç§’
)
```

**ä¸éœ€è¦æ‰‹åŠ¨ `wait_for`**ï¼ŒSDK å·²å®ç°è¶…æ—¶æ§åˆ¶ã€‚

---

### 8.5 å®¹é”™å±‚çº§æ€»ç»“

| å±‚çº§ | å¤±è´¥åœºæ™¯ | å¤„ç†ç­–ç•¥ | ä»£ç ä½ç½® |
|------|---------|---------|---------|
| 1ï¸âƒ£ æŸ¥è¯¢çº§åˆ« | å•ä¸ª Tavily æŸ¥è¯¢å¤±è´¥ | è·³è¿‡ï¼Œç»§ç»­å…¶ä»–æŸ¥è¯¢ | `deep_research()` |
| 2ï¸âƒ£ ä¸»é¢˜çº§åˆ« | 1-4ä¸ªä¸»é¢˜å¤±è´¥ï¼ˆè¿”å›ç©º learningsï¼‰ | è­¦å‘Šï¼Œç»§ç»­å¤„ç† | `run_web_research()` Step 4 |
| ğŸš« å…¨éƒ¨ä¸»é¢˜å¤±è´¥ | 5ä¸ªä¸»é¢˜å…¨éƒ¨è¿”å›ç©º learnings | **æŠ›å‡ºå¼‚å¸¸ï¼Œç»ˆæ­¢** | `run_web_research()` Step 4 |
| âš ï¸ çŸ¥è¯†ç‚¹è¿‡å°‘ | learnings < 5 | ç»§ç»­å¤„ç†ï¼Œ**å¼ºåˆ¶è¦†ç›–** `search_confidence = "ä½"` | `run_web_research()` Step 6 |
| 3ï¸âƒ£ æŠ¥å‘Šç”Ÿæˆçº§åˆ« | LLM ç”ŸæˆæŠ¥å‘Šå¤±è´¥ | **é™çº§æŠ¥å‘Š**ï¼ˆç½®ä¿¡åº¦ "ä½"ï¼Œæ–°é—»åˆ—è¡¨ä¸ºç©ºï¼Œ`raw_learnings` ä¿å­˜åŸå§‹æ•°æ®ï¼‰ | `run_web_research()` Step 5 |
| ğŸš« æœ€ç»ˆæ ¡éªŒå¤±è´¥ | Pydantic æ ¡éªŒå¤±è´¥ | **æŠ›å‡ºå¼‚å¸¸ï¼Œç»ˆæ­¢** | `run_web_research()` Step 6 |

---

## ä¹ã€æ–‡ä»¶ç»“æ„

æ¨¡å—Bçš„ä»£ç æ–‡ä»¶è§„åˆ’ï¼ˆåœ¨ `stock_analyzer/` ç›®å½•ä¸‹ï¼‰ï¼š

```
stock_analyzer/
â”œâ”€â”€ module_b_websearch.py       # æ¨¡å—Bä¸»å…¥å£ï¼šrun_web_research()
â”œâ”€â”€ deep_research.py            # æ ¸å¿ƒé€’å½’é€»è¾‘ï¼šdeep_research()
â”‚                                 generate_serp_queries()
â”‚                                 process_serp_result()
â”œâ”€â”€ tavily_client.py            # Tavily API å°è£…ï¼štavily_search()
â”‚                                 RETRYABLE_EXCEPTIONS å¸¸é‡
â”œâ”€â”€ agents.py                   # Agent å·¥å‚ï¼šcreate_query_agent() ç­‰
â”œâ”€â”€ prompts.py                  # æ‰€æœ‰æç¤ºè¯å¸¸é‡
â”œâ”€â”€ models.py                   # Pydantic æ•°æ®æ¨¡å‹ï¼ˆå« SearchConfigï¼‰
â”œâ”€â”€ llm_helpers.py              # LLM è°ƒç”¨è¾…åŠ©ï¼šcall_agent_with_model()
â”‚                                 extract_json_str() (âš ï¸ ä½¿ç”¨ itertools.chain)
â”œâ”€â”€ exceptions.py               # è‡ªå®šä¹‰å¼‚å¸¸ç±»
â”œâ”€â”€ logger.py                   # æ—¥å¿—é…ç½®ï¼šsetup_logger()
â””â”€â”€ config.py                   # é…ç½®å¸¸é‡
```

| æ–‡ä»¶ | æ ¸å¿ƒå†…å®¹ | ä¾èµ– |
|------|---------|------|
| `module_b_websearch.py` | `run_web_research()` å…¥å£ | æ‰€æœ‰å…¶ä»–æ–‡ä»¶ |
| `deep_research.py` | `deep_research()` é€’å½’å‡½æ•° | `tavily_client`, `llm_helpers`, `models`, `logger` |
| `tavily_client.py` | `tavily_search()` | `tavily-python`, `config`, `logger` |
| `agents.py` | Agent å·¥å‚å‡½æ•° | `agent_framework`, `prompts`, `config` |
| `prompts.py` | 3 ä¸ªç³»ç»Ÿæç¤ºè¯ | æ—  |
| `models.py` | å…¨éƒ¨ Pydantic æ¨¡å‹ | `pydantic` |
| `llm_helpers.py` | `call_agent_with_model()` | `agent_framework`, `logger` |
| `logger.py` | `setup_logger()` æ—¥å¿—é…ç½® | `logging`, `config` |
| `config.py` | ç¯å¢ƒå˜é‡ã€å‚æ•°å¸¸é‡ | `dotenv` |

---

## åã€æœç´¢è§„æ¨¡ä¸èµ„æºä¼°ç®—

### 10.1 å•æ¬¡åˆ†æçš„è°ƒç”¨é‡

**å‚æ•°è®¾ç½®ï¼š** `breadth=3, depth=2`ï¼ˆé»˜è®¤é…ç½®ï¼‰

| æŒ‡æ ‡ | è®¡ç®—æ–¹å¼ | æ•°å€¼ |
|------|---------|------|
| æœç´¢ä¸»é¢˜æ•° | å›ºå®š | 5 |
| æ¯ä¸»é¢˜ Tavily è°ƒç”¨ | ç¬¬1å±‚3æ¬¡ + ç¬¬2å±‚3æ¬¡ = 6 | 6 |
| æ€» Tavily è°ƒç”¨ | 5 Ã— 6 | **30** |
| æ¯ä¸»é¢˜ LLM è°ƒç”¨ï¼ˆç”ŸæˆæŸ¥è¯¢ï¼‰ | ç¬¬1å±‚1æ¬¡ + ç¬¬2å±‚3æ¬¡ = 4 | 4 |
| æ¯ä¸»é¢˜ LLM è°ƒç”¨ï¼ˆæå–çŸ¥è¯†ï¼‰ | ç¬¬1å±‚3æ¬¡ + ç¬¬2å±‚3æ¬¡ = 6 | 6 |
| æ€» LLM è°ƒç”¨ï¼ˆä¸å«æŠ¥å‘Šï¼‰ | 5 Ã— (4 + 6) | **50** |
| æŠ¥å‘Šç”Ÿæˆ LLM è°ƒç”¨ | 1 | **1** |
| **æ€» LLM è°ƒç”¨** | 50 + 1 | **51** |

### 10.2 Token æ¶ˆè€—ä¼°ç®—ï¼ˆå‚è€ƒï¼‰

> **è¯´æ˜ï¼š** ä»¥ä¸‹ Token æ¶ˆè€—ä¸ºä¼°ç®—å€¼ï¼Œä»…ä¾›æˆæœ¬è¯„ä¼°å‚è€ƒã€‚å®é™…ä»£ç ä¸­ä¸è¿›è¡Œ Token è®¡ç®—ã€‚

| è°ƒç”¨ç±»å‹ | å•æ¬¡ input tokens | å•æ¬¡ output tokens | æ€»æ¬¡æ•° | æ€» tokens |
|---------|-------------------|--------------------|----|-----------|
| ç”ŸæˆæŸ¥è¯¢ï¼ˆæ—  learningsï¼‰ | ~300 | ~200 | 5 | ~2,500 |
| ç”ŸæˆæŸ¥è¯¢ï¼ˆå¸¦ learningsï¼‰ | ~800 | ~150 | 15 | ~14,250 |
| æå–çŸ¥è¯† | ~1,500 | ~300 | 30 | ~54,000 |
| ç”ŸæˆæŠ¥å‘Š | ~6,000 | ~1,500 | 1 | ~7,500 |
| **æ€»è®¡** | â€” | â€” | **51** | **~78,250** |

**æˆæœ¬ä¼°ç®—ï¼ˆqwen-plus å®šä»·å‚è€ƒï¼‰ï¼š**

æ ¹æ®è¡¨æ ¼åˆ†é¡¹è®¡ç®—ï¼š
- è¾“å…¥ tokensï¼š300Ã—5 + 800Ã—15 + 1,500Ã—30 + 6,000Ã—1 = **64,500 tokens**
- è¾“å‡º tokensï¼š200Ã—5 + 150Ã—15 + 300Ã—30 + 1,500Ã—1 = **13,750 tokens**
- æ€»è®¡ï¼š64,500 + 13,750 = **78,250 tokens** âœ“

æˆæœ¬è®¡ç®—ï¼š
- è¾“å…¥æˆæœ¬ï¼š0.8å…ƒ/ç™¾ä¸‡tokens Ã— 0.0645M â‰ˆ **0.052å…ƒ**
- è¾“å‡ºæˆæœ¬ï¼š2å…ƒ/ç™¾ä¸‡tokens Ã— 0.01375M â‰ˆ **0.028å…ƒ**
- **å•æ¬¡åˆ†æé¢„ä¼°æ€»æˆæœ¬ï¼šçº¦ 0.08 å…ƒ**

### 10.3 è€—æ—¶ä¼°ç®—

| é˜¶æ®µ | è€—æ—¶ | è¯´æ˜ |
|------|------|------|
| 5ä¸ªä¸»é¢˜å¹¶è¡Œ Deep Research | 40-80ç§’ | å— Tavily å“åº”é€Ÿåº¦å’Œ LLM å»¶è¿Ÿå½±å“ |
| æŠ¥å‘Šç”Ÿæˆ | 8-15ç§’ | å•æ¬¡ LLM è°ƒç”¨ï¼Œè¾“å…¥è¾ƒé•¿ |
| **æ€»è®¡** | **50-95ç§’** | å–å†³äºç½‘ç»œæ¡ä»¶å’Œ API å“åº”é€Ÿåº¦ |

---

## åä¸€ã€æµ‹è¯•ç­–ç•¥

**æ³¨æ„ï¼š** æ–­è¨€å¼‚å¸¸ç±»å‹æ—¶ï¼Œåº”ä½¿ç”¨ `stock_analyzer.exceptions` ä¸­å®šä¹‰çš„å¼‚å¸¸ç±»ï¼Œç¡®ä¿ä¸æ¨¡å—å®ç°ä¿æŒä¸€è‡´ã€‚

### 11.1 å•å…ƒæµ‹è¯•

| æµ‹è¯•é¡¹ | æµ‹è¯•æ–¹æ³• |
|--------|---------|
| `extract_json_str()` | çº¯å‡½æ•°æµ‹è¯•ï¼Œè¦†ç›– 7 ç§åœºæ™¯ï¼šç›´æ¥ JSONã€markdown åŒ…è£¹ã€å‰ç½®æ–‡å­—ã€**å¤šä»£ç å—ï¼ˆå–æœ€åæœ‰æ•ˆï¼‰**ã€æ— æ•ˆè¾“å…¥æŠ›å¼‚å¸¸ã€‚**æ³¨æ„**ï¼šå®ç°ä¸­ä½¿ç”¨ `itertools.chain()` è€Œé `+` è¿ç®—ç¬¦è¿æ¥ `reversed()` è¿­ä»£å™¨ |
| `setup_logger()` | éªŒè¯æ—¥å¿—æ–‡ä»¶åˆ›å»ºã€æ§åˆ¶å°å’Œæ–‡ä»¶æ—¥å¿—çº§åˆ« |
| `_create_fallback_report()` | æµ‹è¯•é™çº§æŠ¥å‘Šç»“æ„ã€å­—æ®µé•¿åº¦é™åˆ¶ã€**ç»Ÿä¸€æˆªæ–­ç­–ç•¥**ï¼ˆæ‹¼æ¥åæˆªæ–­åˆ°500å­—ç¬¦ï¼‰ã€**ç©ºæ–°é—»åˆ—è¡¨éªŒè¯**ã€‚**é‡è¦**ï¼šä¸æ–­è¨€æ•´å¥ä¸­æ–‡æ–‡æ¡ˆï¼Œåªæ–­è¨€å…³é”®è¦ç´ ï¼ˆåŒ…å«æ•°é‡ã€åŒ…å«é”™è¯¯ã€é•¿åº¦<=500ã€è¶…é•¿æ—¶...ï¼‰ï¼Œé¿å…æ–‡æ¡ˆå¾®è°ƒå¯¼è‡´å‡å¤±è´¥ |
| `successful_topics` ç»Ÿè®¡é€»è¾‘ | æµ‹è¯•ç©º learnings çš„ ResearchResult ä¸è®¡å…¥æˆåŠŸæ•° |
| é™çº§æŠ¥å‘Šè¯†åˆ« | éªŒè¯é€šè¿‡ `meta.raw_learnings is not None` è¯†åˆ«é™çº§æŠ¥å‘Š |
| `SearchConfig` ç»“æ„åŒ–è®¿é—® | æµ‹è¯• `meta.search_config.successful_topics` å±æ€§è®¿é—®ï¼ˆé¿å… dict KeyErrorï¼‰ |
| `tavily_search()` å¼‚å¸¸å¤„ç† | **ä¸¤å±‚æ–­è¨€æµ‹è¯•**ï¼šâ‘ å¤–å±‚ `pytest.raises(TavilySearchError)`ï¼›â‘¡å†…å±‚éªŒè¯ `exc.value.cause` æ˜¯åŸå§‹å¼‚å¸¸ã€‚æµ‹è¯•å¯é‡è¯•å¼‚å¸¸ä¼šé‡è¯•ï¼Œä¸å¯é‡è¯•å¼‚å¸¸ç«‹å³å¤±è´¥ |
| `asyncio.CancelledError` ä¼ æ’­ | **ä½¿ç”¨ `@pytest.mark.asyncio`**ï¼Œæµ‹è¯• gather ç»“æœä¸­çš„ `CancelledError` ä¼šå‘ä¸Šä¼ æ’­è€Œéè¢«åæ‰ã€‚Mock è¿”å›å€¼ç±»å‹å¿…é¡»ä¸å‡½æ•°å¥‘çº¦ä¸€è‡´ï¼ˆ`tavily_search` è¿”å› `list[dict]`ï¼‰ |
| Pydantic æ¨¡å‹æ ¡éªŒ | æµ‹è¯•åˆæ³•å’Œéæ³•è¾“å…¥çš„è§£æç»“æœï¼ˆåŒ…å« `raw_learnings` å¯é€‰å­—æ®µï¼‰ |

### 11.2 é›†æˆæµ‹è¯•ï¼ˆMockï¼‰

| æµ‹è¯•é¡¹ | Mock å¯¹è±¡ | éªŒè¯ç‚¹ |
|--------|----------|--------|
| `generate_serp_queries` | Mock `ChatAgent.run()` | æç¤ºè¯æ‹¼æ¥é€»è¾‘ã€è¾“å‡ºè§£æ |
| `process_serp_result` | Mock `ChatAgent.run()` | æœç´¢ç»“æœæ ¼å¼åŒ–ã€learnings æå– |
| `deep_research` | Mock `tavily_search` + Agent | é€’å½’æ·±åº¦æ§åˆ¶ã€breadth é€’å‡ã€åˆå¹¶å»é‡ |
| `run_web_research` | Mock æ‰€æœ‰å¤–éƒ¨è°ƒç”¨ | ä¸»é¢˜å¹¶è¡Œã€é™çº§å¤„ç†ã€`meta.raw_learnings` å¡«å……ã€learnings < 5 æ—¶å¼ºåˆ¶ `search_confidence = "ä½"`ã€**ç©º learnings ä¸»é¢˜ä¸è®¡å…¥æˆåŠŸæ•°** |

**å…³é”®æµ‹è¯•åœºæ™¯ï¼šç©ºç»“æœä¸»é¢˜ç»Ÿè®¡**

```python
# tests/test_successful_topics_counting.py
def test_successful_topics_excludes_empty_results():
    """æµ‹è¯•ç©º learnings çš„ä¸»é¢˜ä¸è®¡å…¥æˆåŠŸæ•°"""
    results = [
        ResearchResult(learnings=["data1", "data2"], visited_urls=["url1"]),  # æˆåŠŸ
        ResearchResult(learnings=[], visited_urls=[]),  # å¤±è´¥ï¼ˆç©ºç»“æœï¼‰
        ResearchResult(learnings=["data3"], visited_urls=["url2"]),  # æˆåŠŸ
        ResearchResult(learnings=[], visited_urls=["url3"]),  # å¤±è´¥ï¼ˆç©ºç»“æœï¼‰
        Exception("Failed"),  # å¼‚å¸¸
    ]
    
    successful_topics = sum(
        1 for r in results 
        if isinstance(r, ResearchResult) and len(r.learnings) > 0
    )
    
    assert successful_topics == 2, "Only 2 topics with non-empty learnings should be counted"


def test_all_topics_empty_triggers_error():
    """æµ‹è¯•æ‰€æœ‰ä¸»é¢˜è¿”å›ç©ºç»“æœæ—¶è§¦å‘å¼‚å¸¸"""
    results = [
        ResearchResult(learnings=[], visited_urls=[]) for _ in range(5)
    ]
    
    successful_topics = sum(
        1 for r in results 
        if isinstance(r, ResearchResult) and len(r.learnings) > 0
    )
    
    assert successful_topics == 0
    # åœ¨å®é™…ä»£ç ä¸­ï¼Œè¿™ä¼šè§¦å‘ WebResearchError


@pytest.mark.asyncio
async def test_deep_research_cancel_propagates():
    """
    æµ‹è¯• deep_research ä¸­ CancelledError ä¼šå‘ä¸Šä¼ æ’­è€Œéè¢«åæ‰
    
    åœºæ™¯ï¼šæ¨¡æ‹Ÿ tavily_search è¢«å–æ¶ˆï¼ŒéªŒè¯å–æ¶ˆå¼‚å¸¸ä¼šä¼ æ’­åˆ°è°ƒç”¨æ–¹
    
    å…³é”®ï¼šMock å¯¹è±¡å¿…é¡»æ»¡è¶³å®é™…ä»£ç å¥‘çº¦ï¼ˆresponse.textï¼‰ï¼Œé¿å…åœ¨æ— å…³ä½ç½®æå‰å¤±è´¥
    """
    from unittest.mock import AsyncMock, patch
    from types import SimpleNamespace
    
    # Mock Agent å¯¹è±¡ï¼ˆè¿”å›å€¼å¿…é¡»æœ‰ .text å±æ€§ï¼ŒåŒ¹é…ä¸»é€»è¾‘å¥‘çº¦ï¼‰
    mock_query_agent = AsyncMock()
    mock_query_agent.name = "query_agent"
    # âœ… æ­£ç¡®ï¼šä½¿ç”¨ SimpleNamespace æ¨¡æ‹Ÿå¸¦ .text å±æ€§çš„å“åº”å¯¹è±¡
    mock_query_agent.run.return_value = SimpleNamespace(
        text='```json\n{"queries": ["q1", "q2", "q3"]}\n```'
    )
    # âŒ é”™è¯¯ï¼šç›´æ¥è¿”å›å­—ç¬¦ä¸²ä¼šåœ¨ response.text è®¿é—®æ—¶å¤±è´¥
    # mock_query_agent.run.return_value = '...'
    
    mock_extract_agent = AsyncMock()
    mock_extract_agent.name = "extract_agent"
    
    # å…³é”®ï¼špatch çš„è·¯å¾„åº”è¯¥æ˜¯ä½¿ç”¨å®ƒçš„æ¨¡å—è·¯å¾„ï¼Œè€Œéå®šä¹‰å®ƒçš„æ¨¡å—è·¯å¾„
    # å‡è®¾ deep_research åœ¨ stock_analyzer.deep_research æ¨¡å—ä¸­
    with patch('stock_analyzer.deep_research.tavily_search') as mock_tavily:
        # æ¨¡æ‹Ÿç¬¬2ä¸ªæŸ¥è¯¢è¢«å–æ¶ˆï¼ˆè¿”å›å€¼ç±»å‹åº”ä¸º list[dict]ï¼Œä¸æ˜¯ {"results": ...}ï¼‰
        mock_tavily.side_effect = [
            [{"url": "url1", "content": "content1"}],  # ç¬¬1ä¸ªæˆåŠŸ
            asyncio.CancelledError("Task cancelled"),  # ç¬¬2ä¸ªè¢«å–æ¶ˆ
            [{"url": "url2", "content": "content2"}],  # ç¬¬3ä¸ªï¼ˆä¸ä¼šæ‰§è¡Œï¼‰
        ]
        
        with pytest.raises(asyncio.CancelledError):
            await deep_research(
                query_agent=mock_query_agent,       # âœ… æ­£ç¡®å‚æ•°å
                extract_agent=mock_extract_agent,   # âœ… æ­£ç¡®å‚æ•°å
                query="æµ‹è¯•ä¸»é¢˜",                    # âœ… query è€Œé topic
                breadth=3,
                depth=1
            )


@pytest.mark.asyncio
async def test_run_web_research_cancel_propagates():
    """
    æµ‹è¯• run_web_research ä¸­ CancelledError ä¼šå‘ä¸Šä¼ æ’­è€Œéè¢«åæ‰
    
    åœºæ™¯ï¼šæ¨¡æ‹ŸæŸä¸ªä¸»é¢˜ç ”ç©¶è¢«å–æ¶ˆï¼ŒéªŒè¯å–æ¶ˆå¼‚å¸¸ä¼šä¼ æ’­åˆ°è°ƒç”¨æ–¹
    
    æ³¨æ„ï¼šæœ¬æµ‹è¯• mock çš„æ˜¯ deep_research å‡½æ•°æœ¬èº«ï¼Œæ— éœ€ mock Agent å“åº”å¯¹è±¡
    """
    from unittest.mock import AsyncMock, patch
    
    # å…³é”®ï¼špatch çš„è·¯å¾„åº”è¯¥æ˜¯ä½¿ç”¨å®ƒçš„æ¨¡å—è·¯å¾„
    with patch('stock_analyzer.module_b_websearch.deep_research') as mock_research:
        # æ¨¡æ‹Ÿç¬¬2ä¸ªä¸»é¢˜è¢«å–æ¶ˆ
        mock_research.side_effect = [
            ResearchResult(learnings=["data1"], visited_urls=["url1"]),  # ä¸»é¢˜1æˆåŠŸ
            asyncio.CancelledError("Topic cancelled"),  # ä¸»é¢˜2è¢«å–æ¶ˆ
            ResearchResult(learnings=["data2"], visited_urls=["url2"]),  # ä¸»é¢˜3ï¼ˆä¸ä¼šæ‰§è¡Œï¼‰
        ]
        
        with pytest.raises(asyncio.CancelledError):
            await run_web_research(
                symbol="000001",
                name="å¹³å®‰é“¶è¡Œ",
                industry="é“¶è¡Œ",
                breadth=3,
                depth=1
            )


def test_extract_json_str_comprehensive():
    """æµ‹è¯• extract_json_str çš„å¤šç§åœºæ™¯"""
    import pytest
    
    # åœºæ™¯1ï¼šç›´æ¥ JSONï¼ˆæœ€å¿«è·¯å¾„ï¼‰
    result1 = extract_json_str('{"key": "value"}')
    assert result1 == '{"key": "value"}'
    
    # åœºæ™¯2ï¼šæ ‡å‡† markdown åŒ…è£¹
    result2 = extract_json_str('```json\n{"key": "value"}\n```')
    assert result2 == '{"key": "value"}'
    
    # åœºæ™¯3ï¼šå‰ç½®è§£é‡Šæ–‡å­—
    result3 = extract_json_str('Here is the result:\n```json\n{"key": "value"}\n```')
    assert result3 == '{"key": "value"}'
    
    # åœºæ™¯4ï¼šå¤šä¸ªä»£ç å—ï¼Œå–æœ€åä¸€ä¸ªæœ‰æ•ˆçš„
    result4 = extract_json_str(
        'Here is my thinking:\n'
        '```text\n'
        'Some explanation...\n'
        '```\n'
        'And here is the result:\n'
        '```json\n'
        '{"queries": ["q1", "q2"]}\n'
        '```'
    )
    assert result4 == '{"queries": ["q1", "q2"]}'
    
    # åœºæ™¯5ï¼šå¤šä¸ª json ä»£ç å—ï¼Œå–æœ€åä¸€ä¸ª
    result5 = extract_json_str(
        '```json\n{"draft": true}\n```\n'
        'Let me revise:\n'
        '```json\n{"final": true}\n```'
    )
    assert result5 == '{"final": true}'
    
    # åœºæ™¯6ï¼šæ— ä»£ç å—ï¼Œç›´æ¥æ˜¯ JSONï¼ˆæ•´æ®µæ–‡æœ¬éªŒè¯ï¼‰
    result6 = extract_json_str('  {"key": "value"}  ')
    assert result6 == '{"key": "value"}'
    
    # åœºæ™¯7ï¼šå…¨éƒ¨æ— æ•ˆï¼ŒæŠ›å‡ºå¼‚å¸¸
    with pytest.raises(ValueError, match="No valid JSON found"):
        extract_json_str('This is not JSON at all')


### 11.1.1 é™çº§æŠ¥å‘Šæµ‹è¯•æœ€ä½³å®è·µ

**é‡è¦åŸåˆ™ï¼šä¸æ–­è¨€æ•´å¥ä¸­æ–‡æ–‡æ¡ˆï¼Œæ–­è¨€å…³é”®è¦ç´ **

**èƒŒæ™¯ï¼š** é™çº§æŠ¥å‘Šçš„é”™è¯¯æè¿°æ˜¯ç”¨æˆ·å¯è§çš„æç¤ºæ–‡æ¡ˆï¼Œå¯èƒ½ä¼šæ ¹æ®äº§å“éœ€æ±‚å¾®è°ƒï¼ˆå¦‚æ”¹è¿›æªè¾ã€è°ƒæ•´æ ¼å¼ç­‰ï¼‰ã€‚å¦‚æœæµ‹è¯•æ–­è¨€æ•´å¥æ–‡æ¡ˆï¼Œä»»ä½•æ–‡æ¡ˆè°ƒæ•´éƒ½ä¼šå¯¼è‡´æµ‹è¯•å¤±è´¥ï¼Œäº§ç”Ÿ"å‡å¤±è´¥"ã€‚

**æœ€ä½³å®è·µï¼š**

| âŒ ä¸æ¨èï¼ˆè„†å¼±ï¼‰ | âœ… æ¨èï¼ˆç¨³å®šï¼‰ |
|-----------------|---------------|
| `assert "æŠ¥å‘Šç”Ÿæˆå¤±è´¥ï¼ˆå…±æ”¶é›† 3 æ¡å­¦ä¹ ç‚¹ï¼‰ï¼š" in desc` | `assert str(len(learnings)) in desc  # åŒ…å«æ•°é‡`<br>`assert error_msg in desc  # åŒ…å«é”™è¯¯` |
| `assert desc == "æŠ¥å‘Šç”Ÿæˆå¤±è´¥ï¼Œå…±æ”¶é›†åˆ° 3 æ¡ä¿¡æ¯..."` | `assert len(desc) <= 500`<br>`assert error_msg in desc` |

**åº”æ–­è¨€çš„å…³é”®è¦ç´ ï¼š**
1. âœ… **é•¿åº¦é™åˆ¶**ï¼š`assert len(desc) <= 500`
2. âœ… **åŒ…å«æ•°é‡**ï¼š`assert str(len(learnings)) in desc`ï¼ˆæ¨èï¼Œé¿å…ç¡¬ç¼–ç ï¼‰
3. âœ… **åŒ…å«é”™è¯¯ä¿¡æ¯**ï¼š`assert error_message in desc`
4. âœ… **è¶…é•¿æˆªæ–­æ ‡å¿—**ï¼š`assert desc.endswith("...")`ï¼ˆå½“è¶…é•¿æ—¶ï¼‰
5. âŒ **ä¸æ–­è¨€å…·ä½“æ–‡æ¡ˆ**ï¼šé¿å… `assert "æŠ¥å‘Šç”Ÿæˆå¤±è´¥ï¼ˆå…±æ”¶é›† X æ¡å­¦ä¹ ç‚¹ï¼‰ï¼š" in desc`
6. âŒ **ä¸ä½¿ç”¨ç¡¬ç¼–ç æ•°å­—**ï¼šé¿å… `assert "3" in desc`ï¼ˆæ”¹ç”¨ `str(len(learnings))`ï¼‰

**ç¤ºä¾‹å¯¹æ¯”ï¼š**

```python
# âŒ åä¾‹ï¼ˆä¼ªä»£ç ï¼‰ï¼šä¸è¦æ–­è¨€å®Œæ•´ä¸­æ–‡æ–‡æ¡ˆ + ç¡¬ç¼–ç æ•°å­—
# bad_assert = 'assert "æŠ¥å‘Šç”Ÿæˆå¤±è´¥ï¼ˆå…±æ”¶é›† 3 æ¡å­¦ä¹ ç‚¹ï¼‰ï¼šConnection timeout" in desc'

# âœ… ç¨³å®šçš„æ–­è¨€ï¼ˆåªéªŒè¯å…³é”®è¦ç´ ï¼Œä½¿ç”¨è¡¨è¾¾å¼ï¼‰
learnings = ["data1", "data2", "data3"]
error_msg = "Connection timeout"
report = _create_fallback_report(learnings, error_msg)
desc = report["competitive_advantage"]["description"]

assert len(desc) <= 500
assert str(len(learnings)) in desc  # ä½¿ç”¨è¡¨è¾¾å¼ï¼Œä¸ç¡¬ç¼–ç  "3"
assert error_msg in desc
```

---

### 11.1.2 é™çº§æŠ¥å‘Šæµ‹è¯•ç¤ºä¾‹

```python
def test_fallback_report_empty_news_is_valid():
    """æµ‹è¯•é™çº§æŠ¥å‘Šå…è®¸ç©ºæ–°é—»åˆ—è¡¨"""
    fallback_report = _create_fallback_report(
        learnings=["data1", "data2"],
        error_message="LLM output invalid JSON"
    )
    
    # é™çº§æŠ¥å‘Šçš„æ–°é—»åˆ—è¡¨åº”è¯¥ä¸ºç©º
    assert fallback_report["news_summary"]["positive"] == []
    assert fallback_report["news_summary"]["negative"] == []
    assert fallback_report["news_summary"]["neutral"] == []
    
    # è¿™æ˜¯é¢„æœŸè¡Œä¸ºï¼Œä¸åº”åœ¨ E2E æµ‹è¯•ä¸­è§†ä¸ºå¤±è´¥


def test_fallback_report_description_length_boundary():
    """
    æµ‹è¯• _create_fallback_report() çš„ description é•¿åº¦æ§åˆ¶
    
    æµ‹è¯•åœºæ™¯ï¼š
    1. çŸ­é”™è¯¯ä¿¡æ¯ï¼šä¸æˆªæ–­
    2. è¶…é•¿é”™è¯¯ä¿¡æ¯ï¼šæˆªæ–­åˆ° 500 å­—ç¬¦
    3. æœ€ç»ˆé•¿åº¦å§‹ç»ˆ <= 500
    
    æ³¨æ„ï¼šä¸æ–­è¨€æ•´å¥ä¸­æ–‡æ–‡æ¡ˆï¼Œåªæ–­è¨€å…³é”®è¦ç´ ï¼Œé¿å…æ–‡æ¡ˆå¾®è°ƒå¯¼è‡´å‡å¤±è´¥
    """
    # åœºæ™¯1ï¼šçŸ­é”™è¯¯ä¿¡æ¯ï¼Œä¸éœ€è¦æˆªæ–­
    learnings = ["data1", "data2", "data3"]
    short_error = "Connection timeout"
    report = _create_fallback_report(learnings, short_error)
    desc = report["competitive_advantage"]["description"]
    
    # æ–­è¨€å…³é”®è¦ç´ ï¼Œè€Œéæ•´å¥æ–‡æ¡ˆ
    assert len(desc) <= 500  # é•¿åº¦é™åˆ¶
    assert str(len(learnings)) in desc  # åŒ…å«æ•°é‡ï¼ˆä½¿ç”¨è¡¨è¾¾å¼ï¼Œé¿å…ç¡¬ç¼–ç ï¼‰
    assert "Connection timeout" in desc  # åŒ…å«é”™è¯¯ä¿¡æ¯
    
    # åœºæ™¯2ï¼šè¶…é•¿é”™è¯¯ä¿¡æ¯ï¼Œéœ€è¦æˆªæ–­
    long_error = "E" * 1000
    report2 = _create_fallback_report(learnings, long_error)
    desc2 = report2["competitive_advantage"]["description"]
    assert len(desc2) == 500  # åˆšå¥½æˆªæ–­åˆ° 500
    assert desc2.endswith("...")  # è¶…é•¿æ—¶ä»¥ ... ç»“å°¾
    assert str(len(learnings)) in desc2  # åŒ…å«æ•°é‡ï¼ˆä½¿ç”¨è¡¨è¾¾å¼ï¼Œé¿å…ç¡¬ç¼–ç ï¼‰
    
    # åœºæ™¯3ï¼šå¤§é‡ learningsï¼Œè¶…é•¿é”™è¯¯
    many_learnings = ["x"] * 9999
    report3 = _create_fallback_report(many_learnings, long_error)
    desc3 = report3["competitive_advantage"]["description"]
    assert len(desc3) == 500
    assert desc3.endswith("...")
```

### 11.3 ç«¯åˆ°ç«¯æµ‹è¯•

**æ³¨æ„ï¼š** æµ‹è¯•ä¸­å¯¼å…¥çš„å¼‚å¸¸ç±»å‹åº”ä» `stock_analyzer.exceptions` æ¨¡å—å¯¼å…¥ï¼Œç¡®ä¿ä¸å®é™…å®ç°ä¸€è‡´ã€‚

```python
# tests/test_module_b_e2e.py
import asyncio
from stock_analyzer.module_b_websearch import run_web_research
from stock_analyzer.exceptions import WebResearchError


async def test_web_research_e2e():
    """
    ç«¯åˆ°ç«¯æµ‹è¯•ï¼šå¯¹å¹³å®‰é“¶è¡Œæ‰§è¡Œå®Œæ•´æ·±åº¦æœç´¢ï¼ˆä½¿ç”¨çœŸå® APIï¼‰
    
    æ³¨æ„ï¼šæµ‹è¯•éœ€è¦å®¹å¿é™çº§æŠ¥å‘Šåœºæ™¯ï¼Œé™çº§æŠ¥å‘Šä¼šè¿”å›ç©ºæ–°é—»åˆ—è¡¨ï¼Œ
    è¿™æ˜¯é¢„æœŸè¡Œä¸ºï¼Œä¸åº”è§†ä¸ºæµ‹è¯•å¤±è´¥ã€‚
    """
    result = await run_web_research(
        symbol="000001",
        name="å¹³å®‰é“¶è¡Œ",
        industry="é“¶è¡Œ",
        # ä½¿ç”¨é»˜è®¤å‚æ•° breadth=3, depth=2
    )
    assert result.meta.symbol == "000001"
    assert result.meta.total_learnings > 0
    assert result.search_confidence in ("é«˜", "ä¸­", "ä½")
    
    # æ£€æŸ¥æ˜¯å¦ä¸ºé™çº§æŠ¥å‘Šï¼ˆåˆ¤æ–­ä¾æ®ï¼šæ˜¯å¦å¡«å……äº† raw_learningsï¼‰
    is_fallback = result.meta.raw_learnings is not None
    
    if is_fallback:
        # é™çº§æŠ¥å‘Šåœºæ™¯ï¼šå…è®¸ç©ºæ–°é—»åˆ—è¡¨ï¼ŒéªŒè¯ raw_learnings å­˜åœ¨
        assert len(result.meta.raw_learnings) == result.meta.total_learnings
        assert result.search_confidence == "ä½", "Fallback report must have 'ä½' confidence"
        # é™çº§æŠ¥å‘Šå¯èƒ½è¿”å›ç©ºæ–°é—»åˆ—è¡¨ï¼Œä¸åšå¼ºåˆ¶è¦æ±‚
    else:
        # æ­£å¸¸æŠ¥å‘Šåœºæ™¯ï¼šå¿…é¡»æœ‰æ–°é—»å†…å®¹ï¼ˆåŒ…æ‹¬ neutralï¼‰
        total_news = (
            len(result.news_summary.positive) + 
            len(result.news_summary.negative) + 
            len(result.news_summary.neutral)
        )
        assert total_news > 0, \
            "Non-fallback report must have news items (positive/negative/neutral)"
    
    # éªŒè¯å¼ºåˆ¶é™çº§é€»è¾‘ï¼šå¦‚æœ learnings < 5ï¼Œå¿…é¡»æ ‡è®°ä¸º "ä½"
    if result.meta.total_learnings < 5:
        assert result.search_confidence == "ä½", \
            f"Expected 'ä½' confidence for {result.meta.total_learnings} learnings"
    
    # éªŒè¯ successful_topics ç»Ÿè®¡ï¼ˆå¿…é¡»æœ‰ learnings æ‰ç®—æˆåŠŸï¼‰
    successful_topics = result.meta.search_config.successful_topics
    assert 0 < successful_topics <= 5, "successful_topics should be in range (0, 5]"
    assert result.meta.total_learnings > 0, "If any topic succeeded, total_learnings must > 0"


if __name__ == "__main__":
    asyncio.run(test_web_research_e2e())
```

---

## åäºŒã€ä½¿ç”¨ç¤ºä¾‹

### 12.1 åŸºç¡€è°ƒç”¨

```python
from stock_analyzer.module_b_websearch import run_web_research

# æ‰§è¡Œæ·±åº¦æœç´¢
result = await run_web_research(
    symbol="000001",
    name="å¹³å®‰é“¶è¡Œ",
    industry="é“¶è¡Œ",
)

# result æ˜¯ WebResearchResult å¯¹è±¡
print(f"æ€»è¯„åˆ†å¯ä¿¡åº¦: {result.search_confidence}")
print(f"æ­£é¢æ–°é—»æ•°é‡: {len(result.news_summary.positive)}")
print(f"æˆåŠŸä¸»é¢˜æ•°: {result.meta.search_config.successful_topics}/{result.meta.search_config.topics_count}")
```

### 12.2 ä¿å­˜ä¸º JSON æ–‡ä»¶

```python
import json
from pathlib import Path

# æ–¹å¼1ï¼šä½¿ç”¨ Pydantic çš„ model_dump_json()ï¼ˆæ¨èï¼‰
json_str = result.model_dump_json(indent=2, ensure_ascii=False)
output_file = Path(f"output/{result.meta.symbol}_web_research.json")
output_file.parent.mkdir(parents=True, exist_ok=True)
output_file.write_text(json_str, encoding="utf-8")

# æ–¹å¼2ï¼šä½¿ç”¨æ ‡å‡† json åº“
with open(f"output/{result.meta.symbol}_web_research.json", "w", encoding="utf-8") as f:
    json.dump(result.model_dump(), f, ensure_ascii=False, indent=2)
```

### 12.3 ä¸å…¶ä»–æ¨¡å—é›†æˆ

```python
# åœ¨ä¸»æµç¨‹ä¸­è°ƒç”¨æ¨¡å—B
async def analyze_stock_complete(symbol: str, name: str, industry: str):
    # æ¨¡å—Aï¼šAKShare æ•°æ®é‡‡é›†ï¼ˆç•¥ï¼‰
    akshare_data = collect_akshare_data(symbol, name)
    
    # æ¨¡å—Bï¼šç½‘ç»œæ·±åº¦æœç´¢
    web_research = await run_web_research(symbol, name, industry)
    
    # æ¨¡å—Cï¼šæŠ€æœ¯åˆ†æï¼ˆç•¥ï¼‰
    technical_analysis = analyze_technical(symbol, name)
    
    # æ¨¡å—Dï¼šé¦–å¸­åˆ†æå¸ˆç»¼åˆåˆ¤æ–­ï¼ˆç•¥ï¼‰
    final_report = generate_final_report(
        akshare_data,
        web_research.model_dump(),  # è½¬ä¸º dict ä¼ é€’
        technical_analysis,
    )
    
    # ä¿å­˜æ‰€æœ‰ç»“æœ
    save_results(symbol, web_research, final_report)
```

### 12.4 é™çº§åœºæ™¯å¤„ç†

```python
try:
    result = await run_web_research(symbol, name, industry)
    
    # æ£€æŸ¥è´¨é‡
    if result.search_confidence == "ä½":
        logger.warning(f"æœç´¢è´¨é‡ä½ï¼š{result.meta.total_learnings} learnings")
        
        # åˆ¤æ–­æ˜¯å¦ä¸ºé™çº§æŠ¥å‘Šï¼ˆæŠ¥å‘Šç”Ÿæˆå¤±è´¥æ—¶è§¦å‘ï¼‰
        if result.meta.raw_learnings is not None:
            logger.warning(f"æ£€æµ‹åˆ°é™çº§æŠ¥å‘Šï¼ˆæŠ¥å‘Šç”Ÿæˆå¤±è´¥ï¼‰ï¼Œå·²ä¿å­˜ {len(result.meta.raw_learnings)} æ¡åŸå§‹ learnings")
            
            # é™çº§æŠ¥å‘Šçš„æ–°é—»åˆ—è¡¨ä¼šæ˜¯ç©ºçš„ï¼Œè¿™æ˜¯é¢„æœŸè¡Œä¸º
            news_count = (
                len(result.news_summary.positive) + 
                len(result.news_summary.negative) + 
                len(result.news_summary.neutral)
            )
            if news_count == 0:
                logger.info("é™çº§æŠ¥å‘ŠæœªåŒ…å«ç»“æ„åŒ–æ–°é—»æ‘˜è¦ï¼Œè¯·æŸ¥çœ‹åŸå§‹ learnings")
            
            # ä¿å­˜åˆ°ç‹¬ç«‹æ–‡ä»¶ä¾›äººå·¥åˆ†æ
            import json
            with open(f"output/{symbol}_raw_learnings.json", "w", encoding="utf-8") as f:
                json.dump(result.meta.raw_learnings, f, ensure_ascii=False, indent=2)
            logger.info(f"åŸå§‹ learnings å·²ä¿å­˜åˆ° output/{symbol}_raw_learnings.json")
    
    # æ£€æŸ¥æˆåŠŸç‡
    success_rate = (
        result.meta.search_config.successful_topics 
        / result.meta.search_config.topics_count
    )
    if success_rate < 0.6:
        logger.warning(f"ä»… {success_rate:.0%} ä¸»é¢˜æˆåŠŸï¼ŒæŠ¥å‘Šè´¨é‡å¯èƒ½å—å½±å“")
    
except WebResearchError as e:
    logger.error(f"Web research å®Œå…¨å¤±è´¥: {e}")
    # ä½¿ç”¨ç©ºå ä½ç¬¦æˆ–è·³è¿‡è¯¥æ¨¡å—
    result = None
```

---

*æ–‡æ¡£ç‰ˆæœ¬ï¼šv1.0*
*æœ€åæ›´æ–°ï¼š2026å¹´2æœˆ*
*å¯¹åº”æ¦‚è¦è®¾è®¡ï¼šstock-analysis-design-v3.1.md ç¬¬å››ç« *
